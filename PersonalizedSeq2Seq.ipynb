{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a2c9e4a2",
            "metadata": {},
            "source": [
                "# Personalized Federated Learning for Seq2Seq\n",
                "\n",
                "This notebook implements a Federated Learning approach for Sequence-to-Sequence models, specifically focusing on personalization for different languages. It includes data handling, model definitions (Encoder-Decoder with Attention), training loops (including Federated Averaging), and evaluation metrics (BLEU score)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "38c8c803",
            "metadata": {},
            "source": [
                "## Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "83eccec9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import random\n",
                "import time\n",
                "import math\n",
                "import copy\n",
                "import csv\n",
                "import re\n",
                "import unicodedata\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch import optim\n",
                "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, Subset\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.ticker as ticker\n",
                "from functools import partial\n",
                "from tqdm.auto import tqdm\n",
                "import sacrebleu\n",
                "import optuna\n",
                "\n",
                "# Set device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Set seed for reproducibility\n",
                "SEED = 1234\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "torch.cuda.manual_seed(SEED)\n",
                "torch.backends.cudnn.deterministic = True"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4ed856ba",
            "metadata": {},
            "source": [
                "## Data Handling\n",
                "Functions and classes for loading, preprocessing, and managing the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "87670704",
            "metadata": {},
            "outputs": [],
            "source": [
                "SOS_token = 0\n",
                "EOS_token = 1\n",
                "\n",
                "class Lang:\n",
                "    def __init__(self, name):\n",
                "        self.name = name\n",
                "        self.word2index = {}\n",
                "        self.word2count = {}\n",
                "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
                "        self.n_words = 2  # Count SOS and EOS\n",
                "\n",
                "    def addSentence(self, sentence):\n",
                "        for word in sentence.split(' '):\n",
                "            self.addWord(word)\n",
                "\n",
                "    def addWord(self, word):\n",
                "        if word not in self.word2index:\n",
                "            self.word2index[word] = self.n_words\n",
                "            self.word2count[word] = 1\n",
                "            self.index2word[self.n_words] = word\n",
                "            self.n_words += 1\n",
                "        else:\n",
                "            self.word2count[word] += 1\n",
                "\n",
                "def unicodeToAscii(s):\n",
                "    return ''.join(\n",
                "        c for c in unicodedata.normalize('NFD', s)\n",
                "        if unicodedata.category(c) != 'Mn'\n",
                "    )\n",
                "\n",
                "def normalizeString(s):\n",
                "    s = unicodeToAscii(s.lower().strip())\n",
                "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
                "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
                "    return s\n",
                "\n",
                "def readLangs(lang1, lang2, reverse=False):\n",
                "    print(\"Reading lines...\")\n",
                "\n",
                "    # Read the file and split into lines\n",
                "    # Assumes data is in 'data/' directory\n",
                "    try:\n",
                "        lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
                "    except FileNotFoundError:\n",
                "        # Fallback if file naming is reversed or different\n",
                "        try:\n",
                "            lines = open('data/%s-%s.txt' % (lang2, lang1), encoding='utf-8').read().strip().split('\\n')\n",
                "        except FileNotFoundError:\n",
                "             # Handle specific cases like flores_kir or kir_test if they exist in root or data\n",
                "             if lang1 == 'flores_kir':\n",
                "                 lines1 = open('data/flores200_devtest_source_eng_Latn-run_Latn.txt', encoding='utf-8').read().strip().split('\\n')\n",
                "                 lines2 = open('data/flores200_devtest_target_eng_Latn-run_Latn.txt', encoding='utf-8').read().strip().split('\\n')\n",
                "                 lines = [f\"{l1}\\t{l2}\" for l1, l2 in zip(lines1, lines2)]\n",
                "             elif lang1 == 'kir_test':\n",
                "                 lines = open('data/kir_test.txt', encoding='utf-8').read().strip().split('\\n')\n",
                "             else:\n",
                "                 raise\n",
                "\n",
                "    # Split every line into pairs and normalize\n",
                "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
                "\n",
                "    # Reverse pairs, make Lang instances\n",
                "    if reverse:\n",
                "        pairs = [list(reversed(p)) for p in pairs]\n",
                "        input_lang = Lang(lang2)\n",
                "        output_lang = Lang(lang1)\n",
                "    else:\n",
                "        input_lang = Lang(lang1)\n",
                "        output_lang = Lang(lang2)\n",
                "\n",
                "    return input_lang, output_lang, pairs\n",
                "\n",
                "MAX_LENGTH = 20\n",
                "\n",
                "def filterPair(p):\n",
                "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
                "        len(p[1].split(' ')) < MAX_LENGTH\n",
                "\n",
                "def filterPairs(pairs):\n",
                "    return [pair for pair in pairs if filterPair(pair)]\n",
                "\n",
                "def prepareData(lang1, lang2, reverse=False):\n",
                "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
                "    print(\"Read %s sentence pairs\" % len(pairs))\n",
                "    pairs = filterPairs(pairs)\n",
                "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
                "    print(\"Counting words...\")\n",
                "    for pair in pairs:\n",
                "        input_lang.addSentence(pair[0])\n",
                "        output_lang.addSentence(pair[1])\n",
                "    print(\"Counted words:\")\n",
                "    print(input_lang.name, input_lang.n_words)\n",
                "    print(output_lang.name, output_lang.n_words)\n",
                "    return input_lang, output_lang, pairs\n",
                "\n",
                "def indexesFromSentence(lang, sentence):\n",
                "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
                "\n",
                "def tensorFromSentence(lang, sentence):\n",
                "    indexes = indexesFromSentence(lang, sentence)\n",
                "    indexes.append(EOS_token)\n",
                "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
                "\n",
                "def tensorsFromPair(pair, input_lang, output_lang):\n",
                "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
                "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
                "    return (input_tensor, target_tensor)\n",
                "\n",
                "def get_dataloader(batch_size, language='fra'):\n",
                "    input_lang, output_lang, pairs = prepareData('eng', language, True)\n",
                "\n",
                "    n = len(pairs)\n",
                "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
                "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
                "\n",
                "    for idx, (inp, tgt) in enumerate(pairs):\n",
                "        inp_ids = indexesFromSentence(input_lang, inp)\n",
                "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
                "        inp_ids.append(EOS_token)\n",
                "        tgt_ids.append(EOS_token)\n",
                "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
                "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
                "\n",
                "    indices_tensor = torch.arange(n)\n",
                "\n",
                "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
                "                               torch.LongTensor(target_ids).to(device),\n",
                "                               indices_tensor.to(device))\n",
                "\n",
                "    train_sampler = RandomSampler(train_data)\n",
                "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
                "    return input_lang, output_lang, train_dataloader, pairs\n",
                "\n",
                "def limited_data_loader(original_dataloader, num_samples, random=True):\n",
                "    dataset = original_dataloader.dataset\n",
                "    assert len(dataset) >= num_samples, \"The original dataset has fewer samples than requested\"\n",
                "    if random:\n",
                "        indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
                "    else:\n",
                "        indices = range(num_samples)\n",
                "    subset = Subset(dataset, indices)\n",
                "    new_dataloader = DataLoader(subset, batch_size=original_dataloader.batch_size, shuffle=False, num_workers=original_dataloader.num_workers)\n",
                "    return new_dataloader\n",
                "\n",
                "def get_pair_index(dataloader):\n",
                "    index_list = []\n",
                "    for batch in dataloader:\n",
                "        index_list.extend(batch[2].tolist())\n",
                "    return index_list\n",
                "\n",
                "def split_dataloader(train_dataloader, ratio=0.9):\n",
                "    dataset = train_dataloader.dataset\n",
                "    train_size = int(ratio * len(dataset))\n",
                "    test_size = len(dataset) - train_size\n",
                "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
                "\n",
                "    train_loader = DataLoader(train_dataset, batch_size=train_dataloader.batch_size, shuffle=True)\n",
                "    test_loader = DataLoader(test_dataset, batch_size=train_dataloader.batch_size, shuffle=False)\n",
                "    return train_loader, test_loader, ratio"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bb67e49f",
            "metadata": {},
            "source": [
                "## Model Architecture\n",
                "Encoder and Decoder definitions, including Attention mechanism."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "fd91d418",
            "metadata": {},
            "outputs": [],
            "source": [
                "class EncoderRNN(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
                "        super(EncoderRNN, self).__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "\n",
                "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
                "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
                "        self.dropout = nn.Dropout(dropout_p)\n",
                "\n",
                "    def forward(self, input):\n",
                "        embedded = self.dropout(self.embedding(input))\n",
                "        output, hidden = self.gru(embedded)\n",
                "        return output, hidden\n",
                "\n",
                "class BahdanauAttention(nn.Module):\n",
                "    def __init__(self, hidden_size):\n",
                "        super(BahdanauAttention, self).__init__()\n",
                "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
                "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
                "        self.Va = nn.Linear(hidden_size, 1)\n",
                "\n",
                "    def forward(self, query, keys):\n",
                "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
                "        scores = scores.squeeze(2).unsqueeze(1)\n",
                "        weights = F.softmax(scores, dim=-1)\n",
                "        context = torch.bmm(weights, keys)\n",
                "        return context, weights\n",
                "\n",
                "class AttnDecoderRNN(nn.Module):\n",
                "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
                "        super(AttnDecoderRNN, self).__init__()\n",
                "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
                "        self.attention = BahdanauAttention(hidden_size)\n",
                "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
                "        self.out = nn.Linear(hidden_size, output_size)\n",
                "        self.dropout = nn.Dropout(dropout_p)\n",
                "\n",
                "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
                "        batch_size = encoder_outputs.size(0)\n",
                "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
                "        decoder_hidden = encoder_hidden\n",
                "        decoder_outputs = []\n",
                "        attentions = []\n",
                "\n",
                "        for i in range(MAX_LENGTH):\n",
                "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
                "                decoder_input, decoder_hidden, encoder_outputs\n",
                "            )\n",
                "            decoder_outputs.append(decoder_output)\n",
                "            attentions.append(attn_weights)\n",
                "\n",
                "            if target_tensor is not None:\n",
                "                # Teacher forcing: Feed the target as the next input\n",
                "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
                "            else:\n",
                "                # Without teacher forcing: use its own predictions as the next input\n",
                "                _, topi = decoder_output.topk(1)\n",
                "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
                "\n",
                "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
                "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
                "        attentions = torch.cat(attentions, dim=1)\n",
                "\n",
                "        return decoder_outputs, decoder_hidden, attentions\n",
                "\n",
                "    def forward_step(self, input, hidden, encoder_outputs):\n",
                "        embedded =  self.dropout(self.embedding(input))\n",
                "        query = hidden.permute(1, 0, 2)\n",
                "        context, attn_weights = self.attention(query, encoder_outputs)\n",
                "        input_gru = torch.cat((embedded, context), dim=2)\n",
                "        output, hidden = self.gru(input_gru, hidden)\n",
                "        output = self.out(output)\n",
                "        return output, hidden, attn_weights\n",
                "\n",
                "class EncoderMetaModel(nn.Module):\n",
                "    def __init__(self, encoders_dict, decoders_dict):\n",
                "        super(EncoderMetaModel, self).__init__()\n",
                "        self.encoders_dict = nn.ModuleDict(encoders_dict)\n",
                "        self.decoders_dict = nn.ModuleDict(decoders_dict)\n",
                "        self.encoder_weights = nn.Parameter(torch.ones(len(encoders_dict)) / len(encoders_dict))\n",
                "        self.decoder_weights = nn.Parameter(torch.ones(len(decoders_dict)) / len(decoders_dict))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        encoder_combined_output = 0\n",
                "        for i, encoder in enumerate(self.encoders_dict.values()):\n",
                "            encoder_combined_output += self.encoder_weights[i] * encoder(x)\n",
                "        \n",
                "        decoder_combined_output = 0\n",
                "        for i, decoder in enumerate(self.decoders_dict.values()):\n",
                "            decoder_combined_output += self.decoder_weights[i] * decoder(encoder_combined_output)\n",
                "        \n",
                "        return decoder_combined_output"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1643b918",
            "metadata": {},
            "source": [
                "## Training and Evaluation\n",
                "Functions for training the model, evaluating performance, and calculating BLEU scores."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "37458a49",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
                "    total_loss = 0\n",
                "    for data in dataloader:\n",
                "        input_tensor, target_tensor, _ = data\n",
                "\n",
                "        encoder_optimizer.zero_grad()\n",
                "        decoder_optimizer.zero_grad()\n",
                "\n",
                "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
                "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
                "\n",
                "        loss = criterion(\n",
                "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
                "            target_tensor.view(-1)\n",
                "        )\n",
                "        loss.backward()\n",
                "\n",
                "        encoder_optimizer.step()\n",
                "        decoder_optimizer.step()\n",
                "\n",
                "        total_loss += loss.item()\n",
                "\n",
                "    return total_loss / len(dataloader)\n",
                "\n",
                "def train(train_dataloader, encoder, decoder, n_epochs, input, output, pairs, test_pairs=None, filename=None, learning_rate=0.001, print_every=100, plot_every=100):\n",
                "    start = time.time()\n",
                "    plot_losses = []\n",
                "    print_loss_total = 0  # Reset every print_every\n",
                "    plot_loss_total = 0  # Reset every plot_every\n",
                "\n",
                "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
                "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
                "    criterion = nn.NLLLoss()\n",
                "\n",
                "    for epoch in range(1, n_epochs + 1):\n",
                "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
                "        print_loss_total += loss\n",
                "        plot_loss_total += loss\n",
                "\n",
                "        if epoch % print_every == 0:\n",
                "            print_loss_avg = print_loss_total / print_every\n",
                "            print_loss_total = 0\n",
                "            \n",
                "            bleu = evaluateBleu(encoder, decoder, input, output, pairs, n=200)\n",
                "            test_bleu = 0\n",
                "            if test_pairs is not None:\n",
                "                test_bleu = evaluateBleu(encoder, decoder, input, output, test_pairs, n=min(200,len(test_pairs)))\n",
                "            \n",
                "            print('Epoch %d >> Loss: %s, Bleu: %s, Test Bleu: %s' % (epoch, print_loss_avg, bleu, test_bleu))\n",
                "            \n",
                "            if filename is not None:\n",
                "                with open(filename, 'a') as f:\n",
                "                    writer = csv.writer(f)\n",
                "                    writer.writerow([epoch, print_loss_avg, bleu, test_bleu])\n",
                "\n",
                "        if epoch % plot_every == 0:\n",
                "            plot_loss_avg = plot_loss_total / plot_every\n",
                "            plot_losses.append(plot_loss_avg)\n",
                "            plot_loss_total = 0\n",
                "\n",
                "    return encoder, decoder, test_pairs\n",
                "\n",
                "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
                "    with torch.no_grad():\n",
                "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
                "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
                "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
                "\n",
                "        _, topi = decoder_outputs.topk(1)\n",
                "        decoded_ids = topi.squeeze()\n",
                "\n",
                "        decoded_words = []\n",
                "        for idx in decoded_ids:\n",
                "            if idx.item() == EOS_token:\n",
                "                decoded_words.append('<EOS>')\n",
                "                break\n",
                "            decoded_words.append(output_lang.index2word[idx.item()])\n",
                "    return decoded_words, decoder_attn\n",
                "\n",
                "def evaluateBleu(encoder, decoder, input_lang, output_lang, pairs, n=10, verbose=False):\n",
                "    # Randomly select n pairs to evaluate\n",
                "    if n > len(pairs):\n",
                "        n = len(pairs)\n",
                "    \n",
                "    # Use random indices\n",
                "    numbers = random.sample(range(len(pairs)), n)\n",
                "    \n",
                "    bleu_sum = 0\n",
                "    sys = []\n",
                "    refs = []\n",
                "\n",
                "    for num in numbers:\n",
                "        pair = pairs[num]\n",
                "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
                "        output_sentence = ' '.join(output_words).split(' ')\n",
                "        if verbose:\n",
                "            print('>', pair[0])\n",
                "            print('=', pair[1])\n",
                "            print('<', output_sentence)\n",
                "            print('')\n",
                "        \n",
                "        # Prepare for sacrebleu\n",
                "        # Remove <EOS> if present\n",
                "        if '<EOS>' in output_sentence:\n",
                "            output_sentence.remove('<EOS>')\n",
                "        \n",
                "        sys.append(\" \".join(output_sentence))\n",
                "        refs.append(\" \".join(pair[1].split(' ')))\n",
                "    \n",
                "    # Calculate BLEU\n",
                "    bleu = sacrebleu.corpus_bleu(sys, [refs])\n",
                "    return bleu.score\n",
                "\n",
                "def evaluateMultipleBleu(encoders, decoders, input_output_langs, n=10):\n",
                "    bleu_sum = 0\n",
                "    bleu_test_sum = 0\n",
                "    lang_length = len(encoders)\n",
                "    bleus = []\n",
                "    test_bleus = []\n",
                "    for i in range(lang_length):\n",
                "        bleu = evaluateBleu(encoders[i], decoders[i], input_output_langs[i][0], input_output_langs[i][1], input_output_langs[i][2], n=n)\n",
                "        bleu_sum += bleu\n",
                "        bleus.append(bleu)\n",
                "    for i in range(lang_length):\n",
                "        test_bleu = evaluateBleu(encoders[i], decoders[i], input_output_langs[i][0], input_output_langs[i][1], input_output_langs[i][3], n=n)\n",
                "        bleu_test_sum += test_bleu\n",
                "        test_bleus.append(test_bleu)\n",
                "    return (bleu_sum / lang_length), (bleu_test_sum / lang_length), bleus, test_bleus\n",
                "\n",
                "def personalize(lang, rounds, encoder_weights=None, decoder_weights=None, sample=None, save=False, lang_info=None):\n",
                "    if lang_info is None:\n",
                "        input_lang, output_lang, train_dataloader, train_pairs = get_dataloader(batch_size=32, language=lang)\n",
                "    else:\n",
                "        input_lang, output_lang, train_dataloader, train_pairs = lang_info\n",
                "\n",
                "    if sample is not None:\n",
                "        train_dataloader = limited_data_loader(train_dataloader, sample, random=True)\n",
                "\n",
                "    # Get test data\n",
                "    test_pairs = train_pairs[int(len(train_pairs)*0.9):] # Simple 10% split for testing\n",
                "    train_pairs = train_pairs[:int(len(train_pairs)*0.9)]\n",
                "\n",
                "    hidden_size = 256\n",
                "    encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
                "    decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
                "\n",
                "    if encoder_weights is not None:\n",
                "        encoder.load_state_dict(encoder_weights)\n",
                "    if decoder_weights is not None:\n",
                "        decoder.load_state_dict(decoder_weights)\n",
                "\n",
                "    filename = None\n",
                "    if save:\n",
                "        is_FL = 0 if encoder_weights is None else 1\n",
                "        num = 1\n",
                "        filename = f\"P|{lang}_{sample}-shot_FL-{is_FL}_epoch{rounds}||{num}.csv\"\n",
                "        while os.path.isfile(filename):\n",
                "            num += 1\n",
                "            filename = f\"P|{lang}_{sample}-shot_FL-{is_FL}_epoch{rounds}||{num}.csv\"\n",
                "\n",
                "    train(train_dataloader, encoder, decoder, rounds, print_every=25, plot_every=5, filename=filename, input=input_lang, output=output_lang, pairs=train_pairs, test_pairs=test_pairs)\n",
                "    return encoder, decoder, test_pairs"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3ade4acc",
            "metadata": {},
            "source": [
                "## Federated Learning Functions\n",
                "Core logic for Federated Averaging."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "da7c71a8",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ClientUpdate(object):\n",
                "    def __init__(self, train_dataloader, learning_rate, epochs, sch_flag):\n",
                "        self.train_loader = train_dataloader\n",
                "        self.learning_rate = learning_rate\n",
                "        self.epochs = epochs\n",
                "        self.sch_flag = sch_flag\n",
                "\n",
                "    def train(self, encoder, decoder):\n",
                "        criterion = nn.NLLLoss()\n",
                "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=self.learning_rate)\n",
                "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=self.learning_rate)\n",
                "        \n",
                "        epoch_loss = []\n",
                "        num_pairs = 0\n",
                "        \n",
                "        for epoch in range(1, self.epochs + 1):\n",
                "            batch_loss = []\n",
                "            for data in self.train_loader:\n",
                "                input_tensor, target_tensor, _ = data\n",
                "                \n",
                "                encoder_optimizer.zero_grad()\n",
                "                decoder_optimizer.zero_grad()\n",
                "                \n",
                "                encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
                "                decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
                "                \n",
                "                loss = criterion(\n",
                "                    decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
                "                    target_tensor.view(-1)\n",
                "                )\n",
                "                loss.backward()\n",
                "                \n",
                "                encoder_optimizer.step()\n",
                "                decoder_optimizer.step()\n",
                "                \n",
                "                batch_loss.append(loss.item())\n",
                "                num_pairs += input_tensor.size(0)\n",
                "                \n",
                "            epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
                "            \n",
                "        return encoder.state_dict(), decoder.state_dict(), sum(epoch_loss) / len(epoch_loss), num_pairs\n",
                "\n",
                "def training(encoders, decoders, input_output_lang, rounds, lr, ds, C, K, E, filename=None, batch_size=None, hidden_size=None, weighting=False, lexical_weights=None):\n",
                "    if batch_size is None:\n",
                "        batch_size = 32\n",
                "        \n",
                "    # Setup for Optuna\n",
                "    input_lang_target, output_lang_target, target_dataloader_target, pairs_target = get_dataloader(batch_size, language='cat')\n",
                "    langs = ['bem', 'kin', 'swa']\n",
                "\n",
                "    # Global weights initialization\n",
                "    global_encoder_weights = {key: value for key, value in encoders[0].state_dict().items() if 'embedding' not in key}\n",
                "    global_decoder_weights = {key: value for key, value in decoders[0].state_dict().items() if 'embedding' not in key and 'out' not in key}\n",
                "    \n",
                "    train_loss = []\n",
                "    best_bleu = 0\n",
                "    best_test_bleu = 0\n",
                "    start = time.time()\n",
                "    \n",
                "    lex_weighting = False\n",
                "    if lexical_weights is not None:\n",
                "        lex_weighting = True\n",
                "    lexical_weights_encoder = lexical_weights\n",
                "    lexical_weights_decoder = lexical_weights\n",
                "    \n",
                "    if filename is not None:\n",
                "        with open(filename, 'a') as f:\n",
                "            writer = csv.writer(f)\n",
                "            writer.writerow(['Rounds', 'Learning Rate', 'Client Fraction', 'Client Number', 'Local Epochs', 'Batch Size', 'Hidden Size', 'Weighting', 'Lexical Weighting'])\n",
                "            writer.writerow([rounds, lr, C, K, E, batch_size, hidden_size, weighting, lex_weighting])\n",
                "    \n",
                "    for curr_round in range(1, rounds + 1):\n",
                "        w_encoder, w_decoder, local_loss, num_pairs = [], [], [], []\n",
                "        \n",
                "        m = max(int(C * K), 1)\n",
                "        idxs_users = np.random.choice(range(K), m, replace=False)\n",
                "        \n",
                "        for k in tqdm(idxs_users):\n",
                "            local_update = ClientUpdate(train_dataloader=ds[k], learning_rate=lr, epochs=E, sch_flag=False)\n",
                "            \n",
                "            e_og = encoders[k].state_dict()\n",
                "            d_og = decoders[k].state_dict()\n",
                "            e_og.update(global_encoder_weights)\n",
                "            d_og.update(global_decoder_weights)\n",
                "            encoders[k].load_state_dict(e_og)\n",
                "            decoders[k].load_state_dict(d_og)\n",
                "            \n",
                "            encoder_weights, decoder_weights, loss, num = local_update.train(encoders[k], decoders[k])\n",
                "            \n",
                "            w_encoder.append({key: value for key, value in copy.deepcopy(encoder_weights).items() if 'embedding' not in key})\n",
                "            w_decoder.append({key: value for key, value in copy.deepcopy(decoder_weights).items() if 'embedding' not in key and 'out' not in key})\n",
                "            local_loss.append(copy.deepcopy(loss))\n",
                "            num_pairs.append(num)\n",
                "            \n",
                "        # Optuna Optimization\n",
                "        if curr_round % 5 == 0 and curr_round >= 25:\n",
                "            # global_encoder_model, global_decoder_model, test_pairs = personalize('kir', 20, save=False, encoder_weights=global_encoder_weights, decoder_weights=global_decoder_weights, sample=None)\n",
                "            objective_full = partial(objective, encoder_models = encoders, decoder_models = decoders, input_lang=input_lang_target, output_lang=output_lang_target, target_dataloader=target_dataloader_target, target_pairs=pairs_target, test_pairs=None, sources = langs, device = device)\n",
                "\n",
                "            study = optuna.create_study(study_name=\"optimizing weights\", direction=\"maximize\", storage=f\"sqlite:///studies/{filename[:-4]}.db\", load_if_exists=True)\n",
                "            study.optimize(objective_full, n_trials=10)\n",
                "\n",
                "            # print(f\"ENCODERS: {proportions_encoder}, {proportions_decoder}\")\n",
                "            print(f\"PARAMS: {study.best_params}, VALUE: {study.best_value}\")\n",
                "            lexical_weights_encoder = []\n",
                "            lexical_weights_decoder = []\n",
                "            for i, (k,v) in enumerate(study.best_params.items()):\n",
                "                if i % 2 == 0:\n",
                "                    lexical_weights_encoder.append(v)\n",
                "                else:\n",
                "                    lexical_weights_decoder.append(v)\n",
                "            lexical_weights_encoder.append(100 - sum(lexical_weights_encoder))\n",
                "            lexical_weights_decoder.append(100 - sum(lexical_weights_decoder))\n",
                "            \n",
                "            # Enable lexical weighting after optimization\n",
                "            lex_weighting = True\n",
                "\n",
                "        # Updating global weights\n",
                "        weights_avg_e = copy.deepcopy(w_encoder[0])\n",
                "        weights_sum = sum(num_pairs)\n",
                "        client_weights = [weight / weights_sum for weight in num_pairs]\n",
                "        client_weights_encoder = lexical_weights_encoder\n",
                "        client_weights_decoder = lexical_weights_decoder\n",
                "        \n",
                "        if weighting:\n",
                "            client_weights_encoder = client_weights_decoder = client_weights\n",
                "            \n",
                "        if lex_weighting:\n",
                "            if weighting:\n",
                "                 # Placeholder for complex weighting if needed, but for now using the logic that matches Optuna output usage\n",
                "                 pass\n",
                "            else:\n",
                "                # Normalize weights if they are not already (Optuna weights sum to 100 approx?)\n",
                "                # The Optuna objective ensures they sum to 100 via logic, but here we might need to be careful.\n",
                "                # The original code had:\n",
                "                client_weights_encoder = [0.25 + 0.25 * (weight / sum(lexical_weights_encoder)) for weight in lexical_weights_encoder]\n",
                "                client_weights_decoder = [0.25 + 0.25 * (weight / sum(lexical_weights_decoder)) for weight in lexical_weights_decoder]\n",
                "        \n",
                "        # print(f\"Encoder:{client_weights_encoder}, Decoder:{client_weights_decoder}\")\n",
                "\n",
                "        for k in weights_avg_e.keys():\n",
                "            if weighting or lex_weighting:\n",
                "                weights_avg_e[k] *= client_weights_encoder[0]\n",
                "                for i in range(1, len(w_encoder)):\n",
                "                    weights_avg_e[k] += w_encoder[i][k] * client_weights_encoder[i]\n",
                "            else:\n",
                "                for i in range(1, len(w_encoder)):\n",
                "                    weights_avg_e[k] += w_encoder[i][k]\n",
                "                weights_avg_e[k] = torch.div(weights_avg_e[k], len(w_encoder))\n",
                "            \n",
                "        global_encoder_weights = weights_avg_e\n",
                "        \n",
                "        weights_avg_d = copy.deepcopy(w_decoder[0])\n",
                "        for k in weights_avg_d.keys():\n",
                "            if weighting or lex_weighting:\n",
                "                weights_avg_d[k] *= client_weights_decoder[0]\n",
                "                for i in range(1, len(w_decoder)):\n",
                "                    weights_avg_d[k] += w_decoder[i][k] * client_weights_decoder[i]\n",
                "            else:\n",
                "                for i in range(1, len(w_decoder)):\n",
                "                    weights_avg_d[k] += w_decoder[i][k]\n",
                "                weights_avg_d[k] = torch.div(weights_avg_d[k], len(w_decoder))\n",
                "            \n",
                "        global_decoder_weights = weights_avg_d\n",
                "        \n",
                "        loss_avg = sum(local_loss) / len(local_loss)\n",
                "        train_loss.append(loss_avg)\n",
                "        \n",
                "        bleu, test_bleu, bleu_list, test_bleu_list = evaluateMultipleBleu(encoders, decoders, input_output_lang, n=200)\n",
                "        \n",
                "        if best_bleu < bleu:\n",
                "            best_bleu = bleu\n",
                "        if best_test_bleu < test_bleu:\n",
                "            best_test_bleu = test_bleu\n",
                "            \n",
                "        print(f\"Round {curr_round} >> Loss: {loss_avg}, BLEU:{bleu}, TEST:{test_bleu}\")\n",
                "        \n",
                "        if filename is not None:\n",
                "            with open(filename, 'a') as f:\n",
                "                writer = csv.writer(f)\n",
                "                writer.writerow([curr_round, loss_avg, bleu, best_bleu, test_bleu, best_test_bleu, str(bleu_list), str(test_bleu_list)])\n",
                "                \n",
                "    end = time.time()\n",
                "    print(\"Training Done!\")\n",
                "    print(\"Total time taken to Train: {}\".format(end - start))\n",
                "    \n",
                "    return global_encoder_weights, global_decoder_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "fd5e39a0",
            "metadata": {},
            "outputs": [],
            "source": [
                "def objective(trial, encoder_models, decoder_models, input_lang, output_lang, target_dataloader, target_pairs, test_pairs, sources, device):\n",
                "    batch_size = 32\n",
                "    hidden_size = 256\n",
                "    \n",
                "    STEP_SIZE = 1\n",
                "\n",
                "    encoder_weights = []\n",
                "    decoder_weights = []\n",
                "\n",
                "    # This is what we fine-tune for\n",
                "    proportions_encoder = torch.ones(len(encoder_models))\n",
                "    proportions_decoder = torch.ones(len(encoder_models))\n",
                "\n",
                "    # Reset Heads\n",
                "    global_encoder_model = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
                "    global_decoder_model = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
                "        \n",
                "    # Combine Decoder and Encoders\n",
                "    # Make a list of encoder and decoder weights\n",
                "    for i in range(len(encoder_models)):\n",
                "        encoder_weights.append({key: value for key, value in copy.deepcopy(encoder_models[i].state_dict()).items() if 'embedding' not in key})\n",
                "        decoder_weights.append({key: value for key, value in copy.deepcopy(decoder_models[i].state_dict()).items() if 'embedding' not in key and 'out' not in key})\n",
                "    \n",
                "    weights_avg_e = copy.deepcopy(encoder_weights[0])\n",
                "    weights_avg_d = copy.deepcopy(decoder_weights[0])\n",
                "\n",
                "    # Make state_dict empty with zeroes\n",
                "    for k in weights_avg_e:\n",
                "        weights_avg_e[k].zero_()\n",
                "    for k in weights_avg_d:\n",
                "        weights_avg_d[k].zero_()\n",
                "    \n",
                "    upper_limit_decoder = upper_limit_encoder = 100\n",
                "\n",
                "    for i, source_lang in enumerate(sources[:-1]):\n",
                "        proportions_encoder[i] = trial.suggest_int(f\"{source_lang}_encoder\", 0, upper_limit_encoder, step=STEP_SIZE)\n",
                "        proportions_decoder[i] = trial.suggest_int(f\"{source_lang}_decoder\", 0, upper_limit_decoder, step=STEP_SIZE)\n",
                "        upper_limit_encoder -= proportions_encoder[i]\n",
                "        upper_limit_decoder -= proportions_decoder[i]\n",
                "    proportions_encoder[-1] = 100 - upper_limit_encoder\n",
                "    proportions_decoder[-1] = 100 - upper_limit_decoder\n",
                "        \n",
                "    # Add and combine weights of all input encoders/decoders according to proportion\n",
                "    for k in weights_avg_e.keys():\n",
                "        for i in range(0, len(encoder_weights)):\n",
                "            weights_avg_e[k] += proportions_encoder[i] * encoder_weights[i][k] / 100\n",
                "    \n",
                "    for k in weights_avg_d.keys():\n",
                "        for i in range(0, len(decoder_weights)):\n",
                "            \n",
                "            weights_avg_d[k] += proportions_decoder[i] * decoder_weights[i][k] / 100\n",
                "\n",
                "    # Aggregate all encoders and decoders to update globals\n",
                "    encoder_og = global_encoder_model.state_dict()\n",
                "    encoder_og.update(weights_avg_e)\n",
                "    global_encoder_model.load_state_dict(encoder_og)\n",
                "\n",
                "    decoder_og = global_decoder_model.state_dict()\n",
                "    decoder_og.update(weights_avg_d)\n",
                "    global_decoder_model.load_state_dict(decoder_og)\n",
                "\n",
                "    # Train head for target lang\n",
                "    global_encoder_model, global_decoder_model, test_pairs = personalize('kir', 10, save=False, encoder_weights=global_encoder_model.state_dict(), decoder_weights=global_decoder_model.state_dict(), sample=None, lang_info=(input_lang, output_lang, target_dataloader, target_pairs))\n",
                "\n",
                "    #REPLACE TEST PAIRS\n",
                "    test_bleu = evaluateBleu(global_encoder_model, global_decoder_model, input_lang, output_lang, test_pairs, n=min(100,len(test_pairs)))\n",
                "\n",
                "    return test_bleu"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0ddd70a6",
            "metadata": {},
            "source": [
                "## Experiment Management\n",
                "Functions to setup and run experiments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "584dba41",
            "metadata": {},
            "outputs": [],
            "source": [
                "def setup_experiment(languages, batch_size=32, hidden_size=256):\n",
                "    data_dict = {}\n",
                "    encoders = {}\n",
                "    decoders = {}\n",
                "    input_output_lang = {}\n",
                "    \n",
                "    for i, lang in enumerate(languages):\n",
                "        print(f\"Setting up {lang}...\")\n",
                "        input_lang, output_lang, train_dataloader, pairs = get_dataloader(batch_size, language=lang)\n",
                "        train_dataloader, test_dataloader, split = split_dataloader(train_dataloader)\n",
                "        train_idx = get_pair_index(train_dataloader)\n",
                "        test_idx = get_pair_index(test_dataloader)\n",
                "        \n",
                "        train_pairs = [pairs[i] for i in train_idx]\n",
                "        test_pairs = [pairs[i] for i in test_idx]\n",
                "        \n",
                "        encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
                "        decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
                "        \n",
                "        encoders[i] = encoder\n",
                "        decoders[i] = decoder\n",
                "        data_dict[i] = train_dataloader\n",
                "        input_output_lang[i] = (input_lang, output_lang, train_pairs, test_pairs)\n",
                "        \n",
                "    return encoders, decoders, data_dict, input_output_lang\n",
                "\n",
                "def run_experiment(languages, rounds=10):\n",
                "    batch_size = 32\n",
                "    hidden_size = 256\n",
                "    \n",
                "    encoders, decoders, data_dict, input_output_lang = setup_experiment(languages, batch_size, hidden_size)\n",
                "    \n",
                "    # Run Federated Learning Training\n",
                "    print(\"Starting Federated Learning...\")\n",
                "    training(encoders, decoders, input_output_lang, rounds, lr=0.001, ds=data_dict, C=1.0, K=len(languages), E=1, filename=\"experiment_log.csv\", batch_size=batch_size, hidden_size=hidden_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "696ac7f4",
            "metadata": {},
            "source": [
                "## Main Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ad64bf5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Setting up bem...\n",
                        "Reading lines...\n",
                        "Read 82370 sentence pairs\n",
                        "Trimmed to 71769 sentence pairs\n",
                        "Counting words...\n",
                        "Counted words:\n",
                        "bem 55089\n",
                        "eng 11061\n",
                        "Setting up kin...\n",
                        "Reading lines...\n",
                        "Read 55667 sentence pairs\n",
                        "Trimmed to 46273 sentence pairs\n",
                        "Counting words...\n",
                        "Counted words:\n",
                        "kin 85445\n",
                        "eng 17897\n",
                        "Setting up swa...\n",
                        "Reading lines...\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/seq2seq/lib/python3.14/site-packages/zmq/backend/cython/_zmq.py:179\u001b[39m, in \u001b[36mzmq.backend.cython._zmq._check_rc\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"internal utility for checking zmq return condition\u001b[39;00m\n\u001b[32m    175\u001b[39m \n\u001b[32m    176\u001b[39m \u001b[33;03mand raising the appropriate Exception class\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    178\u001b[39m errno: C.int = _zmq_errno()\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m PyErr_CheckSignals()\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errno == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m error_without_errno:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Exception ignored in: 'zmq.backend.cython._zmq.Frame.__dealloc__'\n",
                        "Traceback (most recent call last):\n",
                        "  File \"zmq/backend/cython/_zmq.py\", line 179, in zmq.backend.cython._zmq._check_rc\n",
                        "KeyboardInterrupt: \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Read 272543 sentence pairs\n",
                        "Trimmed to 242315 sentence pairs\n",
                        "Counting words...\n",
                        "Counted words:\n",
                        "swa 55715\n",
                        "eng 31494\n",
                        "Starting Federated Learning...\n",
                        "Reading lines...\n",
                        "Read 1375 sentence pairs\n",
                        "Trimmed to 1373 sentence pairs\n",
                        "Counting words...\n",
                        "Counted words:\n",
                        "cat 1815\n",
                        "eng 1464\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/3 [00:00<?, ?it/s]"
                    ]
                }
            ],
            "source": [
                "# Define languages to experiment with\n",
                "langs = ['bem', 'kin', 'swa'] # Example languages\n",
                "\n",
                "# Run the experiment\n",
                "run_experiment(langs, rounds=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2db808ab",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
