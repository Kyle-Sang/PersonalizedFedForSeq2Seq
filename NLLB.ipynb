{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"run_Latn\", tgt_lang=\"eng_Latn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model is your loaded NLLB model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Specify quantization configuration\n",
    "quantization_config = torch.quantization.get_default_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615073792"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:215: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100SdpaAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm(\n",
       "            (1024,), eps=1e-05, elementwise_affine=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(\n",
       "            in_features=1024, out_features=4096, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (fc2): Linear(\n",
       "            in_features=4096, out_features=1024, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm(\n",
       "            (1024,), eps=1e-05, elementwise_affine=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm(\n",
       "        (1024,), eps=1e-05, elementwise_affine=True\n",
       "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100SdpaAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm(\n",
       "            (1024,), eps=1e-05, elementwise_affine=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (encoder_attn): M2M100SdpaAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1024, out_features=1024, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm(\n",
       "            (1024,), eps=1e-05, elementwise_affine=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (fc1): Linear(\n",
       "            in_features=1024, out_features=4096, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (fc2): Linear(\n",
       "            in_features=4096, out_features=1024, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm(\n",
       "            (1024,), eps=1e-05, elementwise_affine=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm(\n",
       "        (1024,), eps=1e-05, elementwise_affine=True\n",
       "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(\n",
       "    in_features=1024, out_features=256206, bias=False\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the configuration to the model\n",
    "model.qconfig = quantization_config\n",
    "torch.quantization.prepare(model, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615073792"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params_quantized = sum(p.numel() for p in model.parameters())\n",
    "total_params_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = \"run_Latn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def word_tokenize(text):\n",
    "    \"\"\"\n",
    "    Split a text into words, numbers, and punctuation marks\n",
    "    (for languages where words are separated by spaces)\n",
    "    \"\"\"\n",
    "    return re.findall('(\\w+|[^\\w\\s])', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Muennighoff/flores200\", 'eng_Latn-run_Latn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = dataset['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flores_train = pd.DataFrame(([dataset['sentence_eng_Latn'], dataset['sentence_run_Latn']]))\n",
    "flores_train = flores_train.T\n",
    "flores_train.columns = ['eng', 'run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38de4b8bd9e4808a734485106bb7eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "['Itangazo ry’uyu munsi ryahaye inguvu irindi Leta yashizeho mu kwa gatatu kw’uno mwaka ryo kwongereza izindi modoka.', \"Mu 1977, Dogoteri Damadian yararangije kwubaka sikaneri ya mbere ya IRM “y'umubiri-wose”, ayita ”Mutananirwa”.\", \"Ibibanza vyo mu bice bizwi cane nka Bright Angel Campgound iri hafi ya Phantom Ranch, mu bisanzwe bifatwa vyose n'ababisaba kw’itariki ya mbere batangurirako kubikisha ibibanza.\", 'Iyo raporo yerekanye ingene amanota y’ibibazo yaduze cane ku rugero rutangaje ko kandi ishure ryabibonye ntirikore na kimwe.', 'Umushikiranganji w’amagara y’abantu yavuze ko atewe impungenge n’abantu bariko bakoresha mategeko y’agateganyo kubijanye n’ukubaho kwabantu ku gatwe kabo, ndetse n’ibihano bifitanye isano n’ibiyovyabwenge vyatanzwe kuva habaye ihinduka rishingiye ku mategekomashasha.']\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "texts_with_unk = [\n",
    "    text for text in tqdm(flores_train.run) \n",
    "    if tokenizer.unk_token_id in tokenizer(text).input_ids\n",
    "]\n",
    "print(len(texts_with_unk))\n",
    "# 163\n",
    "s = random.sample(texts_with_unk, 5)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "\n",
    "mpn = MosesPunctNormalizer(lang=\"en\")\n",
    "mpn.substitutions = [\n",
    "    (re.compile(r), sub) for r, sub in mpn.substitutions\n",
    "]\n",
    "\n",
    "def get_non_printing_char_replacer(replace_by: str = \" \"):\n",
    "    non_printable_map = {\n",
    "        ord(c): replace_by\n",
    "        for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
    "        # same as \\p{C} in perl\n",
    "        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n",
    "    }\n",
    "\n",
    "    def replace_non_printing_char(line) -> str:\n",
    "        return line.translate(non_printable_map)\n",
    "\n",
    "    return replace_non_printing_char\n",
    "\n",
    "replace_nonprint = get_non_printing_char_replacer(\" \")\n",
    "\n",
    "def preproc(text):\n",
    "    clean = mpn.normalize(text)\n",
    "    clean = replace_nonprint(clean)\n",
    "    # replace 𝓕𝔯𝔞𝔫𝔠𝔢𝔰𝔠𝔞 by Francesca\n",
    "    clean = unicodedata.normalize(\"NFKC\", clean)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:00<00:00, 6139.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "texts_with_unk_normed = [\n",
    "    text for text in tqdm(texts_with_unk) \n",
    "    if tokenizer.unk_token_id in tokenizer(preproc(text)).input_ids\n",
    "]\n",
    "print(len(texts_with_unk_normed))  # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "model.cuda();\n",
    "def get_optimizer():\n",
    "    optimizer = Adafactor(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        scale_parameter=False,\n",
    "        relative_step=False,\n",
    "        lr=1e-5,\n",
    "        clip_threshold=1.0,\n",
    "        weight_decay=1e-3,\n",
    "    )\n",
    "    scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On Monday, scientists from the Stanford Univer...</td>\n",
       "      <td>Ku wa mbere, abahinga bo kuri kaminuza yitwa S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lead researchers say this may bring early dete...</td>\n",
       "      <td>Abashakashatsi nyamukuru bavuga ko ako gakores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The JAS 39C Gripen crashed onto a runway at ar...</td>\n",
       "      <td>Isaha 9:30 zo mu gitondo (0230 UTC), iyo ndege...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pilot was identified as Squadron Leader Di...</td>\n",
       "      <td>Basanze umudereva yari Dilokrit Pattavee, umuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Local media reports an airport fire vehicle ro...</td>\n",
       "      <td>Ibimenyeshamakuru vyaho bivuga ko hari kizimya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>The tourist season for the hill stations gener...</td>\n",
       "      <td>Igihe c'ingenzi mu mahuriro yo mu misozi mu bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>However, they have a different kind of beauty ...</td>\n",
       "      <td>N'aho biri ukwo, bifise ubwoko butandukanye bw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Only a few airlines still offer bereavement fa...</td>\n",
       "      <td>Amashirahamwe y'ivyindege amwe gusa niyo azota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Airlines that offer these include Air Canada, ...</td>\n",
       "      <td>Amashirahamwe y'ivyindege atanga ivyo harimwo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>In all cases, you must book by phone directly ...</td>\n",
       "      <td>Ibihe vyose, utegerezwa kubikisha ikibanza nin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   eng  \\\n",
       "0    On Monday, scientists from the Stanford Univer...   \n",
       "1    Lead researchers say this may bring early dete...   \n",
       "2    The JAS 39C Gripen crashed onto a runway at ar...   \n",
       "3    The pilot was identified as Squadron Leader Di...   \n",
       "4    Local media reports an airport fire vehicle ro...   \n",
       "..                                                 ...   \n",
       "992  The tourist season for the hill stations gener...   \n",
       "993  However, they have a different kind of beauty ...   \n",
       "994  Only a few airlines still offer bereavement fa...   \n",
       "995  Airlines that offer these include Air Canada, ...   \n",
       "996  In all cases, you must book by phone directly ...   \n",
       "\n",
       "                                                   run  \n",
       "0    Ku wa mbere, abahinga bo kuri kaminuza yitwa S...  \n",
       "1    Abashakashatsi nyamukuru bavuga ko ako gakores...  \n",
       "2    Isaha 9:30 zo mu gitondo (0230 UTC), iyo ndege...  \n",
       "3    Basanze umudereva yari Dilokrit Pattavee, umuk...  \n",
       "4    Ibimenyeshamakuru vyaho bivuga ko hari kizimya...  \n",
       "..                                                 ...  \n",
       "992  Igihe c'ingenzi mu mahuriro yo mu misozi mu bi...  \n",
       "993  N'aho biri ukwo, bifise ubwoko butandukanye bw...  \n",
       "994  Amashirahamwe y'ivyindege amwe gusa niyo azota...  \n",
       "995  Amashirahamwe y'ivyindege atanga ivyo harimwo ...  \n",
       "996  Ibihe vyose, utegerezwa kubikisha ikibanza nin...  \n",
       "\n",
       "[997 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([\"Ishirahamwe Virgin Group, rya Richard Branson, ryaciye ryankirwa, imbere y'uko iyo banki ishirwa mu minwe ya reta.\"], [\"Sir Richard Branson's Virgin Group had a bid for the bank rejected prior to the bank's nationalisation.\"], 'run_Latn', 'eng_Latn')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "LANGS = [('eng', 'eng_Latn'), ('run', 'run_Latn')]\n",
    "\n",
    "def get_batch_pairs(batch_size, data=flores_train, langs=LANGS):\n",
    "    (l1, long1), (l2, long2) = random.sample(langs, 2)\n",
    "    xx, yy = [], []\n",
    "    for _ in range(batch_size):\n",
    "        item = data.iloc[random.randint(0, len(data)-1)]\n",
    "        xx.append(preproc(item[l1]))\n",
    "        yy.append(preproc(item[l2]))\n",
    "    return xx, yy, long1, long2\n",
    "\n",
    "print(get_batch_pairs(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def get_dataset(lang):\n",
    "    dataset = load_dataset(\"Muennighoff/flores200\", f'eng_Latn-{lang}_Latn')\n",
    "    dataset = dataset['dev']\n",
    "    dataset = pd.DataFrame(([dataset['sentence_eng_Latn'], dataset[f'sentence_{lang}_Latn']]))\n",
    "    dataset = dataset.T\n",
    "    dataset.columns = ['eng', lang]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Try to free GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # 32 already doesn't fit well to 15GB of GPU memory\n",
    "max_length = 128  # token sequences will be truncated\n",
    "training_steps = 100  # Usually, I set a large number of steps,\n",
    "# and then just interrupt the training manually\n",
    "losses = []  # with this list, I do very simple tracking of average loss\n",
    "MODEL_SAVE_PATH = './NLLB/nllb-eng-kir-v1'  # on my Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = []\n",
    "schedulers = []\n",
    "for i in range(3):\n",
    "    optimizer, scheduler = get_optimizer()\n",
    "    optimizers.append(optimizer)\n",
    "    schedulers.append(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "models = [copy.deepcopy(model) for i in range(3)]\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name, src_lang=\"bem_Latn\", tgt_lang=\"eng_Latn\")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name, src_lang=\"kin_Latn\", tgt_lang=\"eng_Latn\")\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(model_name, src_lang=\"lug_Latn\", tgt_lang=\"eng_Latn\")\n",
    "\n",
    "tokenizers = [tokenizer1, tokenizer2, tokenizer3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['bem', 'kin', 'lug']\n",
    "model_langs = [[('eng', 'eng_Latn'), ('bem', 'bem_Latn')], [('eng', 'eng_Latn'), ('kin', 'kin_Latn')], [('eng', 'eng_Latn'), ('lug', 'lug_Latn')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = [get_dataset(x) for x in ['bem', 'kin', 'lug']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d007da23580400da3c5f34572a34536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [2.282008330027262]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [2.282008330027262, 9.868823687235514, 167.23800659179688, 1172.4419962565105, 6177.994791666667, 20783.942057291668, 63603.928385416664, 193023.86979166666, 579949.75, 1728642.6666666667, 5183393.5]\n",
      "2 [2.282008330027262, 9.868823687235514, 167.23800659179688, 1172.4419962565105, 6177.994791666667, 20783.942057291668, 63603.928385416664, 193023.86979166666, 579949.75, 1728642.6666666667, 5183393.5, 15443793.666666666, 46623834.666666664, 138769333.33333334, 415407658.6666667, 1259567744.0, 3767256405.3333335, 11337098240.0, 34228019882.666668, 102652581205.33333, 305526876842.6667]\n",
      "2 [2.282008330027262, 9.868823687235514, 167.23800659179688, 1172.4419962565105, 6177.994791666667, 20783.942057291668, 63603.928385416664, 193023.86979166666, 579949.75, 1728642.6666666667, 5183393.5, 15443793.666666666, 46623834.666666664, 138769333.33333334, 415407658.6666667, 1259567744.0, 3767256405.3333335, 11337098240.0, 34228019882.666668, 102652581205.33333, 305526876842.6667, 916294270976.0, 2773945505109.3335, 8329441749674.667, 24677127465642.668, 73724816523264.0, 224097339790677.34, 665661367211349.4, 2003980603359232.0, 6035325446346069.0, 1.817454925001523e+16]\n",
      "2 [2.282008330027262, 9.868823687235514, 167.23800659179688, 1172.4419962565105, 6177.994791666667, 20783.942057291668, 63603.928385416664, 193023.86979166666, 579949.75, 1728642.6666666667, 5183393.5, 15443793.666666666, 46623834.666666664, 138769333.33333334, 415407658.6666667, 1259567744.0, 3767256405.3333335, 11337098240.0, 34228019882.666668, 102652581205.33333, 305526876842.6667, 916294270976.0, 2773945505109.3335, 8329441749674.667, 24677127465642.668, 73724816523264.0, 224097339790677.34, 665661367211349.4, 2003980603359232.0, 6035325446346069.0, 1.817454925001523e+16, 5.443586291348275e+16, 1.6312834400697277e+17, 4.860631096090078e+17, 1.4655960858283213e+18, 4.42659743791649e+18, 6.385100188430281e+17, 1.4821036952779052e+18, 4.4401056712714706e+18, nan, nan]\n",
      "2 [2.282008330027262, 9.868823687235514, 167.23800659179688, 1172.4419962565105, 6177.994791666667, 20783.942057291668, 63603.928385416664, 193023.86979166666, 579949.75, 1728642.6666666667, 5183393.5, 15443793.666666666, 46623834.666666664, 138769333.33333334, 415407658.6666667, 1259567744.0, 3767256405.3333335, 11337098240.0, 34228019882.666668, 102652581205.33333, 305526876842.6667, 916294270976.0, 2773945505109.3335, 8329441749674.667, 24677127465642.668, 73724816523264.0, 224097339790677.34, 665661367211349.4, 2003980603359232.0, 6035325446346069.0, 1.817454925001523e+16, 5.443586291348275e+16, 1.6312834400697277e+17, 4.860631096090078e+17, 1.4655960858283213e+18, 4.42659743791649e+18, 6.385100188430281e+17, 1.4821036952779052e+18, 4.4401056712714706e+18, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "2 [2.282008330027262, 9.868823687235514, 167.23800659179688, 1172.4419962565105, 6177.994791666667, 20783.942057291668, 63603.928385416664, 193023.86979166666, 579949.75, 1728642.6666666667, 5183393.5, 15443793.666666666, 46623834.666666664, 138769333.33333334, 415407658.6666667, 1259567744.0, 3767256405.3333335, 11337098240.0, 34228019882.666668, 102652581205.33333, 305526876842.6667, 916294270976.0, 2773945505109.3335, 8329441749674.667, 24677127465642.668, 73724816523264.0, 224097339790677.34, 665661367211349.4, 2003980603359232.0, 6035325446346069.0, 1.817454925001523e+16, 5.443586291348275e+16, 1.6312834400697277e+17, 4.860631096090078e+17, 1.4655960858283213e+18, 4.42659743791649e+18, 6.385100188430281e+17, 1.4821036952779052e+18, 4.4401056712714706e+18, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m y\u001b[38;5;241m.\u001b[39minput_ids[y\u001b[38;5;241m.\u001b[39minput_ids \u001b[38;5;241m==\u001b[39m tokenizers[i]\u001b[38;5;241m.\u001b[39mpad_token_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m models[i](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mx, labels\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39minput_ids)\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m local_loss\u001b[38;5;241m.\u001b[39mappend(copy\u001b[38;5;241m.\u001b[39mdeepcopy(loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     29\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(models[i]\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:503\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    495\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    496\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    502\u001b[0m     )\n\u001b[0;32m--> 503\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:254\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in range(3):\n",
    "    models[i].train()\n",
    "x, y, loss = None, None, None\n",
    "cleanup()\n",
    "\n",
    "global_weights = models[0].state_dict()\n",
    "\n",
    "tq = trange(len(losses), training_steps)\n",
    "for n in tq:\n",
    "    w, local_loss = [], 0\n",
    "    for i in range(3):\n",
    "        models[i].load_state_dict(global_weights)\n",
    "        xx, yy, lang1, lang2 = get_batch_pairs(batch_size, data=model_data[i], langs=model_langs[i])\n",
    "        try:\n",
    "            tokenizers[i].src_lang = lang1\n",
    "            x = tokenizers[i](xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(models[i].device)\n",
    "            tokenizers[i].src_lang = lang2\n",
    "            y = tokenizers[i](yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(models[i].device)\n",
    "            # -100 is a magic value ignored in the loss function\n",
    "            # because we don't want the model to learn to predict padding ids\n",
    "            y.input_ids[y.input_ids == tokenizers[i].pad_token_id] = -100\n",
    "\n",
    "            loss = models[i](**x, labels=y.input_ids).loss\n",
    "            loss.backward()\n",
    "            local_loss += loss.item()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(models[i].parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizers[i].step()\n",
    "            optimizers[i].zero_grad(set_to_none=True)\n",
    "            schedulers[i].step()\n",
    "\n",
    "        except RuntimeError as e:  # usually, it is out-of-memory\n",
    "            optimizers[i].zero_grad(set_to_none=True)\n",
    "            x, y, loss = None, None, None\n",
    "            cleanup()\n",
    "            print('error', max(len(s) for s in xx + yy), e)\n",
    "            continue\n",
    "\n",
    "        w.append(copy.deepcopy(models[i].state_dict()))\n",
    "        \n",
    "        if n % 10 == 0 and n > 0:\n",
    "            model.save_pretrained(f'./NLLB/{names[i]}')\n",
    "            tokenizers[i].save_pretrained(f'./NLLB/{names[i]}')\n",
    "\n",
    "    weights_avg = copy.deepcopy(w[0])\n",
    "    for k in weights_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            weights_avg[k] += w[i][k]\n",
    "\n",
    "        weights_avg[k] = torch.div(weights_avg[k], len(w))\n",
    "    \n",
    "    global_weights = weights_avg\n",
    "\n",
    "    losses = [(local_loss / 3)]\n",
    "\n",
    "    if n % 1 == 0:\n",
    "        # each 1000 steps, I report average loss at these steps\n",
    "        print(i, losses)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.shared.weight',\n",
       "              tensor([[-inf, inf, inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf]])),\n",
       "             ('model.encoder.embed_tokens.weight',\n",
       "              tensor([[-inf, inf, inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf]])),\n",
       "             ('model.encoder.layers.0.self_attn.k_proj.weight',\n",
       "              tensor([[ 5.9570e-01,  6.5479e-01,  5.9863e-01,  ...,  1.2024e-01,\n",
       "                       -1.4905e-01, -1.1023e-01],\n",
       "                      [ 3.1909e-01, -1.8079e-01, -2.6294e-01,  ..., -2.0264e-01,\n",
       "                        1.9861e-01,  4.2529e-01],\n",
       "                      [ 5.2148e-01,  8.4229e-01,  9.9951e-01,  ...,  2.0679e-01,\n",
       "                        2.0203e-01, -5.2261e-03],\n",
       "                      ...,\n",
       "                      [ 1.3794e-01, -1.1102e-01, -5.5176e-01,  ..., -3.5400e-03,\n",
       "                        4.2267e-02, -3.9399e-05],\n",
       "                      [-1.5588e-01,  6.3293e-02,  1.2901e-02,  ..., -3.8452e-01,\n",
       "                        1.5308e-01,  2.8540e-01],\n",
       "                      [-2.2595e-01, -9.3994e-02,  3.0981e-01,  ..., -1.6748e-01,\n",
       "                       -7.5378e-02, -1.4185e-01]])),\n",
       "             ('model.encoder.layers.0.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0179,  0.0283,  0.0196,  ..., -0.0025, -0.0250,  0.0193])),\n",
       "             ('model.encoder.layers.0.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0630,  0.0343, -0.0231,  ..., -0.0668, -0.1157, -0.0081],\n",
       "                      [ 0.0947,  0.0685,  0.0461,  ..., -0.0442, -0.1248, -0.0464],\n",
       "                      [ 0.0397,  0.0829,  0.0338,  ...,  0.0314, -0.0782, -0.1476],\n",
       "                      ...,\n",
       "                      [-0.2808, -0.0279,  0.0878,  ...,  0.0007, -0.0660, -0.0214],\n",
       "                      [ 0.0296,  0.0190, -0.0400,  ...,  0.0379, -0.0027, -0.2717],\n",
       "                      [-0.0427, -0.0203, -0.0764,  ...,  0.0536, -0.0583,  0.1981]])),\n",
       "             ('model.encoder.layers.0.self_attn.v_proj.bias',\n",
       "              tensor([-0.0758, -0.1368, -0.0085,  ..., -0.0319,  0.0159, -0.0054])),\n",
       "             ('model.encoder.layers.0.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.1394, -0.0477, -0.5269,  ..., -0.0635,  0.2659,  0.1715],\n",
       "                      [-0.2218, -0.3704, -0.5278,  ..., -0.2737, -0.0536, -0.3032],\n",
       "                      [ 1.0117,  1.0215,  0.9995,  ..., -0.0361, -0.1837,  0.1039],\n",
       "                      ...,\n",
       "                      [-0.1182, -0.1311, -0.4141,  ...,  0.3547, -0.1899,  0.1781],\n",
       "                      [-0.1536,  0.0272,  0.0038,  ...,  0.2732, -0.1216,  0.3218],\n",
       "                      [-0.2279, -0.1813,  0.2534,  ...,  0.2791, -0.2869, -0.0817]])),\n",
       "             ('model.encoder.layers.0.self_attn.q_proj.bias',\n",
       "              tensor([ 0.0445, -0.6504,  0.0458,  ..., -0.0200, -0.2649, -0.0813])),\n",
       "             ('model.encoder.layers.0.self_attn.out_proj.weight',\n",
       "              tensor([[-0.0432, -0.1025, -0.0201,  ...,  0.0115, -0.0321, -0.1892],\n",
       "                      [-0.0038, -0.0169, -0.0073,  ...,  0.0214,  0.0050,  0.0173],\n",
       "                      [ 0.0013, -0.0362,  0.0130,  ..., -0.0028, -0.0018,  0.0048],\n",
       "                      ...,\n",
       "                      [-0.0644,  0.3677,  0.1345,  ...,  0.2361, -0.3904,  0.1660],\n",
       "                      [ 0.0199, -0.1136, -0.2612,  ..., -0.1603,  0.0699, -0.0349],\n",
       "                      [-0.1918, -0.1236, -0.0550,  ...,  0.0294,  0.1755,  0.0011]])),\n",
       "             ('model.encoder.layers.0.self_attn.out_proj.bias',\n",
       "              tensor([-1.2274e-01, -5.3802e-02, -8.2552e-05,  ..., -1.1182e-01,\n",
       "                       1.1978e-02, -4.8340e-02])),\n",
       "             ('model.encoder.layers.0.self_attn_layer_norm.weight',\n",
       "              tensor([0.1989, 0.7793, 1.0020,  ..., 0.0681, 0.1104, 0.1191])),\n",
       "             ('model.encoder.layers.0.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0027,  0.0109,  0.0106,  ..., -0.0605,  0.0066,  0.0079])),\n",
       "             ('model.encoder.layers.0.fc1.weight',\n",
       "              tensor([[-0.0822, -0.0664, -0.0421,  ..., -0.2252, -0.0174, -0.0901],\n",
       "                      [-0.2766,  0.0712, -0.0408,  ...,  0.0138,  0.1761,  0.0470],\n",
       "                      [-0.0440,  0.0100, -0.1147,  ...,  0.3438, -0.1202, -0.3132],\n",
       "                      ...,\n",
       "                      [-0.0398, -0.0275, -0.0571,  ..., -0.1705, -0.0421,  0.1415],\n",
       "                      [-0.0609, -0.7812, -1.0000,  ..., -0.1626,  0.0497,  0.0238],\n",
       "                      [ 0.0217, -0.0949,  0.0983,  ...,  0.0842,  0.1940, -0.0874]])),\n",
       "             ('model.encoder.layers.0.fc1.bias',\n",
       "              tensor([-0.3264, -0.1189, -0.1158,  ..., -0.1076, -0.3582,  0.1184])),\n",
       "             ('model.encoder.layers.0.fc2.weight',\n",
       "              tensor([[ 5.4474e-02, -3.5706e-02,  7.3608e-02,  ..., -6.0089e-02,\n",
       "                       -4.7217e-01,  1.3208e-01],\n",
       "                      [ 1.6308e-04,  2.6016e-02, -9.5901e-03,  ..., -2.8137e-02,\n",
       "                       -1.6527e-03, -2.5024e-02],\n",
       "                      [-5.9280e-03,  3.3844e-02, -2.1988e-02,  ...,  1.0384e-02,\n",
       "                       -3.5498e-01,  1.5053e-02],\n",
       "                      ...,\n",
       "                      [-3.8116e-02, -2.1011e-02,  9.9854e-02,  ..., -1.0352e-01,\n",
       "                        7.6103e-04, -2.3364e-01],\n",
       "                      [ 2.1960e-01,  2.2803e-01, -4.5837e-02,  ..., -3.4943e-02,\n",
       "                       -1.2585e-01,  8.1848e-02],\n",
       "                      [-8.0322e-02, -2.6505e-02, -3.4448e-01,  ..., -8.9417e-02,\n",
       "                       -2.1942e-02, -1.9287e-02]])),\n",
       "             ('model.encoder.layers.0.fc2.bias',\n",
       "              tensor([-0.0136, -0.3655,  0.3784,  ..., -0.4990, -0.1392, -0.7456])),\n",
       "             ('model.encoder.layers.0.final_layer_norm.weight',\n",
       "              tensor([0.3259, 1.1016, 1.6602,  ..., 0.3540, 0.1896, 0.2242])),\n",
       "             ('model.encoder.layers.0.final_layer_norm.bias',\n",
       "              tensor([ 0.0026, -0.0082, -0.0087,  ...,  0.1504,  0.0718,  0.1245])),\n",
       "             ('model.encoder.layers.1.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.3132,  0.0303,  0.0594,  ..., -0.1047,  0.2419, -0.0555],\n",
       "                      [-0.0422,  0.0921,  0.0128,  ..., -0.0507,  0.2769,  0.0223],\n",
       "                      [ 0.4136,  0.1260, -0.0191,  ...,  0.1329, -0.0108, -0.2230],\n",
       "                      ...,\n",
       "                      [-0.1825,  0.0161, -0.1770,  ..., -0.0454,  0.0595, -0.0764],\n",
       "                      [-0.0319, -0.1340,  0.1174,  ...,  0.0326,  0.0575, -0.1683],\n",
       "                      [-0.1371, -0.0249,  0.2296,  ..., -0.0196,  0.2529,  0.0146]])),\n",
       "             ('model.encoder.layers.1.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0179,  0.0086, -0.0042,  ..., -0.0084, -0.0040, -0.0065])),\n",
       "             ('model.encoder.layers.1.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0923, -0.0101, -0.0208,  ..., -0.2000,  0.0222,  0.2756],\n",
       "                      [ 0.0516, -0.0055,  0.0196,  ...,  0.0775,  0.0753,  0.2461],\n",
       "                      [-0.0670, -0.0357, -0.0576,  ...,  0.0538, -0.1516, -0.2198],\n",
       "                      ...,\n",
       "                      [-0.3870,  0.1213, -0.1592,  ...,  0.2500,  0.0792, -0.0464],\n",
       "                      [ 0.0688,  0.0403,  0.0655,  ...,  0.2209,  0.2373,  0.0286],\n",
       "                      [-0.0528, -0.0808,  0.0141,  ...,  0.0310,  0.0302,  0.0488]])),\n",
       "             ('model.encoder.layers.1.self_attn.v_proj.bias',\n",
       "              tensor([ 0.0793, -0.0319,  0.0125,  ...,  0.0307, -0.0509, -0.0137])),\n",
       "             ('model.encoder.layers.1.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.2812,  0.1462,  0.0362,  ..., -0.0667,  0.0091,  0.1052],\n",
       "                      [-0.1653,  0.0444, -0.0217,  ..., -0.1318, -0.0861, -0.2842],\n",
       "                      [ 0.2281, -0.0016,  0.1289,  ...,  0.1704, -0.0197,  0.5918],\n",
       "                      ...,\n",
       "                      [-0.2090,  0.0235, -0.0010,  ..., -0.0251, -0.0866,  0.5020],\n",
       "                      [-0.0695,  0.1296, -0.1885,  ..., -0.3889, -0.0210,  0.4143],\n",
       "                      [ 0.3230, -0.0293, -0.0364,  ..., -0.0184,  0.3271,  0.4307]])),\n",
       "             ('model.encoder.layers.1.self_attn.q_proj.bias',\n",
       "              tensor([ 0.1562, -0.1987,  0.2474,  ..., -0.0505, -0.3267,  0.2563])),\n",
       "             ('model.encoder.layers.1.self_attn.out_proj.weight',\n",
       "              tensor([[-0.1606,  0.1558, -0.1584,  ..., -0.0085,  0.0523, -0.3730],\n",
       "                      [-0.0123,  0.0967,  0.0272,  ...,  0.0414,  0.1367,  0.1249],\n",
       "                      [ 0.0454, -0.0834, -0.0182,  ...,  0.0917, -0.2466,  0.0072],\n",
       "                      ...,\n",
       "                      [-0.1824, -0.0740,  0.1194,  ...,  0.0551, -0.1820, -0.1041],\n",
       "                      [ 0.2206, -0.1924,  0.2886,  ..., -0.0296, -0.3135, -0.0388],\n",
       "                      [ 0.0279,  0.1779, -0.1296,  ..., -0.4524,  0.0889, -0.1515]])),\n",
       "             ('model.encoder.layers.1.self_attn.out_proj.bias',\n",
       "              tensor([-0.5601, -0.4797,  0.5430,  ..., -0.7354, -0.2346, -0.3242])),\n",
       "             ('model.encoder.layers.1.self_attn_layer_norm.weight',\n",
       "              tensor([0.2800, 0.6963, 1.0000,  ..., 0.1310, 0.1289, 0.0534])),\n",
       "             ('model.encoder.layers.1.self_attn_layer_norm.bias',\n",
       "              tensor([-0.0067, -0.0167,  0.0038,  ..., -0.0364,  0.0032,  0.0068])),\n",
       "             ('model.encoder.layers.1.fc1.weight',\n",
       "              tensor([[ 0.0288,  0.0228, -0.0608,  ..., -0.0198, -0.1741,  0.0707],\n",
       "                      [ 0.1438,  0.0527,  0.0828,  ..., -0.2949, -0.1907, -0.1910],\n",
       "                      [ 0.0360,  0.2744,  0.0928,  ..., -0.1534,  0.1022, -0.0169],\n",
       "                      ...,\n",
       "                      [ 0.1482,  0.1733, -0.1611,  ..., -0.0764,  0.1055, -0.1361],\n",
       "                      [ 0.2598,  0.2203,  0.1808,  ..., -0.0300,  0.2542,  0.1102],\n",
       "                      [ 0.2976,  0.0646,  0.2065,  ..., -0.0663, -0.1564, -0.0345]])),\n",
       "             ('model.encoder.layers.1.fc1.bias',\n",
       "              tensor([-0.0112, -0.2332, -0.1604,  ..., -0.1494, -0.0759, -0.0137])),\n",
       "             ('model.encoder.layers.1.fc2.weight',\n",
       "              tensor([[ 0.0429,  0.0976,  0.2073,  ..., -0.2070,  0.0483, -0.0598],\n",
       "                      [ 0.0475,  0.0685, -0.1361,  ..., -0.0488,  0.0395,  0.2761],\n",
       "                      [-0.0568, -0.0254,  0.1608,  ...,  0.1666,  0.0410,  0.0114],\n",
       "                      ...,\n",
       "                      [ 0.1724,  0.2063,  0.0209,  ...,  0.3467,  0.2316,  0.0891],\n",
       "                      [ 0.1193,  0.0363, -0.2162,  ..., -0.0956,  0.0388,  0.0544],\n",
       "                      [-0.1222,  0.2642, -0.2076,  ..., -0.0162, -0.0597, -0.1100]])),\n",
       "             ('model.encoder.layers.1.fc2.bias',\n",
       "              tensor([-0.4980, -0.6235,  0.3506,  ..., -0.8696,  0.0837,  0.0706])),\n",
       "             ('model.encoder.layers.1.final_layer_norm.weight',\n",
       "              tensor([0.3640, 0.5825, 0.7876,  ..., 0.3674, 0.2639, 0.7192])),\n",
       "             ('model.encoder.layers.1.final_layer_norm.bias',\n",
       "              tensor([-0.0160, -0.1505,  0.0006,  ...,  0.2961,  0.1514,  0.3875])),\n",
       "             ('model.encoder.layers.2.self_attn.k_proj.weight',\n",
       "              tensor([[-0.3447, -0.4392, -0.2729,  ...,  0.1851,  0.1113, -0.1281],\n",
       "                      [-0.5283, -0.3757, -0.2126,  ...,  0.0692, -0.0856,  0.1705],\n",
       "                      [ 0.0643, -0.1009,  0.0264,  ..., -0.1879, -0.1941, -0.0309],\n",
       "                      ...,\n",
       "                      [ 0.1146, -0.1388,  0.0802,  ...,  0.0249, -0.2125, -0.0556],\n",
       "                      [ 0.0025,  0.0030,  0.4084,  ...,  0.2128,  0.1897,  0.1888],\n",
       "                      [-0.2020, -0.0406, -0.2024,  ...,  0.0584,  0.1917,  0.2167]])),\n",
       "             ('model.encoder.layers.2.self_attn.k_proj.bias',\n",
       "              tensor([-0.0259, -0.0300,  0.0098,  ...,  0.0023,  0.0161, -0.0312])),\n",
       "             ('model.encoder.layers.2.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0021,  0.0291,  0.2048,  ...,  0.3103, -0.0507, -0.1099],\n",
       "                      [ 0.0806,  0.0523, -0.0200,  ...,  0.1108, -0.0761,  0.0262],\n",
       "                      [-0.0192,  0.1956,  0.0440,  ..., -0.2407, -0.1000, -0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0179,  0.0394, -0.0177,  ...,  0.1243,  0.0283,  0.0198],\n",
       "                      [ 0.0754, -0.0627, -0.0371,  ...,  0.2119, -0.3503,  0.0305],\n",
       "                      [ 0.1018,  0.2954,  0.0267,  ...,  0.0017,  0.0053, -0.0506]])),\n",
       "             ('model.encoder.layers.2.self_attn.v_proj.bias',\n",
       "              tensor([ 0.1757, -0.1736,  0.0048,  ...,  0.0027, -0.0123,  0.0012])),\n",
       "             ('model.encoder.layers.2.self_attn.q_proj.weight',\n",
       "              tensor([[-0.2394, -0.2335, -0.0950,  ...,  0.1547,  0.1436,  0.1914],\n",
       "                      [-0.0340, -0.1176, -0.3926,  ...,  0.1010, -0.0010, -0.0776],\n",
       "                      [ 0.1042, -0.2820,  0.0054,  ..., -0.1914,  0.0817,  0.1759],\n",
       "                      ...,\n",
       "                      [-0.0563, -0.0400,  0.0502,  ..., -0.0404,  0.2847,  0.0385],\n",
       "                      [ 0.1459, -0.0163,  0.0189,  ...,  0.0794,  0.2352,  0.1910],\n",
       "                      [ 0.1403,  0.0652,  0.1050,  ..., -0.0582, -0.0510, -0.1772]])),\n",
       "             ('model.encoder.layers.2.self_attn.q_proj.bias',\n",
       "              tensor([-0.1339,  0.7612,  0.0226,  ...,  0.0240,  0.1281,  0.1191])),\n",
       "             ('model.encoder.layers.2.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.3418, -0.0677,  0.2137,  ...,  0.0499,  0.2649,  0.1776],\n",
       "                      [-0.3020, -0.0951,  0.0147,  ...,  0.0165,  0.0594,  0.0526],\n",
       "                      [-0.2974,  0.2010,  0.0136,  ...,  0.1761,  0.2007, -0.0710],\n",
       "                      ...,\n",
       "                      [ 0.0853,  0.2075,  0.0961,  ..., -0.2235, -0.2292,  0.0688],\n",
       "                      [-0.0068, -0.0393, -0.1675,  ..., -0.1058, -0.2266, -0.1720],\n",
       "                      [ 0.2888,  0.1907,  0.2939,  ...,  0.1801,  0.4055,  0.1396]])),\n",
       "             ('model.encoder.layers.2.self_attn.out_proj.bias',\n",
       "              tensor([-0.3291,  0.1842, -0.0803,  ..., -0.4951, -0.2440, -0.1320])),\n",
       "             ('model.encoder.layers.2.self_attn_layer_norm.weight',\n",
       "              tensor([0.2551, 0.3916, 0.4587,  ..., 0.1593, 0.1455, 0.0640])),\n",
       "             ('model.encoder.layers.2.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0016, -0.0094,  0.0044,  ..., -0.0253,  0.0004, -0.1724])),\n",
       "             ('model.encoder.layers.2.fc1.weight',\n",
       "              tensor([[-0.0267, -0.2546, -0.1114,  ...,  0.0208, -0.4014, -0.0889],\n",
       "                      [ 0.1504,  0.1133, -0.3638,  ..., -0.0380, -0.0839, -0.0189],\n",
       "                      [-0.3149,  0.2098, -0.0369,  ..., -0.0750,  0.1387, -0.1759],\n",
       "                      ...,\n",
       "                      [ 0.1409,  0.0983,  0.0416,  ...,  0.1759, -0.0855,  0.1470],\n",
       "                      [-0.1570,  0.0397, -0.2314,  ..., -0.1272,  0.0359,  0.0133],\n",
       "                      [-0.1584, -0.0536, -0.2089,  ..., -0.1642, -0.5425, -0.1512]])),\n",
       "             ('model.encoder.layers.2.fc1.bias',\n",
       "              tensor([-0.0205, -0.1761, -0.1024,  ..., -0.1119, -0.2397, -0.1015])),\n",
       "             ('model.encoder.layers.2.fc2.weight',\n",
       "              tensor([[ 0.2343,  0.1005, -0.0427,  ..., -0.0775, -0.1626, -0.0025],\n",
       "                      [-0.2034, -0.2035, -0.0301,  ..., -0.2466,  0.1137,  0.2327],\n",
       "                      [ 0.0221,  0.0497,  0.0061,  ...,  0.0156,  0.4207,  0.0813],\n",
       "                      ...,\n",
       "                      [ 0.0395,  0.2274,  0.0329,  ...,  0.0589,  0.1022, -0.2118],\n",
       "                      [-0.0251, -0.1895,  0.0290,  ...,  0.5098,  0.1008, -0.3828],\n",
       "                      [ 0.3801,  0.1416, -0.0853,  ...,  0.0865,  0.0880, -0.0945]])),\n",
       "             ('model.encoder.layers.2.fc2.bias',\n",
       "              tensor([-0.2042, -0.3538,  0.0340,  ..., -0.5020, -0.2240, -0.4961])),\n",
       "             ('model.encoder.layers.2.final_layer_norm.weight',\n",
       "              tensor([0.4373, 0.5439, 0.6646,  ..., 0.4412, 0.3557, 0.5312])),\n",
       "             ('model.encoder.layers.2.final_layer_norm.bias',\n",
       "              tensor([ 0.0157, -0.1758,  0.0136,  ...,  0.3103,  0.2247,  0.3567])),\n",
       "             ('model.encoder.layers.3.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.1273,  0.2241, -0.0561,  ..., -0.1205,  0.2671,  0.1118],\n",
       "                      [-0.1213,  0.0095, -0.0512,  ..., -0.0240,  0.2441,  0.0936],\n",
       "                      [-0.1697, -0.1659,  0.1774,  ...,  0.1174, -0.1907,  0.1812],\n",
       "                      ...,\n",
       "                      [-0.2524, -0.0491, -0.2900,  ..., -0.0951,  0.3584,  0.0298],\n",
       "                      [ 0.3196, -0.3335,  0.0198,  ..., -0.2542,  0.2593, -0.2198],\n",
       "                      [-0.2167, -0.0964, -0.0109,  ...,  0.1262,  0.2217,  0.2588]])),\n",
       "             ('model.encoder.layers.3.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0045, -0.0249,  0.0055,  ...,  0.0129,  0.0192, -0.0107])),\n",
       "             ('model.encoder.layers.3.self_attn.v_proj.weight',\n",
       "              tensor([[-0.1000, -0.0194, -0.0435,  ..., -0.2358,  0.0946,  0.0695],\n",
       "                      [ 0.0120,  0.0917, -0.1650,  ...,  0.1097, -0.0242, -0.0247],\n",
       "                      [ 0.0066, -0.0302,  0.0870,  ..., -0.0209,  0.4578, -0.0189],\n",
       "                      ...,\n",
       "                      [ 0.3220, -0.0787, -0.2520,  ...,  0.1698,  0.2107,  0.0212],\n",
       "                      [ 0.0778,  0.0969,  0.0474,  ...,  0.1864, -0.0052, -0.0772],\n",
       "                      [-0.0798,  0.2871,  0.1097,  ..., -0.0649,  0.0545,  0.0087]])),\n",
       "             ('model.encoder.layers.3.self_attn.v_proj.bias',\n",
       "              tensor([-0.0052,  0.1276, -0.0572,  ..., -0.1461, -0.2104, -0.0482])),\n",
       "             ('model.encoder.layers.3.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.1786, -0.3074, -0.1509,  ...,  0.1709, -0.3638, -0.1603],\n",
       "                      [-0.1456, -0.0075, -0.0852,  ..., -0.1757,  0.4004,  0.3643],\n",
       "                      [ 0.1526,  0.0436, -0.1224,  ...,  0.0580,  0.4180, -0.2462],\n",
       "                      ...,\n",
       "                      [-0.2957, -0.0009, -0.4045,  ...,  0.2405, -0.2418,  0.0562],\n",
       "                      [-0.1401, -0.1481, -0.1549,  ..., -0.3357,  0.0062, -0.2097],\n",
       "                      [ 0.1744, -0.2991, -0.4023,  ...,  0.0531, -0.3079,  0.1447]])),\n",
       "             ('model.encoder.layers.3.self_attn.q_proj.bias',\n",
       "              tensor([ 0.3250,  0.2773,  0.2637,  ...,  0.4526, -0.1826,  0.5928])),\n",
       "             ('model.encoder.layers.3.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.2375,  0.4453, -0.1666,  ..., -0.0097, -0.2749,  0.2230],\n",
       "                      [-0.1320,  0.1287, -0.1155,  ..., -0.0981, -0.1682,  0.0881],\n",
       "                      [ 0.2158, -0.1592,  0.2389,  ..., -0.2021,  0.0601,  0.0741],\n",
       "                      ...,\n",
       "                      [-0.2094,  0.2705,  0.3247,  ..., -0.3774, -0.0038,  0.0504],\n",
       "                      [ 0.1333, -0.0093,  0.0922,  ..., -0.2559, -0.2120, -0.0821],\n",
       "                      [ 0.1238, -0.1958, -0.2073,  ...,  0.0613,  0.0382, -0.0040]])),\n",
       "             ('model.encoder.layers.3.self_attn.out_proj.bias',\n",
       "              tensor([-0.1768,  0.0324, -0.4902,  ..., -0.1864, -0.1381, -0.0352])),\n",
       "             ('model.encoder.layers.3.self_attn_layer_norm.weight',\n",
       "              tensor([0.2632, 0.3484, 0.3823,  ..., 0.1754, 0.1694, 0.0759])),\n",
       "             ('model.encoder.layers.3.self_attn_layer_norm.bias',\n",
       "              tensor([-0.0017, -0.0178,  0.0143,  ..., -0.0205, -0.0016, -0.1876])),\n",
       "             ('model.encoder.layers.3.fc1.weight',\n",
       "              tensor([[ 0.2700,  0.1938,  0.2998,  ...,  0.1562,  0.1998,  0.3589],\n",
       "                      [ 0.1078,  0.3862,  0.1381,  ..., -0.1604, -0.1088,  0.1871],\n",
       "                      [-0.0421,  0.3840, -0.1287,  ...,  0.0122,  0.1149, -0.0363],\n",
       "                      ...,\n",
       "                      [ 0.0747,  0.2203, -0.2174,  ..., -0.3176, -0.0036, -0.1372],\n",
       "                      [ 0.1874,  0.1069, -0.1216,  ..., -0.2939,  0.4365,  0.1703],\n",
       "                      [ 0.1702,  0.2842,  0.0290,  ...,  0.0687, -0.0826,  0.0793]])),\n",
       "             ('model.encoder.layers.3.fc1.bias',\n",
       "              tensor([-0.0987, -0.1757, -0.0022,  ..., -0.2773, -0.2390, -0.1774])),\n",
       "             ('model.encoder.layers.3.fc2.weight',\n",
       "              tensor([[-0.0603, -0.1033,  0.0816,  ..., -0.3188, -0.2788, -0.1556],\n",
       "                      [ 0.0388,  0.0523, -0.2107,  ..., -0.2145,  0.0859,  0.1127],\n",
       "                      [-0.0334, -0.1051, -0.0261,  ...,  0.1476, -0.2554, -0.1295],\n",
       "                      ...,\n",
       "                      [-0.0571, -0.1379, -0.1825,  ..., -0.0071, -0.1080, -0.2607],\n",
       "                      [-0.2651, -0.1130,  0.1184,  ..., -0.3130,  0.2365, -0.1627],\n",
       "                      [-0.2324, -0.0607,  0.2191,  ...,  0.4524,  0.0908,  0.0570]])),\n",
       "             ('model.encoder.layers.3.fc2.bias',\n",
       "              tensor([-0.1322, -0.2524, -0.2625,  ..., -0.6270, -0.4980,  0.1598])),\n",
       "             ('model.encoder.layers.3.final_layer_norm.weight',\n",
       "              tensor([0.5571, 0.6401, 0.6841,  ..., 0.5356, 0.4375, 0.5571])),\n",
       "             ('model.encoder.layers.3.final_layer_norm.bias',\n",
       "              tensor([ 0.0726, -0.2047,  0.0892,  ...,  0.3430,  0.1608,  0.3596])),\n",
       "             ('model.encoder.layers.4.self_attn.k_proj.weight',\n",
       "              tensor([[-0.1092, -0.0028, -0.1350,  ..., -0.1427,  0.2861, -0.0529],\n",
       "                      [ 0.1046,  0.1780, -0.0298,  ..., -0.0937,  0.0679, -0.0216],\n",
       "                      [-0.1721, -0.1276,  0.2137,  ...,  0.0411, -0.0165, -0.1064],\n",
       "                      ...,\n",
       "                      [ 0.1192,  0.1671,  0.2939,  ...,  0.1614, -0.0975,  0.0159],\n",
       "                      [-0.1638, -0.1135, -0.0382,  ...,  0.0856, -0.1583, -0.0257],\n",
       "                      [-0.1689,  0.3257, -0.1021,  ...,  0.0256,  0.0517,  0.1899]])),\n",
       "             ('model.encoder.layers.4.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0186, -0.0253, -0.0241,  ...,  0.0240,  0.0197,  0.0040])),\n",
       "             ('model.encoder.layers.4.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0698, -0.0300,  0.1226,  ..., -0.2991,  0.0958,  0.0717],\n",
       "                      [ 0.1055,  0.0383, -0.1597,  ..., -0.2573,  0.1906,  0.0244],\n",
       "                      [ 0.0713, -0.3877, -0.1547,  ...,  0.0464, -0.0274,  0.0415],\n",
       "                      ...,\n",
       "                      [ 0.0169,  0.1136, -0.2030,  ..., -0.1438,  0.3379,  0.1071],\n",
       "                      [-0.2815,  0.1594,  0.4478,  ...,  0.1702, -0.4883,  0.0009],\n",
       "                      [-0.2661, -0.0815,  0.4202,  ..., -0.1316, -0.3354,  0.0138]])),\n",
       "             ('model.encoder.layers.4.self_attn.v_proj.bias',\n",
       "              tensor([-0.0147, -0.1171,  0.1478,  ..., -0.0172, -0.0030, -0.1357])),\n",
       "             ('model.encoder.layers.4.self_attn.q_proj.weight',\n",
       "              tensor([[-0.3167,  0.2019,  0.2131,  ...,  0.0332, -0.3059, -0.3101],\n",
       "                      [ 0.0149, -0.0750,  0.2404,  ..., -0.1279, -0.1794,  0.1234],\n",
       "                      [-0.0178, -0.0139, -0.2239,  ..., -0.1698, -0.0556, -0.1077],\n",
       "                      ...,\n",
       "                      [-0.1376,  0.0729, -0.2188,  ...,  0.0415, -0.1272,  0.3601],\n",
       "                      [ 0.0909,  0.0670, -0.0636,  ..., -0.1163, -0.2096, -0.0964],\n",
       "                      [ 0.2123, -0.3875, -0.1823,  ..., -0.0737, -0.0089, -0.1017]])),\n",
       "             ('model.encoder.layers.4.self_attn.q_proj.bias',\n",
       "              tensor([-0.1918,  0.3306,  0.0953,  ..., -0.0160, -0.1255,  0.0032])),\n",
       "             ('model.encoder.layers.4.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.3052,  0.0142, -0.2898,  ...,  0.0723,  0.0241,  0.1895],\n",
       "                      [-0.2295, -0.2798, -0.0404,  ...,  0.1388,  0.2332,  0.2834],\n",
       "                      [ 0.1124,  0.0247,  0.2905,  ...,  0.0372, -0.3989, -0.0923],\n",
       "                      ...,\n",
       "                      [ 0.0914,  0.4182,  0.0561,  ..., -0.3076, -0.0513,  0.0178],\n",
       "                      [-0.0081, -0.1631,  0.3223,  ...,  0.0817, -0.0063,  0.0272],\n",
       "                      [-0.0464,  0.0386, -0.3821,  ..., -0.2686,  0.2832, -0.3052]])),\n",
       "             ('model.encoder.layers.4.self_attn.out_proj.bias',\n",
       "              tensor([-0.2148, -0.2712, -0.4692,  ..., -0.4463, -0.2773, -0.1874])),\n",
       "             ('model.encoder.layers.4.self_attn_layer_norm.weight',\n",
       "              tensor([0.2480, 0.2932, 0.3071,  ..., 0.1970, 0.1952, 0.0856])),\n",
       "             ('model.encoder.layers.4.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0061, -0.0186,  0.0067,  ..., -0.0136,  0.0044, -0.1420])),\n",
       "             ('model.encoder.layers.4.fc1.weight',\n",
       "              tensor([[-0.0947, -0.3777,  0.1453,  ...,  0.2079,  0.1239, -0.1075],\n",
       "                      [-0.0224, -0.1260,  0.0084,  ..., -0.1088, -0.1128,  0.4102],\n",
       "                      [-0.1868,  0.2651, -0.2267,  ..., -0.2732,  0.2051,  0.1484],\n",
       "                      ...,\n",
       "                      [-0.1461, -0.2017,  0.1912,  ..., -0.0526, -0.1860,  0.0266],\n",
       "                      [ 0.3555,  0.1292, -0.0576,  ..., -0.0812, -0.0305, -0.1492],\n",
       "                      [ 0.0394, -0.0345,  0.0431,  ...,  0.0626,  0.0320,  0.5259]])),\n",
       "             ('model.encoder.layers.4.fc1.bias',\n",
       "              tensor([-0.1158, -0.1146, -0.0953,  ..., -0.0958, -0.1493,  0.1250])),\n",
       "             ('model.encoder.layers.4.fc2.weight',\n",
       "              tensor([[ 9.1736e-02, -3.4851e-02,  6.6711e-02,  ...,  1.7236e-01,\n",
       "                       -2.4426e-01, -1.8814e-02],\n",
       "                      [ 2.3706e-01,  1.6125e-01, -1.7261e-01,  ...,  1.8347e-01,\n",
       "                        3.2921e-03,  2.4902e-02],\n",
       "                      [ 1.3171e-01,  1.8689e-01, -5.2910e-03,  ..., -8.6365e-02,\n",
       "                       -4.3640e-02,  3.5248e-03],\n",
       "                      ...,\n",
       "                      [-3.4094e-04, -2.9468e-01,  1.6211e-01,  ...,  2.6520e-02,\n",
       "                       -4.6265e-02,  5.0537e-02],\n",
       "                      [-2.1143e-01, -1.3879e-01,  4.9731e-01,  ...,  7.4158e-03,\n",
       "                        2.5854e-01,  2.4811e-02],\n",
       "                      [-8.2092e-02,  2.2675e-02, -2.8735e-01,  ...,  6.6528e-02,\n",
       "                        2.5195e-01,  3.7183e-01]])),\n",
       "             ('model.encoder.layers.4.fc2.bias',\n",
       "              tensor([-0.0294, -0.0543, -0.1670,  ..., -0.4087, -0.3076, -0.2484])),\n",
       "             ('model.encoder.layers.4.final_layer_norm.weight',\n",
       "              tensor([0.8389, 0.8896, 0.9126,  ..., 0.8003, 0.6929, 0.9141])),\n",
       "             ('model.encoder.layers.4.final_layer_norm.bias',\n",
       "              tensor([ 0.0629, -0.1814,  0.1497,  ...,  0.3884,  0.2186,  0.6089])),\n",
       "             ('model.encoder.layers.5.self_attn.k_proj.weight',\n",
       "              tensor([[-0.1027,  0.1290,  0.2034,  ...,  0.0239,  0.0739,  0.1147],\n",
       "                      [ 0.0134,  0.0109,  0.0040,  ..., -0.1880, -0.0925,  0.1240],\n",
       "                      [ 0.1074, -0.2524,  0.1588,  ..., -0.0675, -0.1399, -0.1630],\n",
       "                      ...,\n",
       "                      [-0.1376,  0.0674, -0.1263,  ..., -0.0240, -0.1779,  0.2888],\n",
       "                      [-0.0420, -0.0428, -0.0071,  ...,  0.1560,  0.2296, -0.0860],\n",
       "                      [ 0.2198,  0.2279, -0.0904,  ..., -0.0740,  0.1239, -0.1846]])),\n",
       "             ('model.encoder.layers.5.self_attn.k_proj.bias',\n",
       "              tensor([-0.0248, -0.0112,  0.0246,  ...,  0.0078,  0.0279, -0.0154])),\n",
       "             ('model.encoder.layers.5.self_attn.v_proj.weight',\n",
       "              tensor([[-6.2683e-02,  4.0723e-01,  3.0078e-01,  ...,  2.5342e-01,\n",
       "                        3.8208e-01, -1.4209e-01],\n",
       "                      [ 2.1057e-01, -4.9731e-01, -5.0195e-01,  ...,  4.3604e-01,\n",
       "                       -2.5562e-01,  7.9468e-02],\n",
       "                      [ 2.2192e-01, -4.8315e-01, -4.9951e-01,  ...,  2.4561e-01,\n",
       "                       -1.2524e-01, -8.5876e-02],\n",
       "                      ...,\n",
       "                      [ 2.9297e-03,  1.6528e-01,  1.7249e-01,  ...,  2.3157e-01,\n",
       "                       -8.5205e-02, -1.6647e-02],\n",
       "                      [ 1.7163e-01, -5.5176e-01,  1.1542e-01,  ..., -2.9545e-03,\n",
       "                        3.3618e-01,  3.1757e-04],\n",
       "                      [-1.7358e-01,  1.0901e-01,  3.8971e-02,  ..., -3.2129e-01,\n",
       "                       -6.4148e-02,  9.6985e-02]])),\n",
       "             ('model.encoder.layers.5.self_attn.v_proj.bias',\n",
       "              tensor([ 0.2517,  0.0800, -0.1105,  ...,  0.1067,  0.0918, -0.2163])),\n",
       "             ('model.encoder.layers.5.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0835, -0.2556,  0.1780,  ..., -0.0013,  0.0685,  0.1277],\n",
       "                      [-0.0474,  0.0945,  0.0859,  ..., -0.0839, -0.2522,  0.3069],\n",
       "                      [-0.1220,  0.1346,  0.0013,  ..., -0.1627,  0.1812, -0.3811],\n",
       "                      ...,\n",
       "                      [-0.5020, -0.2435, -0.0367,  ..., -0.1248, -0.0159,  0.0864],\n",
       "                      [-0.0185, -0.1132, -0.1614,  ...,  0.0128, -0.2505,  0.0236],\n",
       "                      [ 0.0471, -0.0351,  0.2712,  ..., -0.0532, -0.0487,  0.1573]])),\n",
       "             ('model.encoder.layers.5.self_attn.q_proj.bias',\n",
       "              tensor([-0.1298,  0.3081, -0.0649,  ...,  0.3333, -0.2089, -0.3911])),\n",
       "             ('model.encoder.layers.5.self_attn.out_proj.weight',\n",
       "              tensor([[-0.1759, -0.2527, -0.4905,  ..., -0.1455,  0.0436,  0.0316],\n",
       "                      [-0.0808,  0.3103, -0.0531,  ...,  0.2908, -0.4807, -0.0984],\n",
       "                      [-0.2214, -0.2119,  0.0950,  ...,  0.3025,  0.3235,  0.0836],\n",
       "                      ...,\n",
       "                      [-0.1042, -0.2352, -0.2393,  ..., -0.0567, -0.1390, -0.5044],\n",
       "                      [ 0.1320, -0.2820,  0.1831,  ..., -0.0705,  0.1049,  0.4670],\n",
       "                      [ 0.1141, -0.1040,  0.2449,  ...,  0.2822, -0.1600,  0.0797]])),\n",
       "             ('model.encoder.layers.5.self_attn.out_proj.bias',\n",
       "              tensor([-0.2974,  0.1377, -0.3240,  ..., -0.3740, -0.3955, -0.0625])),\n",
       "             ('model.encoder.layers.5.self_attn_layer_norm.weight',\n",
       "              tensor([0.2593, 0.2888, 0.2932,  ..., 0.2134, 0.1947, 0.1024])),\n",
       "             ('model.encoder.layers.5.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0130, -0.0358,  0.0096,  ..., -0.0243, -0.0059, -0.1379])),\n",
       "             ('model.encoder.layers.5.fc1.weight',\n",
       "              tensor([[-0.1627,  0.4009,  0.3484,  ..., -0.4468,  0.2238,  0.0247],\n",
       "                      [-0.2316, -0.2040,  0.2120,  ...,  0.0018,  0.2175, -0.0315],\n",
       "                      [ 0.0982,  0.2788, -0.5264,  ...,  0.2260, -0.1183, -0.1161],\n",
       "                      ...,\n",
       "                      [-0.1323, -0.2255,  0.0363,  ..., -0.2161, -0.0649, -0.0839],\n",
       "                      [ 0.0827,  0.3855,  0.0304,  ..., -0.1301,  0.0207,  0.0511],\n",
       "                      [ 0.1125,  0.0268,  0.0286,  ..., -0.0140, -0.2603,  0.0757]])),\n",
       "             ('model.encoder.layers.5.fc1.bias',\n",
       "              tensor([-0.3113, -0.2347, -0.2471,  ...,  0.0296, -0.0106, -0.1477])),\n",
       "             ('model.encoder.layers.5.fc2.weight',\n",
       "              tensor([[ 0.1118, -0.0317,  0.3906,  ...,  0.1901,  0.0196,  0.1421],\n",
       "                      [ 0.2644, -0.2418, -0.0213,  ...,  0.2583, -0.1804,  0.0911],\n",
       "                      [ 0.1860,  0.1185, -0.1493,  ..., -0.0756, -0.0252, -0.0974],\n",
       "                      ...,\n",
       "                      [-0.0878,  0.0639,  0.0248,  ...,  0.1627, -0.0352,  0.0266],\n",
       "                      [-0.2057,  0.3218,  0.3306,  ...,  0.0302, -0.1030, -0.1403],\n",
       "                      [-0.3589,  0.0693,  0.3821,  ...,  0.0552,  0.0178,  0.1909]])),\n",
       "             ('model.encoder.layers.5.fc2.bias',\n",
       "              tensor([-0.1635,  0.1593,  0.2888,  ..., -0.3391, -0.1981,  0.2345])),\n",
       "             ('model.encoder.layers.5.final_layer_norm.weight',\n",
       "              tensor([1.0498, 1.0732, 1.0986,  ..., 1.0576, 0.9355, 1.1182])),\n",
       "             ('model.encoder.layers.5.final_layer_norm.bias',\n",
       "              tensor([ 0.0972, -0.2222,  0.1204,  ...,  0.2439, -0.0014,  0.7427])),\n",
       "             ('model.encoder.layers.6.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0706, -0.0345,  0.2610,  ..., -0.1284,  0.0302, -0.1357],\n",
       "                      [ 0.1028,  0.0043, -0.1156,  ..., -0.1282, -0.1825, -0.1663],\n",
       "                      [-0.2213, -0.1600,  0.1575,  ...,  0.2169, -0.0125,  0.1002],\n",
       "                      ...,\n",
       "                      [-0.0800, -0.0073, -0.0764,  ..., -0.1770, -0.0122, -0.1235],\n",
       "                      [ 0.1169,  0.0127, -0.1982,  ..., -0.0235,  0.0922,  0.0699],\n",
       "                      [-0.4729, -0.2825,  0.0516,  ...,  0.0944,  0.0304,  0.3552]])),\n",
       "             ('model.encoder.layers.6.self_attn.k_proj.bias',\n",
       "              tensor([-0.0212,  0.0140,  0.0011,  ...,  0.0077,  0.0287, -0.0167])),\n",
       "             ('model.encoder.layers.6.self_attn.v_proj.weight',\n",
       "              tensor([[-0.1652,  0.3330,  0.3162,  ..., -0.2389,  0.2849, -0.0800],\n",
       "                      [-0.1203,  0.2966, -0.1333,  ..., -0.2632,  0.3467,  0.0620],\n",
       "                      [ 0.1744, -0.0900,  0.2561,  ..., -0.0182,  0.3352, -0.1104],\n",
       "                      ...,\n",
       "                      [-0.5029, -0.0768,  0.3154,  ..., -0.1250, -0.3794,  0.0287],\n",
       "                      [ 0.2097,  0.2666, -0.5264,  ..., -0.3022,  0.2620, -0.2422],\n",
       "                      [-0.3618,  0.1602,  0.1276,  ...,  0.1013, -0.4070, -0.0555]])),\n",
       "             ('model.encoder.layers.6.self_attn.v_proj.bias',\n",
       "              tensor([-0.1273,  0.0925, -0.0681,  ...,  0.1183,  0.1335,  0.1152])),\n",
       "             ('model.encoder.layers.6.self_attn.q_proj.weight',\n",
       "              tensor([[ 5.8380e-02,  1.7212e-01,  1.0687e-01,  ...,  7.8491e-02,\n",
       "                        2.8854e-02,  3.4326e-01],\n",
       "                      [ 3.1348e-01,  1.0724e-01, -2.2888e-03,  ..., -3.3398e-03,\n",
       "                       -2.1375e-01,  2.3218e-01],\n",
       "                      [-1.6968e-01, -1.7670e-02,  1.2512e-01,  ...,  4.8615e-02,\n",
       "                       -2.1106e-01, -2.4805e-01],\n",
       "                      ...,\n",
       "                      [-4.3365e-02,  4.7638e-02,  7.3303e-02,  ..., -3.2837e-02,\n",
       "                        5.4840e-02, -1.5967e-01],\n",
       "                      [ 7.6599e-02,  1.3269e-01, -4.7241e-02,  ...,  1.3574e-01,\n",
       "                       -1.0687e-01, -2.5854e-01],\n",
       "                      [ 1.8738e-02,  1.0598e-04,  3.7262e-02,  ...,  1.2688e-02,\n",
       "                        3.1647e-02, -2.9468e-01]])),\n",
       "             ('model.encoder.layers.6.self_attn.q_proj.bias',\n",
       "              tensor([-0.4375,  0.0213,  0.2203,  ..., -0.4126, -0.1324,  0.9414])),\n",
       "             ('model.encoder.layers.6.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.4932, -0.2959, -0.2666,  ..., -0.0205,  0.1765, -0.2189],\n",
       "                      [ 0.2407,  0.2262,  0.1125,  ..., -0.0656,  0.1926,  0.0654],\n",
       "                      [ 0.2922,  0.1119,  0.3481,  ...,  0.2129,  0.0788,  0.0352],\n",
       "                      ...,\n",
       "                      [-0.1580,  0.1183,  0.0581,  ..., -0.2288, -0.0887, -0.0232],\n",
       "                      [-0.0381,  0.1404, -0.0415,  ..., -0.2959,  0.0758, -0.3286],\n",
       "                      [-0.5059,  0.4998, -0.0927,  ...,  0.3774, -0.2629, -0.5054]])),\n",
       "             ('model.encoder.layers.6.self_attn.out_proj.bias',\n",
       "              tensor([-0.0626,  0.2250, -0.1521,  ..., -0.4531, -0.2175,  0.0847])),\n",
       "             ('model.encoder.layers.6.self_attn_layer_norm.weight',\n",
       "              tensor([0.2781, 0.3113, 0.3044,  ..., 0.2424, 0.2245, 0.1198])),\n",
       "             ('model.encoder.layers.6.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0091, -0.0322,  0.0044,  ..., -0.0353, -0.0171, -0.1396])),\n",
       "             ('model.encoder.layers.6.fc1.weight',\n",
       "              tensor([[ 0.5049, -0.2141,  0.1185,  ...,  0.0346, -0.0762, -0.2939],\n",
       "                      [-0.1039,  0.0520, -0.2483,  ..., -0.2549,  0.0668,  0.0578],\n",
       "                      [-0.1609, -0.0087,  0.0487,  ...,  0.0201, -0.0370, -0.1423],\n",
       "                      ...,\n",
       "                      [-0.2238,  0.2118,  0.5020,  ..., -0.2603, -0.5088, -0.1318],\n",
       "                      [ 0.0547, -0.1289, -0.1384,  ..., -0.3176, -0.0857,  0.4788],\n",
       "                      [-0.1636, -0.1747,  0.0696,  ...,  0.1959, -0.3271, -0.2164]])),\n",
       "             ('model.encoder.layers.6.fc1.bias',\n",
       "              tensor([-0.2440, -0.1483, -0.1898,  ..., -0.2588, -0.2106, -0.1549])),\n",
       "             ('model.encoder.layers.6.fc2.weight',\n",
       "              tensor([[-0.1581, -0.0368,  0.2334,  ..., -0.3040,  0.1361,  0.1009],\n",
       "                      [ 0.2061,  0.0373, -0.1328,  ...,  0.4993, -0.0421, -0.2350],\n",
       "                      [ 0.0025,  0.0223, -0.1836,  ...,  0.1327, -0.1272,  0.0468],\n",
       "                      ...,\n",
       "                      [-0.1857,  0.1321, -0.2494,  ..., -0.3591, -0.0569,  0.1892],\n",
       "                      [-0.3350, -0.0670,  0.2271,  ...,  0.2461, -0.0009, -0.2045],\n",
       "                      [ 0.0830,  0.0142,  0.1000,  ..., -0.2190,  0.0870,  0.2712]])),\n",
       "             ('model.encoder.layers.6.fc2.bias',\n",
       "              tensor([-0.2479, -0.1766,  0.3108,  ..., -0.2715,  0.1544, -0.0607])),\n",
       "             ('model.encoder.layers.6.final_layer_norm.weight',\n",
       "              tensor([1.2949, 1.2705, 1.2734,  ..., 1.2637, 1.1494, 1.0869])),\n",
       "             ('model.encoder.layers.6.final_layer_norm.bias',\n",
       "              tensor([ 0.1816, -0.1693,  0.0043,  ...,  0.1831, -0.1348,  0.7734])),\n",
       "             ('model.encoder.layers.7.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0403,  0.2040, -0.1902,  ...,  0.1509, -0.1453, -0.1630],\n",
       "                      [-0.0783, -0.0748, -0.1115,  ...,  0.1044, -0.0614, -0.2024],\n",
       "                      [-0.0826, -0.0220,  0.1033,  ...,  0.1572, -0.1219,  0.0381],\n",
       "                      ...,\n",
       "                      [-0.1497, -0.0581, -0.2620,  ..., -0.0068, -0.0917,  0.1129],\n",
       "                      [-0.3176,  0.1254,  0.0963,  ...,  0.0521, -0.0949, -0.0911],\n",
       "                      [ 0.0673, -0.1017, -0.2588,  ...,  0.0936,  0.1602,  0.0665]])),\n",
       "             ('model.encoder.layers.7.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0198,  0.0307,  0.0190,  ...,  0.0079, -0.0173, -0.0075])),\n",
       "             ('model.encoder.layers.7.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.6250,  0.3130, -0.0906,  ...,  0.1709,  0.1100,  0.0266],\n",
       "                      [-0.3528,  0.0912, -0.4282,  ...,  0.2196, -0.1589, -0.0099],\n",
       "                      [-0.0517, -0.0556, -0.1302,  ...,  0.4043, -0.1920,  0.2158],\n",
       "                      ...,\n",
       "                      [-0.0071, -0.0645, -0.3860,  ..., -0.0900,  0.0782, -0.0291],\n",
       "                      [ 0.0308,  0.2062, -0.3958,  ...,  0.0598,  0.4902, -0.1539],\n",
       "                      [ 0.4363, -0.0987, -0.4602,  ...,  0.1437,  0.0404,  0.0533]])),\n",
       "             ('model.encoder.layers.7.self_attn.v_proj.bias',\n",
       "              tensor([ 0.0676,  0.4995, -0.2866,  ..., -0.1379, -0.1763,  0.2477])),\n",
       "             ('model.encoder.layers.7.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0831,  0.0398,  0.1328,  ...,  0.0081, -0.0090,  0.0434],\n",
       "                      [-0.1038,  0.0436, -0.0704,  ..., -0.0676,  0.1190, -0.1210],\n",
       "                      [ 0.0927,  0.0960,  0.0286,  ...,  0.0106,  0.1908,  0.1411],\n",
       "                      ...,\n",
       "                      [ 0.0671,  0.2081,  0.0379,  ...,  0.1769,  0.0137,  0.0703],\n",
       "                      [-0.0284,  0.0137,  0.2231,  ..., -0.0323,  0.2351,  0.2407],\n",
       "                      [ 0.0638,  0.0581, -0.0652,  ...,  0.0352,  0.0883,  0.0115]])),\n",
       "             ('model.encoder.layers.7.self_attn.q_proj.bias',\n",
       "              tensor([-0.0459,  0.5107,  0.0079,  ...,  0.2482,  0.0854,  0.0585])),\n",
       "             ('model.encoder.layers.7.self_attn.out_proj.weight',\n",
       "              tensor([[-0.3962,  0.1968,  0.2440,  ..., -0.2708,  0.0613,  0.5044],\n",
       "                      [-0.1608, -0.2025,  0.0812,  ...,  0.4180,  0.1942,  0.0641],\n",
       "                      [ 0.1642, -0.0959,  0.0797,  ...,  0.0237, -0.0937,  0.5010],\n",
       "                      ...,\n",
       "                      [-0.1365, -0.1866,  0.0842,  ..., -0.2437, -0.0931,  0.2573],\n",
       "                      [-0.1290,  0.1565,  0.2512,  ..., -0.3945,  0.0866, -0.0850],\n",
       "                      [-0.3518,  0.4900,  0.2866,  ...,  0.2408,  0.2727, -0.0621]])),\n",
       "             ('model.encoder.layers.7.self_attn.out_proj.bias',\n",
       "              tensor([-0.1945, -0.0603, -0.1410,  ..., -0.2971, -0.3428, -0.5000])),\n",
       "             ('model.encoder.layers.7.self_attn_layer_norm.weight',\n",
       "              tensor([0.2864, 0.2981, 0.3025,  ..., 0.2600, 0.2637, 0.1532])),\n",
       "             ('model.encoder.layers.7.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0018, -0.0353,  0.0016,  ..., -0.0442, -0.0206, -0.0556])),\n",
       "             ('model.encoder.layers.7.fc1.weight',\n",
       "              tensor([[ 0.3796, -0.2915,  0.0097,  ..., -0.1077,  0.2246,  0.0941],\n",
       "                      [ 0.2150,  0.0715, -0.2764,  ..., -0.3174, -0.0606, -0.1829],\n",
       "                      [-0.1274,  0.3252,  0.2388,  ..., -0.0192, -0.5010,  0.2556],\n",
       "                      ...,\n",
       "                      [-0.1538,  0.1974,  0.0007,  ...,  0.4119,  0.2036, -0.1060],\n",
       "                      [ 0.1875, -0.0032, -0.2629,  ..., -0.0845,  0.0578,  0.1394],\n",
       "                      [ 0.0598, -0.2800,  0.1261,  ..., -0.0494,  0.1498, -0.3682]])),\n",
       "             ('model.encoder.layers.7.fc1.bias',\n",
       "              tensor([-0.1786,  0.0657, -0.2452,  ..., -0.1400, -0.1942, -0.2534])),\n",
       "             ('model.encoder.layers.7.fc2.weight',\n",
       "              tensor([[-0.0225,  0.0469,  0.5161,  ..., -0.0067, -0.4446,  0.1309],\n",
       "                      [-0.1541,  0.0912, -0.1689,  ..., -0.0529, -0.1357, -0.1484],\n",
       "                      [-0.1372,  0.0805, -0.4978,  ..., -0.1472,  0.0931,  0.0875],\n",
       "                      ...,\n",
       "                      [ 0.3457,  0.2556,  0.1699,  ...,  0.0536,  0.0854,  0.0499],\n",
       "                      [ 0.2449, -0.0298,  0.0054,  ...,  0.1077, -0.1932, -0.1685],\n",
       "                      [-0.1469, -0.0487, -0.0607,  ...,  0.0502, -0.5054,  0.0828]])),\n",
       "             ('model.encoder.layers.7.fc2.bias',\n",
       "              tensor([-0.1750, -0.2800,  0.2668,  ..., -0.0279,  0.1831,  0.1775])),\n",
       "             ('model.encoder.layers.7.final_layer_norm.weight',\n",
       "              tensor([1.4658, 1.5234, 1.4697,  ..., 1.4619, 1.3975, 1.0264])),\n",
       "             ('model.encoder.layers.7.final_layer_norm.bias',\n",
       "              tensor([ 0.2140, -0.1531, -0.1310,  ...,  0.1624, -0.3196,  0.5806])),\n",
       "             ('model.encoder.layers.8.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0688, -0.0091, -0.1277,  ..., -0.0792, -0.1315,  0.2549],\n",
       "                      [ 0.0327, -0.0327,  0.1985,  ..., -0.1526, -0.1306, -0.0013],\n",
       "                      [ 0.0291, -0.2593,  0.0817,  ..., -0.0741,  0.0684, -0.0975],\n",
       "                      ...,\n",
       "                      [ 0.1205,  0.0697, -0.0167,  ...,  0.0344, -0.0850,  0.0958],\n",
       "                      [-0.0356, -0.0033,  0.0279,  ...,  0.2449,  0.3352,  0.3486],\n",
       "                      [-0.0198, -0.0017, -0.0009,  ..., -0.1373,  0.0296,  0.0921]])),\n",
       "             ('model.encoder.layers.8.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0190,  0.0186,  0.0075,  ..., -0.0027,  0.0239, -0.0230])),\n",
       "             ('model.encoder.layers.8.self_attn.v_proj.weight',\n",
       "              tensor([[ 2.1698e-02,  5.0391e-01, -6.3599e-02,  ...,  3.4375e-01,\n",
       "                       -5.6732e-02, -2.5439e-01],\n",
       "                      [-5.0098e-01,  4.8828e-01, -3.9917e-01,  ..., -6.4163e-03,\n",
       "                        2.7271e-01,  5.1239e-02],\n",
       "                      [ 5.0342e-01, -2.5781e-01, -9.2224e-02,  ..., -2.5854e-01,\n",
       "                        3.8013e-01,  2.5391e-01],\n",
       "                      ...,\n",
       "                      [-2.3806e-04, -2.4878e-01, -3.6865e-01,  ...,  1.6638e-01,\n",
       "                        4.3030e-02,  1.0706e-01],\n",
       "                      [-7.1472e-02,  5.3406e-02,  3.3667e-01,  ..., -4.8535e-01,\n",
       "                       -4.1321e-02, -2.5244e-01],\n",
       "                      [ 5.5518e-01, -6.1310e-02,  4.2041e-01,  ...,  2.2180e-01,\n",
       "                        1.3086e-01, -1.9629e-01]])),\n",
       "             ('model.encoder.layers.8.self_attn.v_proj.bias',\n",
       "              tensor([ 0.0462, -0.1036, -0.0646,  ..., -0.1625,  0.0988,  0.0123])),\n",
       "             ('model.encoder.layers.8.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0249, -0.0852, -0.0412,  ...,  0.1812,  0.1111,  0.2671],\n",
       "                      [-0.0025,  0.0505, -0.0931,  ...,  0.1137, -0.0060, -0.0775],\n",
       "                      [ 0.2181,  0.0528,  0.0793,  ...,  0.2654,  0.0399, -0.0110],\n",
       "                      ...,\n",
       "                      [-0.0602,  0.0107, -0.1671,  ...,  0.1301,  0.2366,  0.0963],\n",
       "                      [-0.0008,  0.1672,  0.2468,  ..., -0.1220, -0.1705, -0.0371],\n",
       "                      [-0.1020,  0.0135,  0.0702,  ..., -0.0245,  0.1068, -0.0165]])),\n",
       "             ('model.encoder.layers.8.self_attn.q_proj.bias',\n",
       "              tensor([ 0.4907, -0.1603, -0.2605,  ...,  0.0511,  0.5312,  0.1677])),\n",
       "             ('model.encoder.layers.8.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.5000, -0.0681,  0.1069,  ..., -0.0848, -0.1550,  0.4285],\n",
       "                      [-0.2615,  0.5898,  0.2981,  ..., -0.0019, -0.3152, -0.5073],\n",
       "                      [-0.1832,  0.0960,  0.3252,  ..., -0.1443, -0.3098,  0.2847],\n",
       "                      ...,\n",
       "                      [-0.0236,  0.0312,  0.1584,  ..., -0.2113, -0.5024,  0.0351],\n",
       "                      [ 0.4995,  0.0998,  0.1969,  ...,  0.3953,  0.0782, -0.1827],\n",
       "                      [ 0.4517, -0.3076, -0.0215,  ...,  0.5000, -0.2598,  0.3438]])),\n",
       "             ('model.encoder.layers.8.self_attn.out_proj.bias',\n",
       "              tensor([ 0.1201,  0.2529, -0.2769,  ..., -0.3010,  0.0786, -0.0475])),\n",
       "             ('model.encoder.layers.8.self_attn_layer_norm.weight',\n",
       "              tensor([0.3359, 0.3503, 0.3406,  ..., 0.3118, 0.3076, 0.1715])),\n",
       "             ('model.encoder.layers.8.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0137, -0.0487, -0.0163,  ..., -0.0402, -0.0260, -0.1025])),\n",
       "             ('model.encoder.layers.8.fc1.weight',\n",
       "              tensor([[-0.3015, -0.1304,  0.0669,  ..., -0.0325, -0.2152, -0.0944],\n",
       "                      [-0.2681, -0.3765,  0.2029,  ...,  0.0684, -0.3738,  0.5010],\n",
       "                      [ 0.1047, -0.2003, -0.0580,  ...,  0.0778, -0.3823,  0.1543],\n",
       "                      ...,\n",
       "                      [ 0.2312, -0.0673, -0.2435,  ..., -0.1077,  0.1111,  0.4194],\n",
       "                      [-0.0049, -0.4768,  0.1390,  ...,  0.1024,  0.1753,  0.2595],\n",
       "                      [-0.2419, -0.3213, -0.1992,  ..., -0.2561, -0.0931,  0.1797]])),\n",
       "             ('model.encoder.layers.8.fc1.bias',\n",
       "              tensor([ 0.0402, -0.1921,  0.0009,  ..., -0.1022, -0.2727, -0.0250])),\n",
       "             ('model.encoder.layers.8.fc2.weight',\n",
       "              tensor([[ 2.2571e-01,  5.2917e-02,  3.8025e-02,  ...,  1.0181e-01,\n",
       "                       -2.2070e-01,  8.1055e-02],\n",
       "                      [-2.0561e-03, -7.6782e-02,  1.0541e-01,  ...,  3.5132e-01,\n",
       "                       -4.7266e-01,  1.1688e-01],\n",
       "                      [ 7.1564e-03, -2.0728e-01, -2.0337e-01,  ...,  1.1371e-01,\n",
       "                       -2.3010e-01,  1.0876e-01],\n",
       "                      ...,\n",
       "                      [-2.3773e-02,  1.3578e-04, -8.6914e-02,  ...,  7.0740e-02,\n",
       "                       -2.6953e-01,  2.6047e-02],\n",
       "                      [ 9.0515e-02, -1.7542e-01,  3.3740e-01,  ...,  2.4524e-01,\n",
       "                        1.2427e-01, -1.3501e-01],\n",
       "                      [-9.7717e-02, -6.4453e-02,  9.1858e-02,  ...,  1.1902e-02,\n",
       "                       -2.8101e-01,  2.2546e-01]])),\n",
       "             ('model.encoder.layers.8.fc2.bias',\n",
       "              tensor([-0.2666, -0.3450,  0.4014,  ..., -0.3533,  0.4529,  0.5000])),\n",
       "             ('model.encoder.layers.8.final_layer_norm.weight',\n",
       "              tensor([1.7168, 1.6738, 1.7178,  ..., 1.6064, 1.6738, 1.0039])),\n",
       "             ('model.encoder.layers.8.final_layer_norm.bias',\n",
       "              tensor([ 0.3455, -0.1840, -0.2661,  ...,  0.2267, -0.2754,  0.4849])),\n",
       "             ('model.encoder.layers.9.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0934,  0.0262,  0.0886,  ...,  0.1398,  0.0416,  0.0706],\n",
       "                      [ 0.0422, -0.0273,  0.0023,  ...,  0.0563, -0.0426, -0.0468],\n",
       "                      [ 0.0401,  0.0974, -0.2075,  ...,  0.0203, -0.0175, -0.0652],\n",
       "                      ...,\n",
       "                      [-0.1060,  0.0012, -0.0681,  ...,  0.2020, -0.1016, -0.1328],\n",
       "                      [-0.0526,  0.1748, -0.0415,  ...,  0.1202,  0.0574, -0.2179],\n",
       "                      [-0.1506, -0.1725,  0.0459,  ..., -0.1196, -0.1444, -0.0065]])),\n",
       "             ('model.encoder.layers.9.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0083,  0.0081,  0.0163,  ..., -0.0311,  0.0125, -0.0013])),\n",
       "             ('model.encoder.layers.9.self_attn.v_proj.weight',\n",
       "              tensor([[-0.2930, -0.3538,  0.0164,  ...,  0.4600,  0.1323, -0.1270],\n",
       "                      [ 0.0593, -0.1552, -0.1039,  ..., -0.1021, -0.5010, -0.3408],\n",
       "                      [ 0.5088, -0.0402, -0.3328,  ...,  0.3240,  0.4419, -0.1626],\n",
       "                      ...,\n",
       "                      [ 0.1453,  0.3970, -0.5234,  ..., -0.2471, -0.4275, -0.0396],\n",
       "                      [ 0.2164,  0.4973, -0.1378,  ...,  0.0124, -0.0421,  0.1594],\n",
       "                      [-0.1237, -0.2634, -0.2815,  ...,  0.0323,  0.0985, -0.0444]])),\n",
       "             ('model.encoder.layers.9.self_attn.v_proj.bias',\n",
       "              tensor([-0.0606,  0.2057,  0.1285,  ...,  0.0414, -0.0179, -0.1126])),\n",
       "             ('model.encoder.layers.9.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0533, -0.0367, -0.0298,  ..., -0.1113, -0.1151,  0.0941],\n",
       "                      [-0.1364,  0.1113, -0.1127,  ..., -0.2859,  0.1389,  0.1482],\n",
       "                      [-0.0801, -0.3044, -0.0587,  ...,  0.1190,  0.0527, -0.0526],\n",
       "                      ...,\n",
       "                      [ 0.0027,  0.0098,  0.0071,  ..., -0.0561, -0.0421,  0.0130],\n",
       "                      [-0.0845, -0.0067,  0.0457,  ..., -0.0438,  0.0351,  0.0575],\n",
       "                      [ 0.0757,  0.1078,  0.1119,  ...,  0.0146, -0.0102,  0.1037]])),\n",
       "             ('model.encoder.layers.9.self_attn.q_proj.bias',\n",
       "              tensor([-0.0395,  0.2544,  0.0701,  ..., -0.0873, -0.4170,  0.1514])),\n",
       "             ('model.encoder.layers.9.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.1595,  0.1053,  0.3035,  ..., -0.3784, -0.0549, -0.2336],\n",
       "                      [-0.2150, -0.3525,  0.2207,  ..., -0.0771, -0.3467,  0.3674],\n",
       "                      [ 0.0105,  0.2457,  0.1207,  ...,  0.3616, -0.4292,  0.0353],\n",
       "                      ...,\n",
       "                      [ 0.5269, -0.0899, -0.2500,  ...,  0.1394, -0.0237,  0.4280],\n",
       "                      [ 0.0251, -0.3645,  0.2576,  ..., -0.2942, -0.0091, -0.1046],\n",
       "                      [-0.3721,  0.2549, -0.1851,  ...,  0.2076, -0.0285, -0.1213]])),\n",
       "             ('model.encoder.layers.9.self_attn.out_proj.bias',\n",
       "              tensor([ 0.1194,  0.1234, -0.3315,  ..., -0.1862, -0.2507, -0.3210])),\n",
       "             ('model.encoder.layers.9.self_attn_layer_norm.weight',\n",
       "              tensor([0.3367, 0.3257, 0.3416,  ..., 0.3267, 0.3186, 0.2015])),\n",
       "             ('model.encoder.layers.9.self_attn_layer_norm.bias',\n",
       "              tensor([-0.0108, -0.0378, -0.0120,  ..., -0.0397, -0.0296,  0.0007])),\n",
       "             ('model.encoder.layers.9.fc1.weight',\n",
       "              tensor([[-0.0850, -0.0334,  0.3296,  ..., -0.1797,  0.3176, -0.1475],\n",
       "                      [ 0.2062,  0.0526,  0.4924,  ...,  0.3694,  0.2479, -0.2455],\n",
       "                      [-0.1028, -0.2615, -0.3381,  ...,  0.1091, -0.3706,  0.1146],\n",
       "                      ...,\n",
       "                      [ 0.1309, -0.0182, -0.2957,  ...,  0.1622,  0.2084,  0.1324],\n",
       "                      [-0.0830, -0.2047, -0.0750,  ..., -0.1594, -0.1129, -0.0185],\n",
       "                      [ 0.2568,  0.1365, -0.0729,  ...,  0.1531,  0.0869,  0.1686]])),\n",
       "             ('model.encoder.layers.9.fc1.bias',\n",
       "              tensor([-0.0652, -0.2603, -0.2491,  ..., -0.2477, -0.1270,  0.0648])),\n",
       "             ('model.encoder.layers.9.fc2.weight',\n",
       "              tensor([[ 0.0651, -0.1250,  0.2908,  ..., -0.2876,  0.0086, -0.3081],\n",
       "                      [-0.4285,  0.4309, -0.1884,  ..., -0.0551, -0.2028, -0.0470],\n",
       "                      [ 0.1276, -0.3735, -0.4778,  ..., -0.2922,  0.0392,  0.0829],\n",
       "                      ...,\n",
       "                      [-0.3789,  0.1226,  0.0482,  ...,  0.4937,  0.3669, -0.0059],\n",
       "                      [-0.2620,  0.1331, -0.2130,  ..., -0.1647, -0.0382,  0.0402],\n",
       "                      [-0.1763, -0.3599,  0.0088,  ..., -0.2683, -0.1316,  0.0857]])),\n",
       "             ('model.encoder.layers.9.fc2.bias',\n",
       "              tensor([-0.2073, -0.3306,  0.3516,  ..., -0.1648,  0.4053,  0.5000])),\n",
       "             ('model.encoder.layers.9.final_layer_norm.weight',\n",
       "              tensor([1.8896, 1.8975, 1.8369,  ..., 1.7158, 1.8867, 1.0020])),\n",
       "             ('model.encoder.layers.9.final_layer_norm.bias',\n",
       "              tensor([ 0.3481, -0.0704, -0.3877,  ...,  0.0132, -0.2754,  0.4976])),\n",
       "             ('model.encoder.layers.10.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0754, -0.0750, -0.0309,  ..., -0.0340,  0.1871, -0.0750],\n",
       "                      [-0.1177,  0.2014,  0.1215,  ...,  0.3188, -0.0710, -0.0375],\n",
       "                      [ 0.1743, -0.0444,  0.1678,  ..., -0.1851,  0.1493,  0.0254],\n",
       "                      ...,\n",
       "                      [-0.1746, -0.0400,  0.0757,  ...,  0.1132, -0.1665,  0.1760],\n",
       "                      [ 0.0428, -0.0995, -0.0690,  ...,  0.0299, -0.1648,  0.1179],\n",
       "                      [-0.0603,  0.0314, -0.0255,  ...,  0.0553,  0.0923,  0.2301]])),\n",
       "             ('model.encoder.layers.10.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0078, -0.0056, -0.0106,  ..., -0.0212, -0.0186, -0.0229])),\n",
       "             ('model.encoder.layers.10.self_attn.v_proj.weight',\n",
       "              tensor([[-0.1522,  0.2383,  0.1865,  ...,  0.2905,  0.0820,  0.0166],\n",
       "                      [ 0.2666,  0.1146, -0.0781,  ...,  0.1699, -0.3335,  0.1304],\n",
       "                      [-0.4468, -0.3083,  0.0360,  ..., -0.0930, -0.3240, -0.0488],\n",
       "                      ...,\n",
       "                      [ 0.0994, -0.2683,  0.5088,  ..., -0.0464, -0.1176,  0.3032],\n",
       "                      [ 0.2908,  0.2324, -0.0009,  ..., -0.1620,  0.3689,  0.2500],\n",
       "                      [ 0.0656, -0.1532, -0.0412,  ...,  0.4946, -0.2261, -0.2732]])),\n",
       "             ('model.encoder.layers.10.self_attn.v_proj.bias',\n",
       "              tensor([-0.0977, -0.1240,  0.0131,  ..., -0.0121,  0.0525, -0.0734])),\n",
       "             ('model.encoder.layers.10.self_attn.q_proj.weight',\n",
       "              tensor([[-0.1930,  0.0522,  0.0082,  ...,  0.0521,  0.0998, -0.0263],\n",
       "                      [-0.0575,  0.1549, -0.1658,  ..., -0.1860,  0.2107, -0.1044],\n",
       "                      [-0.0764,  0.0324,  0.0944,  ...,  0.1270, -0.0540, -0.0314],\n",
       "                      ...,\n",
       "                      [-0.0464, -0.0764,  0.0583,  ...,  0.2264,  0.1029,  0.0735],\n",
       "                      [ 0.0351, -0.0892, -0.0684,  ...,  0.1586,  0.0099, -0.1727],\n",
       "                      [-0.0319, -0.0603, -0.1011,  ..., -0.3140,  0.0933, -0.0415]])),\n",
       "             ('model.encoder.layers.10.self_attn.q_proj.bias',\n",
       "              tensor([-0.3320,  0.1873,  0.0091,  ...,  0.1371,  0.0784, -0.0020])),\n",
       "             ('model.encoder.layers.10.self_attn.out_proj.weight',\n",
       "              tensor([[-0.5034, -0.2983,  0.0629,  ...,  0.0390, -0.5015, -0.2040],\n",
       "                      [ 0.1003,  0.1798,  0.1031,  ..., -0.3835,  0.0084,  0.5010],\n",
       "                      [ 0.1758,  0.1630, -0.2917,  ...,  0.0962,  0.4963,  0.5000],\n",
       "                      ...,\n",
       "                      [ 0.1755,  0.3667,  0.3611,  ..., -0.2289,  0.1060, -0.2825],\n",
       "                      [-0.2671, -0.4214, -0.3074,  ..., -0.2625, -0.3059,  0.0285],\n",
       "                      [ 0.4990,  0.3005, -0.4709,  ...,  0.2185, -0.4902,  0.2612]])),\n",
       "             ('model.encoder.layers.10.self_attn.out_proj.bias',\n",
       "              tensor([-0.0365,  0.1382, -0.2482,  ..., -0.3245, -0.1206, -0.3130])),\n",
       "             ('model.encoder.layers.10.self_attn_layer_norm.weight',\n",
       "              tensor([0.3611, 0.3506, 0.3416,  ..., 0.3398, 0.3438, 0.2307])),\n",
       "             ('model.encoder.layers.10.self_attn_layer_norm.bias',\n",
       "              tensor([-0.0012, -0.0452, -0.0125,  ..., -0.0408, -0.0147,  0.0466])),\n",
       "             ('model.encoder.layers.10.fc1.weight',\n",
       "              tensor([[ 0.1156, -0.0344,  0.1548,  ..., -0.2179,  0.3853, -0.0242],\n",
       "                      [ 0.3975, -0.1139,  0.0091,  ..., -0.0827,  0.4434, -0.1761],\n",
       "                      [ 0.3960,  0.0216, -0.0314,  ...,  0.0225, -0.2169, -0.0543],\n",
       "                      ...,\n",
       "                      [ 0.1395, -0.3721,  0.1024,  ..., -0.0687,  0.1179, -0.2325],\n",
       "                      [-0.1383, -0.0200,  0.1689,  ...,  0.2786, -0.1664, -0.4736],\n",
       "                      [-0.0500, -0.1879,  0.0883,  ...,  0.0870,  0.1624, -0.0334]])),\n",
       "             ('model.encoder.layers.10.fc1.bias',\n",
       "              tensor([-0.1774, -0.2493,  0.0089,  ...,  0.0812, -0.1042, -0.0112])),\n",
       "             ('model.encoder.layers.10.fc2.weight',\n",
       "              tensor([[ 0.0633,  0.3196, -0.3215,  ..., -0.1116,  0.1827,  0.2673],\n",
       "                      [ 0.0570, -0.2937, -0.0856,  ...,  0.1217, -0.1844,  0.2048],\n",
       "                      [-0.4844, -0.1295, -0.0923,  ...,  0.0325, -0.0368, -0.0098],\n",
       "                      ...,\n",
       "                      [-0.0031,  0.2913, -0.1672,  ...,  0.0862,  0.0590, -0.1099],\n",
       "                      [-0.2598,  0.1019,  0.2947,  ..., -0.1364, -0.1061,  0.0092],\n",
       "                      [ 0.1287, -0.1024,  0.1221,  ..., -0.0401,  0.0828, -0.0964]])),\n",
       "             ('model.encoder.layers.10.fc2.bias',\n",
       "              tensor([-0.1252, -0.2593,  0.1232,  ...,  0.0485,  0.2494,  0.5000])),\n",
       "             ('model.encoder.layers.10.final_layer_norm.weight',\n",
       "              tensor([1.7891, 1.7773, 1.8184,  ..., 1.7373, 1.7686, 1.0000])),\n",
       "             ('model.encoder.layers.10.final_layer_norm.bias',\n",
       "              tensor([ 0.3123, -0.1324, -0.2803,  ..., -0.2485, -0.1224,  0.3059])),\n",
       "             ('model.encoder.layers.11.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0086, -0.0414, -0.3499,  ...,  0.1313,  0.1610,  0.0051],\n",
       "                      [ 0.1420,  0.0161, -0.0805,  ...,  0.0367,  0.0750, -0.0147],\n",
       "                      [ 0.1501,  0.0788, -0.0873,  ..., -0.0138,  0.0100, -0.0401],\n",
       "                      ...,\n",
       "                      [-0.2258,  0.1349, -0.0797,  ...,  0.0210, -0.0510, -0.0725],\n",
       "                      [ 0.0133, -0.2283, -0.1971,  ...,  0.2808, -0.1448,  0.1022],\n",
       "                      [-0.2223,  0.0643, -0.2065,  ..., -0.0892,  0.0763,  0.0452]])),\n",
       "             ('model.encoder.layers.11.self_attn.k_proj.bias',\n",
       "              tensor([-0.0247, -0.0124,  0.0158,  ...,  0.0298,  0.0184, -0.0053])),\n",
       "             ('model.encoder.layers.11.self_attn.v_proj.weight',\n",
       "              tensor([[-0.3228, -0.2213, -0.4314,  ..., -0.1243,  0.0023, -0.1805],\n",
       "                      [-0.2544,  0.2683,  0.0088,  ..., -0.3044,  0.3425, -0.0644],\n",
       "                      [ 0.0420,  0.0554, -0.0193,  ..., -0.2462,  0.1820, -0.2064],\n",
       "                      ...,\n",
       "                      [-0.3894,  0.0691, -0.2698,  ...,  0.0479,  0.0766,  0.0311],\n",
       "                      [-0.5674, -0.0828, -0.1783,  ..., -0.1588,  0.3806, -0.0439],\n",
       "                      [ 0.2032,  0.0678, -0.4326,  ..., -0.1093, -0.0641,  0.1697]])),\n",
       "             ('model.encoder.layers.11.self_attn.v_proj.bias',\n",
       "              tensor([ 0.0423,  0.0190, -0.0619,  ..., -0.0843,  0.0345, -0.0243])),\n",
       "             ('model.encoder.layers.11.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.1095, -0.1726,  0.1010,  ..., -0.0748, -0.0682, -0.0528],\n",
       "                      [ 0.0183, -0.0203,  0.2111,  ...,  0.5200,  0.1798, -0.0073],\n",
       "                      [ 0.1667,  0.0603, -0.1227,  ...,  0.1139,  0.0238,  0.0292],\n",
       "                      ...,\n",
       "                      [-0.1715,  0.1740,  0.0278,  ..., -0.0693,  0.1809, -0.0255],\n",
       "                      [-0.0011, -0.0814,  0.0205,  ...,  0.0806, -0.0358,  0.1978],\n",
       "                      [ 0.0307, -0.0151,  0.0194,  ..., -0.1848, -0.0357,  0.0113]])),\n",
       "             ('model.encoder.layers.11.self_attn.q_proj.bias',\n",
       "              tensor([-0.0655,  0.1114, -0.0804,  ..., -0.0082,  0.4026, -0.2406])),\n",
       "             ('model.encoder.layers.11.self_attn.out_proj.weight',\n",
       "              tensor([[-0.1425, -0.1304,  0.2463,  ...,  0.2605,  0.0704,  0.5000],\n",
       "                      [-0.3113,  0.3472,  0.5015,  ...,  0.2778,  0.4353,  0.2678],\n",
       "                      [-0.1364, -0.0914, -0.2534,  ..., -0.2903, -0.2021,  0.0350],\n",
       "                      ...,\n",
       "                      [-0.3306, -0.3472,  0.4177,  ...,  0.0071, -0.5371, -0.0557],\n",
       "                      [ 0.4880, -0.2247, -0.1022,  ...,  0.0623, -0.1614, -0.0088],\n",
       "                      [-0.0360, -0.0154,  0.0797,  ...,  0.0604, -0.0828,  0.0997]])),\n",
       "             ('model.encoder.layers.11.self_attn.out_proj.bias',\n",
       "              tensor([-0.0739, -0.0063, -0.0346,  ..., -0.2489, -0.1971, -0.2751])),\n",
       "             ('model.encoder.layers.11.self_attn_layer_norm.weight',\n",
       "              tensor([0.3723, 0.3530, 0.3521,  ..., 0.3430, 0.3640, 0.5117])),\n",
       "             ('model.encoder.layers.11.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0076, -0.0373, -0.0152,  ..., -0.0352,  0.0002,  0.1772])),\n",
       "             ('model.encoder.layers.11.fc1.weight',\n",
       "              tensor([[ 2.5732e-01, -1.6187e-01, -4.4678e-01,  ...,  5.9845e-02,\n",
       "                       -3.4668e-01, -6.9824e-02],\n",
       "                      [ 1.7688e-01, -1.4697e-01,  3.6548e-01,  ..., -4.4287e-01,\n",
       "                       -1.0376e-01, -8.9600e-02],\n",
       "                      [-2.4670e-01,  2.3022e-01,  3.3618e-01,  ..., -1.1487e-01,\n",
       "                       -3.8867e-01,  1.1646e-01],\n",
       "                      ...,\n",
       "                      [-1.4368e-01, -6.0242e-02,  3.1592e-01,  ...,  7.5500e-02,\n",
       "                        1.4355e-01,  8.1177e-02],\n",
       "                      [ 4.7302e-02,  3.1885e-01,  1.1621e-01,  ..., -6.1920e-02,\n",
       "                       -9.5032e-02,  5.9395e-03],\n",
       "                      [-3.4888e-01, -3.6890e-01, -3.6548e-01,  ...,  2.2339e-01,\n",
       "                        2.6584e-04,  1.1902e-01]])),\n",
       "             ('model.encoder.layers.11.fc1.bias',\n",
       "              tensor([-0.2379, -0.0254, -0.0665,  ..., -0.1392, -0.1793, -0.1711])),\n",
       "             ('model.encoder.layers.11.fc2.weight',\n",
       "              tensor([[-0.0247, -0.5405,  0.3447,  ...,  0.0279,  0.1028,  0.2759],\n",
       "                      [ 0.1445,  0.2554,  0.0915,  ..., -0.1010, -0.0536,  0.1388],\n",
       "                      [-0.0830, -0.0148, -0.2334,  ..., -0.0434, -0.1500,  0.2351],\n",
       "                      ...,\n",
       "                      [-0.4978,  0.0639, -0.3618,  ..., -0.0822,  0.0499,  0.0111],\n",
       "                      [-0.1675, -0.2073, -0.1583,  ..., -0.1167, -0.0303,  0.3142],\n",
       "                      [-0.0075,  0.0165,  0.0419,  ..., -0.0137,  0.0292, -0.0189]])),\n",
       "             ('model.encoder.layers.11.fc2.bias',\n",
       "              tensor([-0.0811, -0.1230,  0.0258,  ...,  0.1039,  0.1157,  0.2913])),\n",
       "             ('model.encoder.layers.11.final_layer_norm.weight',\n",
       "              tensor([1.3672, 1.2451, 1.1787,  ..., 1.4297, 1.2148, 1.0000])),\n",
       "             ('model.encoder.layers.11.final_layer_norm.bias',\n",
       "              tensor([ 0.1785, -0.0873, -0.0390,  ..., -0.2549,  0.0047, -0.2499])),\n",
       "             ('model.encoder.layer_norm.weight',\n",
       "              tensor([0.4553, 0.4626, 0.4673,  ..., 0.4241, 0.4702, 0.7456])),\n",
       "             ('model.encoder.layer_norm.bias',\n",
       "              tensor([ 0.0014, -0.0053,  0.0018,  ..., -0.0308,  0.0012, -0.5000])),\n",
       "             ('model.decoder.embed_tokens.weight',\n",
       "              tensor([[-inf, inf, inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf]])),\n",
       "             ('model.decoder.layers.0.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.2527, -0.0136, -0.0460,  ...,  0.0115,  0.2996, -0.2917],\n",
       "                      [-0.2238, -0.0130,  0.0161,  ..., -0.0285, -0.1180, -0.1647],\n",
       "                      [-0.0419,  0.1165,  0.0529,  ..., -0.1235, -0.1420, -0.0377],\n",
       "                      ...,\n",
       "                      [-0.1418, -0.2262, -0.2668,  ..., -0.4019,  0.1725, -0.4397],\n",
       "                      [ 0.8843,  0.7051, -0.0199,  ...,  0.0214, -0.0945, -0.0924],\n",
       "                      [ 0.5151,  0.7905,  0.5708,  ..., -0.1842,  0.1663, -0.4202]])),\n",
       "             ('model.decoder.layers.0.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0171,  0.0190, -0.0028,  ...,  0.0214,  0.0051,  0.0162])),\n",
       "             ('model.decoder.layers.0.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.1139, -0.0122,  0.0055,  ..., -0.0195, -0.0603, -0.2316],\n",
       "                      [-0.0013,  0.0484, -0.0739,  ...,  0.0040,  0.0352,  0.0255],\n",
       "                      [ 0.0599, -0.0119, -0.0344,  ...,  0.0804,  0.0247, -0.1047],\n",
       "                      ...,\n",
       "                      [-0.0121, -0.1064, -0.1040,  ...,  0.1565, -0.1312, -0.0582],\n",
       "                      [ 0.0189, -0.0357, -0.0127,  ...,  0.1639, -0.0505,  0.2944],\n",
       "                      [ 0.0603,  0.0012, -0.0043,  ...,  0.0229, -0.0510, -0.0253]])),\n",
       "             ('model.decoder.layers.0.self_attn.v_proj.bias',\n",
       "              tensor([-0.1753, -0.0671,  0.0499,  ...,  0.0657, -0.1055,  0.1422])),\n",
       "             ('model.decoder.layers.0.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0608, -0.0219, -0.0292,  ..., -0.2030,  0.2261,  0.2169],\n",
       "                      [ 0.1031,  0.2079, -0.1537,  ...,  0.4563, -0.2065, -0.2493],\n",
       "                      [-0.0536, -0.1057,  0.1284,  ...,  0.4861,  0.1749,  0.3396],\n",
       "                      ...,\n",
       "                      [-0.5005, -0.5000, -0.5034,  ...,  0.0190,  0.0634,  0.2839],\n",
       "                      [ 0.7227,  0.9077,  0.5024,  ..., -0.3882,  0.0387, -0.2537],\n",
       "                      [ 0.4541,  0.6118,  0.9834,  ...,  0.1675, -0.1345,  0.0667]])),\n",
       "             ('model.decoder.layers.0.self_attn.q_proj.bias',\n",
       "              tensor([ 0.1714, -0.2122,  0.0087,  ..., -0.0495, -0.3044, -0.4622])),\n",
       "             ('model.decoder.layers.0.self_attn.out_proj.weight',\n",
       "              tensor([[-0.0922, -0.0627,  0.0594,  ..., -0.0709, -0.0629,  0.0347],\n",
       "                      [-0.0212,  0.0095,  0.0910,  ..., -0.0070,  0.0226,  0.1028],\n",
       "                      [ 0.0100, -0.0276,  0.0167,  ...,  0.0674,  0.0024, -0.0916],\n",
       "                      ...,\n",
       "                      [-0.0880, -0.1554, -0.0174,  ...,  0.0215, -0.1620,  0.0854],\n",
       "                      [-0.1394,  0.1870,  0.1448,  ...,  0.1100, -0.1603,  0.1107],\n",
       "                      [-0.1072,  0.2668,  0.0040,  ..., -0.1271, -0.0542,  0.1467]])),\n",
       "             ('model.decoder.layers.0.self_attn.out_proj.bias',\n",
       "              tensor([-0.1847, -0.5151, -0.2632,  ...,  0.0610,  0.0335,  0.0851])),\n",
       "             ('model.decoder.layers.0.self_attn_layer_norm.weight',\n",
       "              tensor([0.2671, 1.0840, 1.7480,  ..., 0.0716, 0.1120, 0.1166])),\n",
       "             ('model.decoder.layers.0.self_attn_layer_norm.bias',\n",
       "              tensor([-0.0013,  0.0142,  0.0267,  ...,  0.0082,  0.0058,  0.0043])),\n",
       "             ('model.decoder.layers.0.encoder_attn.k_proj.weight',\n",
       "              tensor([[ 2.7686e-01,  1.1139e-01,  1.2846e-03,  ...,  1.1462e-01,\n",
       "                       -3.6523e-01,  2.1698e-02],\n",
       "                      [ 3.3984e-01,  5.4596e-02,  1.8262e-01,  ..., -1.6614e-01,\n",
       "                        5.4962e-02,  9.5703e-02],\n",
       "                      [-3.4485e-02,  3.1982e-02,  2.4927e-01,  ...,  1.3464e-01,\n",
       "                       -1.5210e-01,  2.5131e-02],\n",
       "                      ...,\n",
       "                      [ 1.2830e-01, -9.8999e-02, -4.3060e-02,  ..., -1.1230e-01,\n",
       "                        4.0527e-02,  3.7445e-02],\n",
       "                      [-6.1572e-01,  3.0398e-05, -5.9692e-02,  ..., -2.1881e-02,\n",
       "                        9.4528e-03,  9.6970e-03],\n",
       "                      [-4.3845e-04, -4.5288e-02,  7.2693e-02,  ..., -2.0020e-02,\n",
       "                       -1.5234e-01, -4.1809e-02]])),\n",
       "             ('model.decoder.layers.0.encoder_attn.k_proj.bias',\n",
       "              tensor([-0.0035,  0.0259,  0.0131,  ...,  0.0161, -0.0030, -0.0134])),\n",
       "             ('model.decoder.layers.0.encoder_attn.v_proj.weight',\n",
       "              tensor([[-2.2168e-01,  1.9531e-01, -2.2302e-01,  ..., -1.5430e-01,\n",
       "                       -1.2484e-03, -3.5038e-03],\n",
       "                      [-3.2806e-02, -1.0162e-01, -2.2998e-01,  ...,  1.2439e-01,\n",
       "                        1.1713e-01,  2.1912e-02],\n",
       "                      [ 8.3557e-02, -7.8430e-03,  4.4479e-03,  ..., -7.7515e-02,\n",
       "                        1.2612e-04,  6.9237e-03],\n",
       "                      ...,\n",
       "                      [-1.2396e-01, -4.8584e-02,  6.2744e-02,  ...,  1.0675e-01,\n",
       "                       -1.0156e-01, -3.1647e-02],\n",
       "                      [ 1.4001e-01, -7.8613e-02, -2.6871e-02,  ..., -1.4417e-01,\n",
       "                        1.5289e-02,  5.5725e-02],\n",
       "                      [ 7.7400e-03,  1.3008e-02,  1.4258e-01,  ..., -1.0828e-01,\n",
       "                        1.8225e-01, -2.6154e-02]])),\n",
       "             ('model.decoder.layers.0.encoder_attn.v_proj.bias',\n",
       "              tensor([ 0.0178, -0.0514, -0.0856,  ..., -0.0892, -0.1333,  0.0561])),\n",
       "             ('model.decoder.layers.0.encoder_attn.q_proj.weight',\n",
       "              tensor([[ 0.0181,  0.1831, -0.3489,  ...,  0.0055,  0.0179,  0.3159],\n",
       "                      [-0.0963,  0.0248, -0.2091,  ..., -0.0937, -0.1661,  0.0223],\n",
       "                      [-0.3262,  0.1173,  0.0812,  ..., -0.0140, -0.0066,  0.2837],\n",
       "                      ...,\n",
       "                      [ 0.1371, -0.1003,  0.0622,  ...,  0.0039, -0.0370, -0.1047],\n",
       "                      [-0.5054,  0.0831, -0.1569,  ..., -0.0121,  0.0830, -0.0902],\n",
       "                      [-0.2988, -0.0426,  0.0012,  ...,  0.2725,  0.2629, -0.3853]])),\n",
       "             ('model.decoder.layers.0.encoder_attn.q_proj.bias',\n",
       "              tensor([-0.0820,  0.0203,  0.0393,  ...,  0.0010, -0.1102, -0.0185])),\n",
       "             ('model.decoder.layers.0.encoder_attn.out_proj.weight',\n",
       "              tensor([[ 5.9479e-02,  5.2246e-02,  1.3879e-01,  ..., -9.1675e-02,\n",
       "                       -2.8214e-02,  1.6602e-02],\n",
       "                      [ 4.7424e-02,  9.0332e-02, -1.6830e-02,  ..., -4.2999e-02,\n",
       "                        5.8899e-02, -5.5199e-03],\n",
       "                      [ 5.9357e-02, -1.4062e-01,  1.0791e-01,  ...,  7.4707e-02,\n",
       "                       -3.4363e-02, -7.2876e-02],\n",
       "                      ...,\n",
       "                      [-1.1938e-01, -1.7102e-01, -6.7566e-02,  ...,  5.9052e-02,\n",
       "                        5.6091e-02,  7.9163e-02],\n",
       "                      [ 2.0984e-01,  8.2825e-02,  1.6260e-01,  ...,  4.7180e-02,\n",
       "                       -1.4214e-02,  1.7029e-01],\n",
       "                      [ 4.0222e-02,  1.0779e-01, -3.9624e-01,  ...,  9.2896e-02,\n",
       "                       -4.1656e-02, -9.8169e-05]])),\n",
       "             ('model.decoder.layers.0.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.2084,  0.1163, -0.0021,  ..., -0.0227,  0.0979,  0.2081])),\n",
       "             ('model.decoder.layers.0.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1486, 0.3196, 0.3987,  ..., 0.1199, 0.0912, 0.0984])),\n",
       "             ('model.decoder.layers.0.encoder_attn_layer_norm.bias',\n",
       "              tensor([-0.0154, -0.0182, -0.0321,  ..., -0.0105, -0.0070, -0.0171])),\n",
       "             ('model.decoder.layers.0.fc1.weight',\n",
       "              tensor([[ 0.1344,  0.7622,  0.2412,  ...,  0.2900, -0.2639, -0.3789],\n",
       "                      [-0.0172,  0.1448, -0.2893,  ...,  0.1109, -0.2581, -0.0345],\n",
       "                      [ 0.1434, -0.1132,  0.1077,  ..., -0.1946,  0.0080, -0.4224],\n",
       "                      ...,\n",
       "                      [-0.0646,  0.4250, -0.0431,  ..., -0.1595, -0.1165,  0.3054],\n",
       "                      [ 0.2461, -0.0331,  0.0623,  ...,  0.1078,  0.2034, -0.1764],\n",
       "                      [ 0.3833, -0.1958, -0.3770,  ...,  0.0027, -0.2030, -0.2375]])),\n",
       "             ('model.decoder.layers.0.fc1.bias',\n",
       "              tensor([-0.2781, -0.0742, -0.0874,  ..., -0.0566, -0.0241, -0.2001])),\n",
       "             ('model.decoder.layers.0.fc2.weight',\n",
       "              tensor([[-0.1371, -0.1132,  0.0466,  ...,  0.2681, -0.0298, -0.1131],\n",
       "                      [ 0.2164, -0.0201, -0.0924,  ...,  0.1089, -0.1300, -0.1272],\n",
       "                      [ 0.0881,  0.0721, -0.0802,  ..., -0.0063,  0.0746,  0.1890],\n",
       "                      ...,\n",
       "                      [ 0.1101,  0.0833, -0.1298,  ..., -0.0008, -0.0935, -0.0485],\n",
       "                      [-0.3896,  0.1687, -0.1404,  ...,  0.0498,  0.0757,  0.1858],\n",
       "                      [-0.2925, -0.0252,  0.2664,  ...,  0.1076,  0.4316,  0.0273]])),\n",
       "             ('model.decoder.layers.0.fc2.bias',\n",
       "              tensor([ 9.9304e-02,  5.1483e-02,  2.4277e-02,  ..., -4.9146e-01,\n",
       "                      -1.7102e-01, -5.7220e-06])),\n",
       "             ('model.decoder.layers.0.final_layer_norm.weight',\n",
       "              tensor([0.2993, 0.6899, 0.8115,  ..., 0.2859, 0.1858, 0.2041])),\n",
       "             ('model.decoder.layers.0.final_layer_norm.bias',\n",
       "              tensor([-0.0114, -0.0121, -0.0416,  ...,  0.0494,  0.0188,  0.0542])),\n",
       "             ('model.decoder.layers.1.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.1534, -0.1125, -0.1119,  ..., -0.1871,  0.2374,  0.0232],\n",
       "                      [ 0.3218, -0.1428,  0.0255,  ..., -0.1315, -0.0137,  0.1196],\n",
       "                      [-0.1437, -0.1470, -0.0161,  ..., -0.2886,  0.1324, -0.1108],\n",
       "                      ...,\n",
       "                      [-0.1249,  0.1697,  0.4082,  ..., -0.3542, -0.2754, -0.1086],\n",
       "                      [ 0.4036, -0.0794,  0.0101,  ..., -0.0826,  0.5649, -0.0558],\n",
       "                      [-0.0320,  0.0214, -0.0146,  ...,  0.2869, -0.0469, -0.2600]])),\n",
       "             ('model.decoder.layers.1.self_attn.k_proj.bias',\n",
       "              tensor([-0.0115,  0.0259, -0.0176,  ...,  0.0260,  0.0236,  0.0175])),\n",
       "             ('model.decoder.layers.1.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.1611,  0.2081,  0.1771,  ..., -0.2059, -0.0025,  0.0425],\n",
       "                      [ 0.0439, -0.1763, -0.0055,  ..., -0.1410, -0.0632, -0.1220],\n",
       "                      [ 0.1693,  0.2229,  0.4146,  ...,  0.0634,  0.0104,  0.1014],\n",
       "                      ...,\n",
       "                      [ 0.0663, -0.0892, -0.1144,  ..., -0.2166, -0.0717,  0.0168],\n",
       "                      [ 0.0919,  0.2917, -0.1696,  ..., -0.0205, -0.2695,  0.1384],\n",
       "                      [-0.1870,  0.1490, -0.2499,  ..., -0.0961, -0.0006,  0.1333]])),\n",
       "             ('model.decoder.layers.1.self_attn.v_proj.bias',\n",
       "              tensor([ 0.0769, -0.0881, -0.1488,  ..., -0.0063, -0.0072,  0.0170])),\n",
       "             ('model.decoder.layers.1.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0667,  0.2040,  0.0103,  ..., -0.0268, -0.1711,  0.0939],\n",
       "                      [-0.2396,  0.3296, -0.2190,  ...,  0.1805,  0.2666, -0.0803],\n",
       "                      [ 0.0565,  0.0851, -0.0085,  ...,  0.4153,  0.1097, -0.0129],\n",
       "                      ...,\n",
       "                      [-0.0325, -0.0953,  0.1277,  ..., -0.4856, -0.5068,  0.2008],\n",
       "                      [ 0.0311, -0.1217, -0.0499,  ...,  0.2189, -0.4949,  0.3477],\n",
       "                      [-0.2493,  0.1967,  0.4160,  ..., -0.4136, -0.1923,  0.0126]])),\n",
       "             ('model.decoder.layers.1.self_attn.q_proj.bias',\n",
       "              tensor([ 0.0458, -0.0001,  0.0027,  ..., -0.0262, -0.0651,  0.0208])),\n",
       "             ('model.decoder.layers.1.self_attn.out_proj.weight',\n",
       "              tensor([[-0.1395,  0.1007, -0.2021,  ..., -0.2854,  0.2642,  0.0026],\n",
       "                      [-0.2791,  0.3120, -0.3350,  ...,  0.0471, -0.1516,  0.1459],\n",
       "                      [ 0.2573, -0.1997,  0.0403,  ...,  0.0028,  0.0232, -0.1825],\n",
       "                      ...,\n",
       "                      [ 0.4160,  0.3020, -0.0767,  ...,  0.2146, -0.3484,  0.1781],\n",
       "                      [-0.0596, -0.2515,  0.1478,  ...,  0.0984, -0.0452,  0.0968],\n",
       "                      [-0.3391,  0.4890, -0.1302,  ...,  0.3733, -0.3181, -0.0583]])),\n",
       "             ('model.decoder.layers.1.self_attn.out_proj.bias',\n",
       "              tensor([-0.1322, -0.5068, -0.3179,  ..., -0.4104,  0.5020,  0.0186])),\n",
       "             ('model.decoder.layers.1.self_attn_layer_norm.weight',\n",
       "              tensor([0.2335, 0.3899, 0.4363,  ..., 0.1646, 0.1715, 0.1766])),\n",
       "             ('model.decoder.layers.1.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0122,  0.0031,  0.0142,  ..., -0.0172,  0.0160,  0.0054])),\n",
       "             ('model.decoder.layers.1.encoder_attn.k_proj.weight',\n",
       "              tensor([[ 3.3398e-01,  3.1158e-02,  4.3152e-02,  ..., -1.9287e-01,\n",
       "                       -1.6858e-01,  4.2175e-02],\n",
       "                      [ 3.4515e-02, -1.7725e-01, -7.6660e-02,  ..., -5.5206e-02,\n",
       "                       -6.0150e-02, -2.6245e-01],\n",
       "                      [-1.0291e-01, -2.0947e-01,  4.8615e-02,  ..., -1.3843e-01,\n",
       "                        1.7810e-01, -4.2267e-02],\n",
       "                      ...,\n",
       "                      [ 3.3081e-02,  8.9050e-02, -9.6863e-02,  ..., -4.3579e-02,\n",
       "                       -4.2603e-02,  7.5928e-02],\n",
       "                      [ 1.0266e-01, -3.2788e-01, -1.7285e-01,  ..., -2.2192e-01,\n",
       "                        2.8801e-04,  2.7344e-01],\n",
       "                      [ 3.6304e-01, -2.3499e-01, -4.5776e-01,  ...,  1.2268e-01,\n",
       "                        1.1481e-01, -2.8976e-02]])),\n",
       "             ('model.decoder.layers.1.encoder_attn.k_proj.bias',\n",
       "              tensor([-0.0274, -0.0099, -0.0295,  ...,  0.0261, -0.0028, -0.0228])),\n",
       "             ('model.decoder.layers.1.encoder_attn.v_proj.weight',\n",
       "              tensor([[-0.0637, -0.1584,  0.0154,  ..., -0.1114, -0.0318, -0.0013],\n",
       "                      [ 0.2046,  0.1764, -0.1729,  ..., -0.3098,  0.1407, -0.0028],\n",
       "                      [ 0.0202,  0.1407,  0.1124,  ...,  0.0605, -0.0785, -0.0263],\n",
       "                      ...,\n",
       "                      [ 0.1542,  0.1243,  0.1514,  ...,  0.1077, -0.0696,  0.0235],\n",
       "                      [-0.1639, -0.0312,  0.2416,  ...,  0.0569,  0.0289,  0.0510],\n",
       "                      [-0.0209,  0.0379,  0.0607,  ...,  0.0742,  0.1700, -0.0098]])),\n",
       "             ('model.decoder.layers.1.encoder_attn.v_proj.bias',\n",
       "              tensor([ 0.0453, -0.0273, -0.0125,  ..., -0.0927, -0.0215,  0.1722])),\n",
       "             ('model.decoder.layers.1.encoder_attn.q_proj.weight',\n",
       "              tensor([[-0.0729, -0.0311, -0.1705,  ...,  0.1267,  0.0778, -0.0111],\n",
       "                      [ 0.0575,  0.2003,  0.1246,  ...,  0.1231, -0.0115,  0.0741],\n",
       "                      [ 0.1191,  0.1030, -0.2544,  ...,  0.0974,  0.0105, -0.0771],\n",
       "                      ...,\n",
       "                      [-0.5625, -0.1671,  0.2000,  ..., -0.2129,  0.0068,  0.2164],\n",
       "                      [ 0.0772, -0.0503,  0.0390,  ..., -0.3298, -0.1742,  0.3379],\n",
       "                      [ 0.0915,  0.0277, -0.4065,  ..., -0.1199,  0.4858, -0.0520]])),\n",
       "             ('model.decoder.layers.1.encoder_attn.q_proj.bias',\n",
       "              tensor([-0.2184, -0.5171, -0.0284,  ..., -0.0183,  0.0548,  0.1344])),\n",
       "             ('model.decoder.layers.1.encoder_attn.out_proj.weight',\n",
       "              tensor([[-0.0327,  0.3237, -0.1772,  ...,  0.2573, -0.1348,  0.1152],\n",
       "                      [ 0.0838,  0.0265, -0.2900,  ...,  0.2081,  0.1089,  0.0315],\n",
       "                      [ 0.1299,  0.1479,  0.1301,  ..., -0.1229, -0.1685,  0.0691],\n",
       "                      ...,\n",
       "                      [-0.1422,  0.1223,  0.1954,  ..., -0.1614, -0.1797,  0.1772],\n",
       "                      [ 0.0510,  0.0167, -0.0936,  ..., -0.1155, -0.2397,  0.1880],\n",
       "                      [-0.3721, -0.2397,  0.0360,  ...,  0.0610, -0.0598,  0.0428]])),\n",
       "             ('model.decoder.layers.1.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.0711,  0.3042, -0.3445,  ..., -0.2491,  0.4126,  0.2225])),\n",
       "             ('model.decoder.layers.1.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1310, 0.2273, 0.2188,  ..., 0.1075, 0.1068, 0.0997])),\n",
       "             ('model.decoder.layers.1.encoder_attn_layer_norm.bias',\n",
       "              tensor([-0.0150, -0.0313, -0.0118,  ...,  0.0090,  0.0072, -0.0125])),\n",
       "             ('model.decoder.layers.1.fc1.weight',\n",
       "              tensor([[ 0.0250,  0.2991, -0.1091,  ..., -0.1277, -0.1576,  0.0674],\n",
       "                      [ 0.2976, -0.1415,  0.4507,  ..., -0.1866,  0.1283,  0.0190],\n",
       "                      [ 0.3267, -0.0324,  0.1790,  ..., -0.1440, -0.3320, -0.0600],\n",
       "                      ...,\n",
       "                      [ 0.1567,  0.0956,  0.1346,  ..., -0.3232, -0.1976, -0.3655],\n",
       "                      [ 0.4209,  0.2257,  0.0028,  ..., -0.1316, -0.3711,  0.0569],\n",
       "                      [ 0.1774, -0.2188,  0.1565,  ..., -0.2905,  0.0203, -0.2708]])),\n",
       "             ('model.decoder.layers.1.fc1.bias',\n",
       "              tensor([-0.1600, -0.0608, -0.1709,  ..., -0.2771, -0.2356, -0.0980])),\n",
       "             ('model.decoder.layers.1.fc2.weight',\n",
       "              tensor([[-0.2366, -0.0439, -0.2294,  ...,  0.1348,  0.0668, -0.1799],\n",
       "                      [ 0.1781,  0.1968,  0.0236,  ..., -0.1300, -0.2268,  0.2125],\n",
       "                      [-0.0632,  0.1316,  0.0245,  ...,  0.0402,  0.0947, -0.1364],\n",
       "                      ...,\n",
       "                      [ 0.0862,  0.2155, -0.0999,  ..., -0.3252, -0.0651,  0.0750],\n",
       "                      [ 0.1539,  0.5024,  0.3135,  ..., -0.2551,  0.0806, -0.0781],\n",
       "                      [ 0.0167,  0.2075,  0.0334,  ..., -0.1337,  0.0619,  0.1277]])),\n",
       "             ('model.decoder.layers.1.fc2.bias',\n",
       "              tensor([ 0.0667,  0.1331,  0.1181,  ..., -0.2947, -0.1676, -0.1841])),\n",
       "             ('model.decoder.layers.1.final_layer_norm.weight',\n",
       "              tensor([0.3525, 0.4556, 0.4563,  ..., 0.3020, 0.2478, 0.2815])),\n",
       "             ('model.decoder.layers.1.final_layer_norm.bias',\n",
       "              tensor([-0.0249, -0.0007, -0.0149,  ...,  0.0699,  0.0163,  0.0864])),\n",
       "             ('model.decoder.layers.2.self_attn.k_proj.weight',\n",
       "              tensor([[-0.3308,  0.3665,  0.0386,  ...,  0.1205, -0.4648,  0.2186],\n",
       "                      [ 0.0590,  0.1039,  0.0496,  ...,  0.1836, -0.3901,  0.3811],\n",
       "                      [-0.4600, -0.1686,  0.0210,  ...,  0.0736,  0.1978,  0.0778],\n",
       "                      ...,\n",
       "                      [-0.5742, -0.1058, -0.0801,  ...,  0.3740, -0.0912, -0.0032],\n",
       "                      [ 0.2705, -0.2336,  0.1692,  ..., -0.0341, -0.2332,  0.3018],\n",
       "                      [ 0.0272, -0.0501,  0.1230,  ..., -0.1545, -0.0927, -0.0757]])),\n",
       "             ('model.decoder.layers.2.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0152,  0.0089,  0.0074,  ...,  0.0129, -0.0011,  0.0167])),\n",
       "             ('model.decoder.layers.2.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0995, -0.1671, -0.5259,  ..., -0.2869, -0.1268,  0.0750],\n",
       "                      [ 0.0398, -0.0916, -0.0912,  ..., -0.1322, -0.2208,  0.1377],\n",
       "                      [ 0.3433,  0.0499,  0.0323,  ...,  0.1874, -0.1367,  0.1506],\n",
       "                      ...,\n",
       "                      [ 0.0250,  0.0933,  0.0583,  ...,  0.0499, -0.1655, -0.2617],\n",
       "                      [ 0.1227, -0.1143,  0.0930,  ...,  0.0697,  0.1770,  0.0898],\n",
       "                      [ 0.0276,  0.1170, -0.1428,  ..., -0.0865,  0.0061, -0.1729]])),\n",
       "             ('model.decoder.layers.2.self_attn.v_proj.bias',\n",
       "              tensor([-0.0004,  0.0150,  0.0520,  ...,  0.0121,  0.0303, -0.0188])),\n",
       "             ('model.decoder.layers.2.self_attn.q_proj.weight',\n",
       "              tensor([[-0.3401,  0.2352,  0.4128,  ...,  0.2554, -0.1636, -0.0297],\n",
       "                      [ 0.1205, -0.1771, -0.1298,  ..., -0.0942,  0.0591, -0.1454],\n",
       "                      [-0.0718, -0.0007,  0.0856,  ..., -0.1609,  0.2598, -0.0106],\n",
       "                      ...,\n",
       "                      [-0.0469, -0.1349,  0.0050,  ...,  0.1782,  0.1202, -0.1854],\n",
       "                      [ 0.0954,  0.0739, -0.3987,  ...,  0.0170,  0.0279,  0.0166],\n",
       "                      [-0.0461,  0.4138, -0.0368,  ..., -0.1155, -0.2113, -0.0650]])),\n",
       "             ('model.decoder.layers.2.self_attn.q_proj.bias',\n",
       "              tensor([-0.0418, -0.0768,  0.0216,  ..., -0.0235,  0.2318, -0.0897])),\n",
       "             ('model.decoder.layers.2.self_attn.out_proj.weight',\n",
       "              tensor([[-1.5540e-01, -1.6052e-01,  9.6558e-02,  ...,  9.5215e-02,\n",
       "                       -5.2490e-01, -1.0329e-04],\n",
       "                      [-1.7261e-01,  1.2408e-01,  8.1604e-02,  ..., -2.1411e-01,\n",
       "                       -1.5759e-01,  1.6846e-01],\n",
       "                      [-2.4548e-01, -2.9126e-01, -7.3509e-03,  ..., -1.0956e-01,\n",
       "                       -1.0693e-01, -7.5806e-02],\n",
       "                      ...,\n",
       "                      [ 2.4243e-01, -2.2388e-01,  3.2446e-01,  ..., -3.1104e-01,\n",
       "                        1.0699e-01, -3.1201e-01],\n",
       "                      [-4.9121e-01,  1.8164e-01,  1.7285e-01,  ..., -5.4102e-01,\n",
       "                        1.8896e-01, -1.7456e-01],\n",
       "                      [-1.4868e-01, -4.2686e-03, -1.5247e-01,  ..., -2.5244e-01,\n",
       "                        2.0667e-01, -1.2164e-01]])),\n",
       "             ('model.decoder.layers.2.self_attn.out_proj.bias',\n",
       "              tensor([-0.4189,  0.2380, -0.4163,  ..., -0.5039, -0.1493,  0.4475])),\n",
       "             ('model.decoder.layers.2.self_attn_layer_norm.weight',\n",
       "              tensor([0.2588, 0.3323, 0.3691,  ..., 0.1847, 0.1929, 0.1981])),\n",
       "             ('model.decoder.layers.2.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0080,  0.0052,  0.0182,  ..., -0.0058,  0.0125, -0.0036])),\n",
       "             ('model.decoder.layers.2.encoder_attn.k_proj.weight',\n",
       "              tensor([[ 3.9368e-03,  1.2466e-02, -1.2500e-01,  ..., -4.4922e-01,\n",
       "                        1.3086e-01, -1.2421e-01],\n",
       "                      [-4.1553e-01, -2.4854e-01,  6.1249e-02,  ..., -1.4575e-01,\n",
       "                        1.7358e-01, -3.6469e-02],\n",
       "                      [ 2.8638e-01,  1.9629e-01, -1.6016e-01,  ...,  2.6993e-02,\n",
       "                        1.7017e-01, -1.0522e-01],\n",
       "                      ...,\n",
       "                      [ 4.3457e-01, -1.6064e-01,  2.1777e-01,  ...,  7.3486e-02,\n",
       "                       -2.4500e-01,  2.2827e-02],\n",
       "                      [ 8.1360e-02,  3.6682e-02, -1.1883e-03,  ..., -8.8074e-02,\n",
       "                       -5.0568e-02, -5.5603e-02],\n",
       "                      [ 3.4475e-04,  4.4116e-01, -6.1188e-02,  ...,  6.7322e-02,\n",
       "                        1.3672e-01, -5.3192e-02]])),\n",
       "             ('model.decoder.layers.2.encoder_attn.k_proj.bias',\n",
       "              tensor([ 0.0274,  0.0169, -0.0277,  ..., -0.0282, -0.0058, -0.0312])),\n",
       "             ('model.decoder.layers.2.encoder_attn.v_proj.weight',\n",
       "              tensor([[ 0.0640, -0.0147, -0.0991,  ..., -0.4907, -0.1158,  0.0133],\n",
       "                      [-0.0900, -0.2211, -0.1050,  ..., -0.0369,  0.0284, -0.0867],\n",
       "                      [ 0.2096, -0.0718, -0.1810,  ..., -0.0245,  0.1345,  0.0412],\n",
       "                      ...,\n",
       "                      [-0.1543, -0.3372, -0.2124,  ...,  0.0370, -0.2200, -0.0192],\n",
       "                      [-0.0036,  0.1351, -0.0414,  ..., -0.2722, -0.1978,  0.0154],\n",
       "                      [-0.0208,  0.1348, -0.2732,  ...,  0.0633,  0.1826,  0.0693]])),\n",
       "             ('model.decoder.layers.2.encoder_attn.v_proj.bias',\n",
       "              tensor([-0.0737,  0.0847,  0.0364,  ...,  0.0214,  0.0005, -0.0131])),\n",
       "             ('model.decoder.layers.2.encoder_attn.q_proj.weight',\n",
       "              tensor([[-0.4275,  0.0265,  0.0817,  ..., -0.0469,  0.2341,  0.2118],\n",
       "                      [ 0.3743,  0.1727,  0.0540,  ..., -0.3494,  0.1705, -0.0107],\n",
       "                      [-0.1183, -0.0934,  0.1401,  ..., -0.1702,  0.0417, -0.0448],\n",
       "                      ...,\n",
       "                      [ 0.1122,  0.2097,  0.0602,  ...,  0.1998, -0.2957,  0.0191],\n",
       "                      [ 0.2045, -0.2030, -0.1022,  ..., -0.0297, -0.2910,  0.0159],\n",
       "                      [-0.4851,  0.4463,  0.0319,  ...,  0.0139, -0.0826, -0.0349]])),\n",
       "             ('model.decoder.layers.2.encoder_attn.q_proj.bias',\n",
       "              tensor([-0.0117,  0.0652, -0.1204,  ...,  0.0750,  0.0415, -0.0578])),\n",
       "             ('model.decoder.layers.2.encoder_attn.out_proj.weight',\n",
       "              tensor([[-5.6915e-02,  1.0187e-01, -2.7832e-01,  ...,  2.2229e-01,\n",
       "                       -3.0542e-01, -7.3204e-03],\n",
       "                      [ 7.6111e-02,  1.6968e-01, -1.3794e-01,  ..., -8.0750e-02,\n",
       "                       -4.1077e-02,  4.9683e-02],\n",
       "                      [-4.5776e-02, -2.0325e-01, -1.5405e-01,  ..., -2.6611e-01,\n",
       "                        2.3071e-01, -2.0093e-01],\n",
       "                      ...,\n",
       "                      [ 3.4741e-01,  2.8296e-01,  3.5187e-02,  ...,  3.4546e-02,\n",
       "                        7.2754e-02,  1.6708e-02],\n",
       "                      [-2.9907e-01,  3.6499e-02, -2.6807e-01,  ..., -3.5181e-01,\n",
       "                       -2.1741e-01, -3.6224e-02],\n",
       "                      [-2.3279e-01,  3.2013e-02,  2.1243e-04,  ..., -2.9346e-01,\n",
       "                        4.3140e-01,  1.1359e-01]])),\n",
       "             ('model.decoder.layers.2.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.1670,  0.2517,  0.0768,  ..., -0.3137, -0.0340, -0.1135])),\n",
       "             ('model.decoder.layers.2.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1266, 0.1838, 0.1929,  ..., 0.1061, 0.0986, 0.1010])),\n",
       "             ('model.decoder.layers.2.encoder_attn_layer_norm.bias',\n",
       "              tensor([-0.0076, -0.0356, -0.0011,  ..., -0.0028, -0.0035, -0.0181])),\n",
       "             ('model.decoder.layers.2.fc1.weight',\n",
       "              tensor([[-0.1963, -0.0854,  0.0271,  ..., -0.0640, -0.1954,  0.2534],\n",
       "                      [-0.3438,  0.1814, -0.1447,  ..., -0.3826, -0.0184, -0.3777],\n",
       "                      [ 0.0417,  0.0775,  0.2869,  ...,  0.1899,  0.0681, -0.3806],\n",
       "                      ...,\n",
       "                      [-0.2186, -0.0697, -0.2954,  ..., -0.0013, -0.0532, -0.4646],\n",
       "                      [-0.1605,  0.4458,  0.0753,  ...,  0.0030, -0.1979,  0.0666],\n",
       "                      [ 0.1459,  0.1836,  0.0529,  ...,  0.0804, -0.0881,  0.0609]])),\n",
       "             ('model.decoder.layers.2.fc1.bias',\n",
       "              tensor([-0.1088, -0.2505, -0.1223,  ..., -0.2303, -0.1230, -0.0989])),\n",
       "             ('model.decoder.layers.2.fc2.weight',\n",
       "              tensor([[-0.1610, -0.3735, -0.0555,  ...,  0.2761, -0.0305, -0.1532],\n",
       "                      [-0.0477, -0.2759,  0.1680,  ...,  0.0407, -0.1489, -0.1984],\n",
       "                      [-0.0375, -0.1848, -0.0933,  ...,  0.1871,  0.2727, -0.1232],\n",
       "                      ...,\n",
       "                      [-0.0792, -0.1058, -0.3218,  ...,  0.0746,  0.1451,  0.1779],\n",
       "                      [-0.0492, -0.1247,  0.2181,  ...,  0.3079, -0.0853,  0.2299],\n",
       "                      [ 0.4763,  0.1882, -0.0437,  ..., -0.0560, -0.4338, -0.0142]])),\n",
       "             ('model.decoder.layers.2.fc2.bias',\n",
       "              tensor([-0.1108, -0.2070, -0.0164,  ..., -0.2815, -0.0843, -0.2876])),\n",
       "             ('model.decoder.layers.2.final_layer_norm.weight',\n",
       "              tensor([0.3970, 0.4524, 0.4319,  ..., 0.3435, 0.3228, 0.3274])),\n",
       "             ('model.decoder.layers.2.final_layer_norm.bias',\n",
       "              tensor([-0.0156,  0.0081,  0.0066,  ...,  0.0600, -0.0039,  0.0792])),\n",
       "             ('model.decoder.layers.3.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0989,  0.3494,  0.1242,  ..., -0.0166, -0.2554, -0.0117],\n",
       "                      [-0.0774, -0.2800,  0.0182,  ..., -0.2566, -0.0605, -0.3606],\n",
       "                      [ 0.1135, -0.0656,  0.0745,  ...,  0.1853, -0.1089, -0.1870],\n",
       "                      ...,\n",
       "                      [-0.1294, -0.1048, -0.0658,  ...,  0.0122, -0.0033, -0.1639],\n",
       "                      [-0.0522,  0.1150, -0.0699,  ..., -0.0789,  0.0958,  0.0314],\n",
       "                      [-0.1689,  0.0449,  0.0771,  ..., -0.0376, -0.2283, -0.1467]])),\n",
       "             ('model.decoder.layers.3.self_attn.k_proj.bias',\n",
       "              tensor([-0.0116, -0.0057, -0.0070,  ...,  0.0291, -0.0091, -0.0291])),\n",
       "             ('model.decoder.layers.3.self_attn.v_proj.weight',\n",
       "              tensor([[-0.1167, -0.2267, -0.1385,  ..., -0.1125,  0.1224, -0.4749],\n",
       "                      [-0.1150, -0.0827, -0.2052,  ...,  0.1290, -0.1851,  0.3667],\n",
       "                      [ 0.0052,  0.1235,  0.1295,  ..., -0.2764,  0.4204,  0.4370],\n",
       "                      ...,\n",
       "                      [ 0.3010, -0.2048, -0.3379,  ..., -0.2324,  0.0846,  0.2581],\n",
       "                      [-0.4292, -0.1766,  0.1385,  ...,  0.1300,  0.1146,  0.2145],\n",
       "                      [ 0.2388, -0.5005, -0.0915,  ..., -0.0520, -0.3730,  0.1678]])),\n",
       "             ('model.decoder.layers.3.self_attn.v_proj.bias',\n",
       "              tensor([-0.0255,  0.0488,  0.0688,  ...,  0.0421, -0.0326,  0.0906])),\n",
       "             ('model.decoder.layers.3.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.2737, -0.1266,  0.1499,  ...,  0.0596,  0.1345,  0.2371],\n",
       "                      [-0.0415,  0.2070,  0.0330,  ..., -0.1792,  0.1012,  0.2859],\n",
       "                      [-0.0612,  0.1016,  0.1646,  ..., -0.2700, -0.0338, -0.0159],\n",
       "                      ...,\n",
       "                      [ 0.1676, -0.0497,  0.4077,  ..., -0.0945,  0.0942, -0.0088],\n",
       "                      [-0.1548, -0.1628, -0.3193,  ...,  0.2554, -0.0501,  0.0714],\n",
       "                      [ 0.2166,  0.1407, -0.0704,  ...,  0.0569,  0.3152, -0.0768]])),\n",
       "             ('model.decoder.layers.3.self_attn.q_proj.bias',\n",
       "              tensor([ 0.0457,  0.1059,  0.3831,  ...,  0.3333,  0.0374, -0.0630])),\n",
       "             ('model.decoder.layers.3.self_attn.out_proj.weight',\n",
       "              tensor([[-0.1353, -0.2498,  0.0072,  ...,  0.0026,  0.3059,  0.0142],\n",
       "                      [ 0.0606, -0.2023,  0.1234,  ...,  0.5112, -0.1735,  0.4958],\n",
       "                      [ 0.0301,  0.1292,  0.4243,  ..., -0.1786, -0.1874,  0.2764],\n",
       "                      ...,\n",
       "                      [ 0.1820,  0.1312,  0.3752,  ..., -0.4602,  0.6631, -0.1891],\n",
       "                      [ 0.1621, -0.3711,  0.2749,  ...,  0.4641,  0.3682, -0.2791],\n",
       "                      [-0.3870,  0.2444,  0.1794,  ..., -0.2311,  0.4253, -0.3052]])),\n",
       "             ('model.decoder.layers.3.self_attn.out_proj.bias',\n",
       "              tensor([-0.3191, -0.0130, -0.3569,  ..., -0.4990, -0.2812,  0.3171])),\n",
       "             ('model.decoder.layers.3.self_attn_layer_norm.weight',\n",
       "              tensor([0.2649, 0.2991, 0.3367,  ..., 0.1940, 0.2025, 0.2090])),\n",
       "             ('model.decoder.layers.3.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0098,  0.0048,  0.0200,  ..., -0.0096,  0.0063, -0.0139])),\n",
       "             ('model.decoder.layers.3.encoder_attn.k_proj.weight',\n",
       "              tensor([[-0.1865, -0.0974,  0.1832,  ..., -0.2744, -0.0507, -0.1809],\n",
       "                      [ 0.3071, -0.1432,  0.2676,  ...,  0.0955, -0.1697,  0.0543],\n",
       "                      [ 0.0473,  0.2034,  0.2032,  ..., -0.0693, -0.2028,  0.1306],\n",
       "                      ...,\n",
       "                      [-0.2595,  0.1549, -0.1284,  ..., -0.0772, -0.1320,  0.1205],\n",
       "                      [-0.1119,  0.0875,  0.0643,  ...,  0.0091, -0.0217, -0.0189],\n",
       "                      [ 0.0364,  0.2268,  0.0769,  ..., -0.1428,  0.0320,  0.0672]])),\n",
       "             ('model.decoder.layers.3.encoder_attn.k_proj.bias',\n",
       "              tensor([-0.0310,  0.0280,  0.0024,  ...,  0.0099, -0.0284, -0.0260])),\n",
       "             ('model.decoder.layers.3.encoder_attn.v_proj.weight',\n",
       "              tensor([[ 2.4078e-02,  2.6733e-01,  2.0782e-02,  ...,  1.9263e-01,\n",
       "                        1.7105e-02,  2.2621e-03],\n",
       "                      [ 1.1975e-01,  6.5063e-02, -1.6510e-02,  ..., -3.9642e-02,\n",
       "                        1.5393e-01,  4.3671e-02],\n",
       "                      [ 2.0248e-02,  1.1452e-02,  8.4961e-02,  ..., -2.9495e-02,\n",
       "                       -1.4844e-01,  1.0025e-02],\n",
       "                      ...,\n",
       "                      [ 2.6025e-01,  3.3112e-02,  6.1096e-02,  ...,  1.0437e-01,\n",
       "                        3.6987e-01, -4.9896e-03],\n",
       "                      [-7.1680e-01, -2.0828e-02, -1.8994e-01,  ...,  4.0741e-02,\n",
       "                        2.2595e-01,  1.6713e-04],\n",
       "                      [ 2.0004e-02,  2.5366e-01,  2.1960e-01,  ..., -1.0065e-01,\n",
       "                        6.7871e-02,  1.7929e-02]])),\n",
       "             ('model.decoder.layers.3.encoder_attn.v_proj.bias',\n",
       "              tensor([-0.0029,  0.0220, -0.1296,  ...,  0.0403,  0.0458,  0.0118])),\n",
       "             ('model.decoder.layers.3.encoder_attn.q_proj.weight',\n",
       "              tensor([[-0.0173,  0.0173,  0.1548,  ...,  0.4448,  0.2622, -0.3184],\n",
       "                      [ 0.0507,  0.5000,  0.1766,  ...,  0.2209, -0.3264, -0.1075],\n",
       "                      [ 0.0124, -0.0648,  0.1494,  ..., -0.2532,  0.1809, -0.1466],\n",
       "                      ...,\n",
       "                      [ 0.2498, -0.0932, -0.2915,  ..., -0.2399, -0.0685,  0.0255],\n",
       "                      [ 0.0212,  0.2756,  0.0865,  ...,  0.0206, -0.1403,  0.0765],\n",
       "                      [ 0.0407,  0.2849, -0.2264,  ..., -0.1996,  0.0529,  0.0208]])),\n",
       "             ('model.decoder.layers.3.encoder_attn.q_proj.bias',\n",
       "              tensor([-0.1206,  0.1025,  0.0757,  ..., -0.0092, -0.0883, -0.1151])),\n",
       "             ('model.decoder.layers.3.encoder_attn.out_proj.weight',\n",
       "              tensor([[ 0.0876, -0.1027,  0.1193,  ...,  0.2145,  0.0585, -0.1265],\n",
       "                      [ 0.0244,  0.0413, -0.1685,  ...,  0.0888,  0.1541, -0.0012],\n",
       "                      [-0.1416, -0.0668, -0.3159,  ..., -0.0815,  0.0106, -0.0503],\n",
       "                      ...,\n",
       "                      [ 0.0349, -0.1460, -0.0229,  ...,  0.1868, -0.0360,  0.0837],\n",
       "                      [ 0.1722,  0.0825, -0.0028,  ...,  0.3721,  0.1023,  0.1249],\n",
       "                      [-0.0881,  0.0850, -0.1405,  ..., -0.2732,  0.2292, -0.1927]])),\n",
       "             ('model.decoder.layers.3.encoder_attn.out_proj.bias',\n",
       "              tensor([ 0.1139,  0.2313,  0.3328,  ...,  0.1598,  0.1838, -0.0592])),\n",
       "             ('model.decoder.layers.3.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1249, 0.1664, 0.1544,  ..., 0.0951, 0.1037, 0.1024])),\n",
       "             ('model.decoder.layers.3.encoder_attn_layer_norm.bias',\n",
       "              tensor([-0.0096, -0.0279, -0.0147,  ..., -0.0196,  0.0010, -0.0201])),\n",
       "             ('model.decoder.layers.3.fc1.weight',\n",
       "              tensor([[ 0.2854, -0.1564,  0.2632,  ...,  0.3538,  0.1858, -0.2240],\n",
       "                      [ 0.1320, -0.0382,  0.1373,  ...,  0.0072, -0.1225,  0.2688],\n",
       "                      [-0.0849, -0.1731,  0.4026,  ..., -0.0120,  0.2983, -0.0650],\n",
       "                      ...,\n",
       "                      [-0.2664, -0.1324,  0.2439,  ...,  0.1598,  0.0373,  0.1060],\n",
       "                      [-0.0652,  0.0218,  0.0721,  ..., -0.0628, -0.5562, -0.1991],\n",
       "                      [ 0.0721,  0.0939, -0.2037,  ..., -0.0305, -0.1075, -0.0715]])),\n",
       "             ('model.decoder.layers.3.fc1.bias',\n",
       "              tensor([-0.1403, -0.0063, -0.0494,  ...,  0.0057, -0.2385, -0.1196])),\n",
       "             ('model.decoder.layers.3.fc2.weight',\n",
       "              tensor([[-3.0322e-01, -6.6833e-02,  1.0114e-01,  ..., -5.2452e-04,\n",
       "                        4.5197e-02,  2.4658e-02],\n",
       "                      [ 7.6111e-02, -1.4819e-01,  2.0898e-01,  ..., -1.3293e-01,\n",
       "                        1.6663e-02,  5.0812e-02],\n",
       "                      [ 1.4941e-01, -1.0266e-01, -1.2354e-01,  ..., -6.5327e-04,\n",
       "                       -8.2458e-02, -1.0616e-04],\n",
       "                      ...,\n",
       "                      [ 8.2153e-02,  1.2646e-01, -1.3855e-01,  ...,  7.0679e-02,\n",
       "                       -1.0352e-01, -8.3435e-02],\n",
       "                      [-2.7199e-03,  3.4009e-01, -2.1057e-01,  ..., -4.7485e-01,\n",
       "                        7.7454e-02, -3.9380e-01],\n",
       "                      [ 2.1704e-01,  3.9139e-03,  8.0872e-02,  ..., -1.1017e-01,\n",
       "                       -1.6699e-01, -2.3010e-02]])),\n",
       "             ('model.decoder.layers.3.fc2.bias',\n",
       "              tensor([-0.1926, -0.1788,  0.1004,  ..., -0.1218,  0.0177, -0.1982])),\n",
       "             ('model.decoder.layers.3.final_layer_norm.weight',\n",
       "              tensor([0.4517, 0.4846, 0.4919,  ..., 0.3994, 0.3762, 0.3960])),\n",
       "             ('model.decoder.layers.3.final_layer_norm.bias',\n",
       "              tensor([0.0141, 0.0581, 0.0347,  ..., 0.0001, 0.0165, 0.0414])),\n",
       "             ('model.decoder.layers.4.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.2207,  0.1823, -0.0239,  ..., -0.0892, -0.2250, -0.0438],\n",
       "                      [-0.1675,  0.0461, -0.2908,  ..., -0.0573,  0.2861, -0.1185],\n",
       "                      [-0.2162,  0.2639,  0.0948,  ...,  0.0665,  0.1389,  0.2520],\n",
       "                      ...,\n",
       "                      [-0.0280, -0.0919,  0.0092,  ...,  0.2295, -0.2079,  0.0559],\n",
       "                      [ 0.1720,  0.1978, -0.1338,  ..., -0.3010,  0.5542,  0.1371],\n",
       "                      [ 0.1345,  0.1317, -0.1661,  ...,  0.0614, -0.0829, -0.2491]])),\n",
       "             ('model.decoder.layers.4.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0078,  0.0124, -0.0312,  ..., -0.0085, -0.0288, -0.0269])),\n",
       "             ('model.decoder.layers.4.self_attn.v_proj.weight',\n",
       "              tensor([[-0.2673,  0.0440,  0.2004,  ...,  0.0255, -0.0587,  0.0551],\n",
       "                      [ 0.0173, -0.0415, -0.1914,  ...,  0.2194, -0.2268,  0.0632],\n",
       "                      [-0.3674, -0.1038,  0.1384,  ...,  0.1515,  0.1569, -0.2007],\n",
       "                      ...,\n",
       "                      [ 0.0292,  0.3396, -0.3240,  ...,  0.2029,  0.3286,  0.3315],\n",
       "                      [ 0.1886, -0.2532,  0.0946,  ..., -0.0008,  0.2737,  0.4102],\n",
       "                      [ 0.4709, -0.1427, -0.1335,  ..., -0.3120, -0.1081,  0.0392]])),\n",
       "             ('model.decoder.layers.4.self_attn.v_proj.bias',\n",
       "              tensor([ 0.0049,  0.0786,  0.0156,  ..., -0.0225, -0.0117, -0.1213])),\n",
       "             ('model.decoder.layers.4.self_attn.q_proj.weight',\n",
       "              tensor([[-0.1050, -0.0892, -0.0248,  ..., -0.2117,  0.2351, -0.1338],\n",
       "                      [-0.0142, -0.0568, -0.2524,  ...,  0.1010, -0.2367,  0.1089],\n",
       "                      [ 0.0411, -0.0777,  0.0359,  ..., -0.0947,  0.0923,  0.1312],\n",
       "                      ...,\n",
       "                      [-0.0166,  0.0511, -0.0418,  ...,  0.2347,  0.0373,  0.0219],\n",
       "                      [-0.3071, -0.1300,  0.0977,  ..., -0.3276, -0.2328, -0.2103],\n",
       "                      [-0.0814, -0.2233,  0.1093,  ..., -0.1879, -0.1793, -0.1509]])),\n",
       "             ('model.decoder.layers.4.self_attn.q_proj.bias',\n",
       "              tensor([-0.1097, -0.2244,  0.1980,  ..., -0.0424,  0.0051, -0.0490])),\n",
       "             ('model.decoder.layers.4.self_attn.out_proj.weight',\n",
       "              tensor([[-0.2305, -0.0163,  0.1232,  ...,  0.3679,  0.2944, -0.0201],\n",
       "                      [ 0.2825, -0.3196,  0.0299,  ...,  0.3411,  0.2332, -0.2466],\n",
       "                      [ 0.1153, -0.1301, -0.1711,  ...,  0.0217, -0.2856, -0.2861],\n",
       "                      ...,\n",
       "                      [-0.2355,  0.1714,  0.0613,  ..., -0.0398, -0.2347,  0.0804],\n",
       "                      [ 0.1771, -0.1948,  0.1324,  ...,  0.2856, -0.0511, -0.0304],\n",
       "                      [ 0.0372,  0.3025, -0.4363,  ...,  0.2578, -0.0452, -0.1451]])),\n",
       "             ('model.decoder.layers.4.self_attn.out_proj.bias',\n",
       "              tensor([-0.2571, -0.0980, -0.3325,  ..., -0.5020, -0.4917,  0.4910])),\n",
       "             ('model.decoder.layers.4.self_attn_layer_norm.weight',\n",
       "              tensor([0.2859, 0.3113, 0.3257,  ..., 0.2101, 0.2292, 0.2230])),\n",
       "             ('model.decoder.layers.4.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0132,  0.0039,  0.0213,  ..., -0.0128,  0.0103, -0.0153])),\n",
       "             ('model.decoder.layers.4.encoder_attn.k_proj.weight',\n",
       "              tensor([[-0.3931,  0.0401,  0.1293,  ..., -0.1195, -0.0222,  0.0602],\n",
       "                      [ 0.2490, -0.2781, -0.0809,  ..., -0.0480,  0.1019, -0.0571],\n",
       "                      [-0.1403,  0.0654, -0.0033,  ...,  0.0448,  0.0354,  0.1317],\n",
       "                      ...,\n",
       "                      [-0.1971, -0.1921, -0.2170,  ...,  0.2629,  0.1705, -0.0448],\n",
       "                      [ 0.0312, -0.1581,  0.1361,  ..., -0.0624, -0.0419,  0.0023],\n",
       "                      [-0.0662,  0.0148, -0.1705,  ..., -0.0999, -0.0140, -0.0345]])),\n",
       "             ('model.decoder.layers.4.encoder_attn.k_proj.bias',\n",
       "              tensor([ 0.0095,  0.0270,  0.0019,  ...,  0.0180, -0.0116,  0.0144])),\n",
       "             ('model.decoder.layers.4.encoder_attn.v_proj.weight',\n",
       "              tensor([[ 0.0465,  0.1771, -0.3411,  ..., -0.0360, -0.1864,  0.0047],\n",
       "                      [ 0.0253, -0.0935,  0.0439,  ...,  0.2808,  0.1083, -0.0020],\n",
       "                      [ 0.0543, -0.3330, -0.0420,  ..., -0.2228,  0.1534,  0.0454],\n",
       "                      ...,\n",
       "                      [-0.1678, -0.2131, -0.0310,  ..., -0.1007,  0.1016, -0.0550],\n",
       "                      [-0.1109,  0.0102,  0.1556,  ...,  0.1377, -0.0242,  0.0895],\n",
       "                      [ 0.0603,  0.3479,  0.2795,  ...,  0.0861, -0.2308, -0.0095]])),\n",
       "             ('model.decoder.layers.4.encoder_attn.v_proj.bias',\n",
       "              tensor([-0.0964, -0.0339,  0.0177,  ..., -0.0941, -0.0497, -0.0724])),\n",
       "             ('model.decoder.layers.4.encoder_attn.q_proj.weight',\n",
       "              tensor([[-0.1450, -0.3315, -0.1952,  ...,  0.0352,  0.0111,  0.0752],\n",
       "                      [-0.0774, -0.0184, -0.0053,  ...,  0.1453,  0.0826,  0.3909],\n",
       "                      [-0.2399, -0.0374, -0.0986,  ..., -0.2700,  0.3972,  0.3582],\n",
       "                      ...,\n",
       "                      [-0.0827, -0.0493, -0.0366,  ...,  0.0909, -0.2639,  0.0032],\n",
       "                      [ 0.1049,  0.1342,  0.3726,  ..., -0.0033,  0.0166,  0.1332],\n",
       "                      [-0.2563, -0.3057, -0.1438,  ..., -0.1454, -0.0888, -0.0361]])),\n",
       "             ('model.decoder.layers.4.encoder_attn.q_proj.bias',\n",
       "              tensor([ 0.0404,  0.1339, -0.0085,  ...,  0.1522,  0.0819,  0.0841])),\n",
       "             ('model.decoder.layers.4.encoder_attn.out_proj.weight',\n",
       "              tensor([[ 0.0031, -0.0746,  0.2087,  ..., -0.1304, -0.2308, -0.2993],\n",
       "                      [-0.0189, -0.2008, -0.2449,  ..., -0.1711, -0.0311,  0.1272],\n",
       "                      [-0.2607,  0.1437, -0.4104,  ...,  0.0939, -0.0751,  0.2810],\n",
       "                      ...,\n",
       "                      [ 0.0663, -0.4783,  0.0988,  ..., -0.0980,  0.1279, -0.1688],\n",
       "                      [-0.1671, -0.1387,  0.1354,  ..., -0.2666, -0.3020, -0.0603],\n",
       "                      [ 0.0801, -0.0495,  0.4460,  ...,  0.1710, -0.0508, -0.0991]])),\n",
       "             ('model.decoder.layers.4.encoder_attn.out_proj.bias',\n",
       "              tensor([ 0.1298,  0.2185,  0.3086,  ...,  0.0321, -0.1702, -0.3853])),\n",
       "             ('model.decoder.layers.4.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1193, 0.1481, 0.1348,  ..., 0.0941, 0.1001, 0.0947])),\n",
       "             ('model.decoder.layers.4.encoder_attn_layer_norm.bias',\n",
       "              tensor([-0.0073, -0.0116, -0.0107,  ..., -0.0408, -0.0026, -0.0193])),\n",
       "             ('model.decoder.layers.4.fc1.weight',\n",
       "              tensor([[-0.4097, -0.2700,  0.0737,  ...,  0.0936,  0.0958,  0.2720],\n",
       "                      [ 0.0781, -0.1074,  0.0117,  ...,  0.2820,  0.2788, -0.1580],\n",
       "                      [-0.2754, -0.2556,  0.0128,  ...,  0.2820,  0.0628,  0.0275],\n",
       "                      ...,\n",
       "                      [ 0.1747,  0.2019,  0.3281,  ...,  0.3142,  0.0674,  0.1089],\n",
       "                      [-0.1899, -0.1687,  0.3840,  ..., -0.1477, -0.3643,  0.2465],\n",
       "                      [ 0.4243,  0.2018, -0.3101,  ...,  0.2864, -0.1592,  0.0157]])),\n",
       "             ('model.decoder.layers.4.fc1.bias',\n",
       "              tensor([-0.2307,  0.0841, -0.3652,  ..., -0.2961, -0.1600, -0.1461])),\n",
       "             ('model.decoder.layers.4.fc2.weight',\n",
       "              tensor([[-0.0831, -0.1288,  0.0689,  ..., -0.2583, -0.0450,  0.1154],\n",
       "                      [-0.3525, -0.0558, -0.0667,  ..., -0.2041,  0.1604, -0.0671],\n",
       "                      [-0.1544, -0.0462, -0.3406,  ...,  0.0112,  0.2053,  0.0386],\n",
       "                      ...,\n",
       "                      [ 0.2124, -0.1754, -0.0749,  ...,  0.2812,  0.3555,  0.1595],\n",
       "                      [-0.0262, -0.1567,  0.1155,  ...,  0.2015, -0.0365,  0.1342],\n",
       "                      [ 0.1613,  0.0448,  0.0573,  ...,  0.3638, -0.1046,  0.0436]])),\n",
       "             ('model.decoder.layers.4.fc2.bias',\n",
       "              tensor([-0.1475, -0.1466,  0.2227,  ..., -0.0585,  0.1493,  0.0556])),\n",
       "             ('model.decoder.layers.4.final_layer_norm.weight',\n",
       "              tensor([0.5186, 0.5400, 0.5186,  ..., 0.4551, 0.4565, 0.4644])),\n",
       "             ('model.decoder.layers.4.final_layer_norm.bias',\n",
       "              tensor([ 0.0312,  0.1107,  0.0245,  ..., -0.1113, -0.0250,  0.0216])),\n",
       "             ('model.decoder.layers.5.self_attn.k_proj.weight',\n",
       "              tensor([[-6.5857e-02,  2.0251e-01, -5.7465e-02,  ...,  4.4647e-02,\n",
       "                       -1.0632e-01,  7.2449e-02],\n",
       "                      [ 1.3733e-02, -1.0150e-01, -2.9434e-02,  ...,  1.1426e-01,\n",
       "                        1.4832e-01,  8.0338e-03],\n",
       "                      [ 1.1176e-01, -4.4739e-02, -8.9355e-02,  ..., -5.9605e-06,\n",
       "                       -1.0858e-01, -2.6672e-02],\n",
       "                      ...,\n",
       "                      [-2.2217e-01, -1.2128e-01, -5.0537e-02,  ...,  1.6589e-01,\n",
       "                       -1.8567e-01, -5.2338e-02],\n",
       "                      [ 5.4565e-02,  7.6027e-03, -1.1421e-02,  ..., -1.1682e-01,\n",
       "                        5.4169e-02,  2.6636e-01],\n",
       "                      [-1.3696e-01, -3.6450e-01,  2.5366e-01,  ..., -3.2410e-02,\n",
       "                       -1.6187e-01,  9.6558e-02]])),\n",
       "             ('model.decoder.layers.5.self_attn.k_proj.bias',\n",
       "              tensor([-0.0155,  0.0081, -0.0183,  ..., -0.0101, -0.0080,  0.0053])),\n",
       "             ('model.decoder.layers.5.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.3101, -0.0659, -0.4985,  ..., -0.0261,  0.1904, -0.0820],\n",
       "                      [ 0.1340, -0.1665, -0.0553,  ..., -0.0634,  0.3303, -0.4929],\n",
       "                      [-0.5005, -0.4053,  0.0614,  ...,  0.1809, -0.4783,  0.2988],\n",
       "                      ...,\n",
       "                      [-0.0119, -0.3416, -0.1237,  ...,  0.0163, -0.1481, -0.0235],\n",
       "                      [-0.2915,  0.0356, -0.1187,  ..., -0.1050, -0.2200,  0.3716],\n",
       "                      [-0.2551,  0.0391,  0.1863,  ..., -0.0909, -0.0381,  0.0756]])),\n",
       "             ('model.decoder.layers.5.self_attn.v_proj.bias',\n",
       "              tensor([-0.0251, -0.0740,  0.1063,  ..., -0.0436,  0.0135,  0.0956])),\n",
       "             ('model.decoder.layers.5.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0420, -0.3079, -0.0490,  ...,  0.1382,  0.0266,  0.1229],\n",
       "                      [-0.0640,  0.0458,  0.1144,  ...,  0.1064, -0.1210, -0.0217],\n",
       "                      [ 0.1219, -0.0167,  0.0353,  ..., -0.0523, -0.1027,  0.1458],\n",
       "                      ...,\n",
       "                      [ 0.3435, -0.0014,  0.2062,  ...,  0.0873, -0.1273, -0.0268],\n",
       "                      [-0.1941,  0.1359, -0.2434,  ...,  0.2717,  0.1084,  0.2108],\n",
       "                      [-0.1007,  0.1511, -0.1353,  ...,  0.1998, -0.2651, -0.3638]])),\n",
       "             ('model.decoder.layers.5.self_attn.q_proj.bias',\n",
       "              tensor([-0.0583, -0.0863,  0.0507,  ..., -0.0739,  0.1298,  0.1917])),\n",
       "             ('model.decoder.layers.5.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.2006, -0.1216,  0.1897,  ...,  0.0042,  0.0221, -0.0220],\n",
       "                      [-0.2788,  0.2749,  0.3147,  ..., -0.3538, -0.1953,  0.1487],\n",
       "                      [-0.1451,  0.3015, -0.4993,  ..., -0.1499,  0.0264, -0.0385],\n",
       "                      ...,\n",
       "                      [-0.3020,  0.2366, -0.0396,  ..., -0.0085, -0.3369, -0.0049],\n",
       "                      [ 0.2009, -0.1084, -0.2537,  ..., -0.3242, -0.2581,  0.1196],\n",
       "                      [ 0.4978, -0.2510,  0.3047,  ..., -0.0637,  0.0760, -0.1024]])),\n",
       "             ('model.decoder.layers.5.self_attn.out_proj.bias',\n",
       "              tensor([-0.2795,  0.4678, -0.2693,  ..., -0.4160, -0.0284,  0.2915])),\n",
       "             ('model.decoder.layers.5.self_attn_layer_norm.weight',\n",
       "              tensor([0.2969, 0.3320, 0.3101,  ..., 0.2297, 0.2537, 0.2499])),\n",
       "             ('model.decoder.layers.5.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0145,  0.0063,  0.0268,  ..., -0.0158,  0.0065, -0.0284])),\n",
       "             ('model.decoder.layers.5.encoder_attn.k_proj.weight',\n",
       "              tensor([[ 0.0375, -0.0067,  0.0670,  ..., -0.3262, -0.0666,  0.0624],\n",
       "                      [-0.0308,  0.0286, -0.1774,  ..., -0.1131, -0.0634,  0.1097],\n",
       "                      [-0.0727,  0.0724, -0.2861,  ...,  0.2018,  0.0775, -0.0070],\n",
       "                      ...,\n",
       "                      [-0.1735, -0.0476, -0.1813,  ..., -0.2632, -0.2131, -0.0457],\n",
       "                      [ 0.2214, -0.1711,  0.0067,  ..., -0.3740,  0.2871, -0.0119],\n",
       "                      [-0.2520, -0.1650,  0.0536,  ..., -0.2666, -0.4377,  0.1949]])),\n",
       "             ('model.decoder.layers.5.encoder_attn.k_proj.bias',\n",
       "              tensor([ 0.0144, -0.0133,  0.0258,  ...,  0.0226, -0.0057, -0.0240])),\n",
       "             ('model.decoder.layers.5.encoder_attn.v_proj.weight',\n",
       "              tensor([[ 0.1630, -0.0317,  0.1660,  ..., -0.4785, -0.2400, -0.0245],\n",
       "                      [ 0.0836, -0.0155,  0.2700,  ...,  0.0586, -0.1124,  0.0049],\n",
       "                      [-0.0585,  0.0667, -0.0399,  ..., -0.2175, -0.0422, -0.0768],\n",
       "                      ...,\n",
       "                      [ 0.2810,  0.2720, -0.0113,  ..., -0.2407, -0.2625,  0.0165],\n",
       "                      [ 0.1631,  0.1704,  0.0071,  ..., -0.2157, -0.0864,  0.0110],\n",
       "                      [ 0.0294,  0.1296,  0.1190,  ...,  0.1326, -0.0536,  0.0583]])),\n",
       "             ('model.decoder.layers.5.encoder_attn.v_proj.bias',\n",
       "              tensor([ 0.0158, -0.1880,  0.0825,  ..., -0.0160, -0.0419, -0.0264])),\n",
       "             ('model.decoder.layers.5.encoder_attn.q_proj.weight',\n",
       "              tensor([[ 0.0022,  0.1277, -0.0131,  ...,  0.0833, -0.0323, -0.1841],\n",
       "                      [ 0.1743,  0.0327, -0.2566,  ..., -0.0372, -0.0149,  0.1061],\n",
       "                      [ 0.2786,  0.0632,  0.1578,  ...,  0.1000,  0.0400,  0.3135],\n",
       "                      ...,\n",
       "                      [-0.1490, -0.1283, -0.1852,  ...,  0.0140, -0.1982,  0.0712],\n",
       "                      [-0.0038,  0.1048, -0.1748,  ...,  0.1429,  0.2014, -0.0426],\n",
       "                      [ 0.2123,  0.0413, -0.0383,  ...,  0.0410, -0.1842,  0.3259]])),\n",
       "             ('model.decoder.layers.5.encoder_attn.q_proj.bias',\n",
       "              tensor([-0.0406, -0.0256,  0.0312,  ..., -0.0875,  0.0524,  0.0250])),\n",
       "             ('model.decoder.layers.5.encoder_attn.out_proj.weight',\n",
       "              tensor([[ 0.0500, -0.2297, -0.2686,  ...,  0.1371, -0.0784,  0.0501],\n",
       "                      [ 0.3943,  0.1156,  0.1155,  ...,  0.1086, -0.0457,  0.1461],\n",
       "                      [-0.0682, -0.1912, -0.1614,  ...,  0.2607,  0.1146, -0.2332],\n",
       "                      ...,\n",
       "                      [-0.2773, -0.2771,  0.3865,  ..., -0.0358,  0.0470, -0.2032],\n",
       "                      [-0.0219,  0.0157, -0.1818,  ..., -0.1792, -0.1313,  0.1989],\n",
       "                      [ 0.0125,  0.2400, -0.2211,  ..., -0.0269, -0.1218, -0.4712]])),\n",
       "             ('model.decoder.layers.5.encoder_attn.out_proj.bias',\n",
       "              tensor([ 0.1956,  0.0657,  0.3936,  ...,  0.0127, -0.3669, -0.3601])),\n",
       "             ('model.decoder.layers.5.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1075, 0.1220, 0.1219,  ..., 0.0916, 0.0882, 0.0904])),\n",
       "             ('model.decoder.layers.5.encoder_attn_layer_norm.bias',\n",
       "              tensor([-0.0060, -0.0189, -0.0099,  ..., -0.0571, -0.0138, -0.0278])),\n",
       "             ('model.decoder.layers.5.fc1.weight',\n",
       "              tensor([[ 0.0246, -0.1316,  0.0693,  ...,  0.0710,  0.0610, -0.1390],\n",
       "                      [-0.0105,  0.1313, -0.1672,  ...,  0.0849,  0.3064, -0.0118],\n",
       "                      [-0.0682,  0.0808,  0.1024,  ..., -0.0484,  0.1665,  0.0039],\n",
       "                      ...,\n",
       "                      [ 0.3459,  0.1885,  0.3340,  ...,  0.1039, -0.0782, -0.0128],\n",
       "                      [ 0.0859, -0.0403,  0.0482,  ..., -0.0431,  0.1514,  0.0355],\n",
       "                      [ 0.1876,  0.1115, -0.5215,  ..., -0.3174, -0.3076,  0.0912]])),\n",
       "             ('model.decoder.layers.5.fc1.bias',\n",
       "              tensor([-0.1182, -0.0240, -0.1111,  ..., -0.0903,  0.0151, -0.2751])),\n",
       "             ('model.decoder.layers.5.fc2.weight',\n",
       "              tensor([[-0.0988, -0.2712,  0.0759,  ..., -0.0606,  0.0951,  0.0548],\n",
       "                      [-0.0896,  0.0380, -0.1500,  ...,  0.1359,  0.1032, -0.2303],\n",
       "                      [-0.2039,  0.2030, -0.0993,  ...,  0.0145, -0.0025,  0.0596],\n",
       "                      ...,\n",
       "                      [-0.2715,  0.0426,  0.1118,  ..., -0.2710,  0.0401,  0.3145],\n",
       "                      [ 0.3484, -0.2305, -0.2507,  ...,  0.0446, -0.0930,  0.0582],\n",
       "                      [ 0.0522,  0.2581,  0.1652,  ...,  0.1074, -0.0864,  0.0522]])),\n",
       "             ('model.decoder.layers.5.fc2.bias',\n",
       "              tensor([-0.0361,  0.0535,  0.2190,  ..., -0.0262,  0.0795, -0.0141])),\n",
       "             ('model.decoder.layers.5.final_layer_norm.weight',\n",
       "              tensor([0.5527, 0.5947, 0.5669,  ..., 0.5171, 0.5190, 0.5498])),\n",
       "             ('model.decoder.layers.5.final_layer_norm.bias',\n",
       "              tensor([-0.0238,  0.1709,  0.0458,  ..., -0.1510, -0.0305, -0.0568])),\n",
       "             ('model.decoder.layers.6.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0374,  0.2539,  0.0807,  ..., -0.2534, -0.1730,  0.1367],\n",
       "                      [ 0.1406,  0.1671, -0.1053,  ..., -0.0083, -0.1061,  0.0356],\n",
       "                      [ 0.1313,  0.2104,  0.0596,  ..., -0.2021,  0.1768,  0.1305],\n",
       "                      ...,\n",
       "                      [-0.0445,  0.3679,  0.3291,  ..., -0.1658,  0.0309,  0.0033],\n",
       "                      [ 0.1257,  0.0351,  0.0739,  ..., -0.0812, -0.0751,  0.0815],\n",
       "                      [-0.3774, -0.2363, -0.0306,  ...,  0.2869, -0.1887,  0.2236]])),\n",
       "             ('model.decoder.layers.6.self_attn.k_proj.bias',\n",
       "              tensor([-0.0245,  0.0265,  0.0033,  ...,  0.0219, -0.0003, -0.0004])),\n",
       "             ('model.decoder.layers.6.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.1777,  0.1213, -0.0913,  ..., -0.0241, -0.2084,  0.0174],\n",
       "                      [ 0.5625,  0.0407,  0.4060,  ...,  0.0525, -0.2460, -0.0887],\n",
       "                      [-0.0046, -0.1355,  0.1956,  ..., -0.2605,  0.1372, -0.1455],\n",
       "                      ...,\n",
       "                      [ 0.3633, -0.1598,  0.4990,  ...,  0.0370, -0.4541,  0.3074],\n",
       "                      [-0.6509,  0.4268,  0.5879,  ...,  0.2170, -0.2910,  0.5000],\n",
       "                      [ 0.0174, -0.2291,  0.1272,  ...,  0.3635,  0.0128, -0.2561]])),\n",
       "             ('model.decoder.layers.6.self_attn.v_proj.bias',\n",
       "              tensor([-0.0603,  0.0184,  0.1882,  ..., -0.0049,  0.0327, -0.1523])),\n",
       "             ('model.decoder.layers.6.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0482, -0.3528, -0.1902,  ...,  0.0988,  0.0460,  0.0452],\n",
       "                      [-0.0535, -0.1464,  0.1217,  ..., -0.1593, -0.1014,  0.0892],\n",
       "                      [-0.0216, -0.0822,  0.0185,  ..., -0.0079,  0.3145,  0.1541],\n",
       "                      ...,\n",
       "                      [-0.1473,  0.0253, -0.0071,  ..., -0.0023, -0.0278, -0.0009],\n",
       "                      [ 0.3037, -0.0680,  0.0512,  ..., -0.0131, -0.1829, -0.1847],\n",
       "                      [-0.0150, -0.1372, -0.0367,  ...,  0.0043, -0.1564,  0.1874]])),\n",
       "             ('model.decoder.layers.6.self_attn.q_proj.bias',\n",
       "              tensor([-0.1130, -0.0684, -0.0753,  ..., -0.3828, -0.1595,  0.0504])),\n",
       "             ('model.decoder.layers.6.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.1996,  0.2776,  0.0723,  ...,  0.6182, -0.4133, -0.1593],\n",
       "                      [-0.1759,  0.1315,  0.0181,  ...,  0.3464,  0.4951, -0.2612],\n",
       "                      [-0.2367,  0.1771,  0.1803,  ..., -0.5269, -0.2527, -0.2202],\n",
       "                      ...,\n",
       "                      [ 0.0767, -0.0532, -0.1245,  ...,  0.1116,  0.5005, -0.0312],\n",
       "                      [-0.0836,  0.0947,  0.0076,  ...,  0.2340, -0.0242, -0.0476],\n",
       "                      [ 0.0997,  0.3879, -0.1774,  ...,  0.1447,  0.4998,  0.0815]])),\n",
       "             ('model.decoder.layers.6.self_attn.out_proj.bias',\n",
       "              tensor([-0.2966,  0.2942, -0.3508,  ..., -0.3872,  0.0909,  0.3867])),\n",
       "             ('model.decoder.layers.6.self_attn_layer_norm.weight',\n",
       "              tensor([0.3369, 0.3384, 0.3252,  ..., 0.2742, 0.3064, 0.2991])),\n",
       "             ('model.decoder.layers.6.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0210,  0.0098,  0.0302,  ..., -0.0051,  0.0017, -0.0262])),\n",
       "             ('model.decoder.layers.6.encoder_attn.k_proj.weight',\n",
       "              tensor([[ 0.3401,  0.1907,  0.0227,  ...,  0.1218, -0.2205, -0.1613],\n",
       "                      [-0.1348,  0.0866,  0.0611,  ...,  0.0736,  0.0831, -0.0015],\n",
       "                      [-0.2825,  0.1018,  0.0981,  ..., -0.0231,  0.2522, -0.1433],\n",
       "                      ...,\n",
       "                      [-0.0963,  0.3123, -0.3767,  ...,  0.2382, -0.0230,  0.0461],\n",
       "                      [ 0.3325, -0.4280, -0.1127,  ..., -0.1572,  0.1757,  0.0476],\n",
       "                      [ 0.1704,  0.1230, -0.1737,  ..., -0.1663, -0.2683, -0.0545]])),\n",
       "             ('model.decoder.layers.6.encoder_attn.k_proj.bias',\n",
       "              tensor([-0.0267, -0.0164,  0.0116,  ...,  0.0257, -0.0025,  0.0022])),\n",
       "             ('model.decoder.layers.6.encoder_attn.v_proj.weight',\n",
       "              tensor([[ 1.6235e-01, -8.6121e-02, -4.5074e-02,  ...,  1.9666e-01,\n",
       "                       -3.8672e-01,  8.2092e-03],\n",
       "                      [-2.8613e-01,  3.4271e-02, -4.3677e-01,  ..., -5.0964e-02,\n",
       "                       -6.2012e-02,  7.8857e-02],\n",
       "                      [-2.7466e-01, -2.5659e-01,  4.3921e-01,  ..., -4.0820e-01,\n",
       "                       -3.2642e-01,  3.2349e-02],\n",
       "                      ...,\n",
       "                      [-4.6753e-02,  2.1582e-01,  1.4502e-01,  ...,  6.9336e-02,\n",
       "                        6.7322e-02, -1.3863e-02],\n",
       "                      [-7.0251e-02, -2.0645e-02, -1.1169e-01,  ...,  1.6626e-01,\n",
       "                        8.9355e-02,  2.6276e-02],\n",
       "                      [ 5.1416e-01, -5.3525e-05,  3.3600e-02,  ...,  2.0093e-01,\n",
       "                        2.9614e-01,  3.6316e-02]])),\n",
       "             ('model.decoder.layers.6.encoder_attn.v_proj.bias',\n",
       "              tensor([-0.0952, -0.1188, -0.1442,  ..., -0.0037,  0.0580,  0.0503])),\n",
       "             ('model.decoder.layers.6.encoder_attn.q_proj.weight',\n",
       "              tensor([[ 0.2246, -0.0369, -0.0668,  ..., -0.2939, -0.1271,  0.0820],\n",
       "                      [-0.2778, -0.0588,  0.1204,  ..., -0.2460,  0.1610, -0.0326],\n",
       "                      [ 0.1885,  0.0735,  0.1874,  ..., -0.1624,  0.1464, -0.0825],\n",
       "                      ...,\n",
       "                      [-0.1677,  0.0193, -0.3943,  ...,  0.0600, -0.1323, -0.0068],\n",
       "                      [-0.2080, -0.0361,  0.1622,  ..., -0.1429,  0.1573, -0.1770],\n",
       "                      [-0.0502,  0.0901,  0.1974,  ...,  0.0373,  0.0597,  0.1471]])),\n",
       "             ('model.decoder.layers.6.encoder_attn.q_proj.bias',\n",
       "              tensor([-0.0816, -0.0087, -0.1379,  ...,  0.0420,  0.0195, -0.1461])),\n",
       "             ('model.decoder.layers.6.encoder_attn.out_proj.weight',\n",
       "              tensor([[-0.0215, -0.2637, -0.1755,  ..., -0.4727, -0.0643,  0.0902],\n",
       "                      [ 0.0960,  0.2106,  0.0665,  ...,  0.1514, -0.2832, -0.0386],\n",
       "                      [ 0.2603, -0.0852,  0.4165,  ..., -0.0095, -0.2157, -0.1486],\n",
       "                      ...,\n",
       "                      [-0.0221,  0.1790, -0.2588,  ..., -0.1587, -0.1099, -0.0893],\n",
       "                      [-0.1489,  0.0752,  0.2411,  ...,  0.0696, -0.1183, -0.0610],\n",
       "                      [-0.1570, -0.2546, -0.0327,  ...,  0.0521, -0.1205,  0.4062]])),\n",
       "             ('model.decoder.layers.6.encoder_attn.out_proj.bias',\n",
       "              tensor([ 0.4001,  0.2559,  0.4946,  ...,  0.2712, -0.3901, -0.4368])),\n",
       "             ('model.decoder.layers.6.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1088, 0.1266, 0.1223,  ..., 0.0982, 0.0980, 0.0988])),\n",
       "             ('model.decoder.layers.6.encoder_attn_layer_norm.bias',\n",
       "              tensor([-0.0053, -0.0124, -0.0187,  ..., -0.0555, -0.0078, -0.0295])),\n",
       "             ('model.decoder.layers.6.fc1.weight',\n",
       "              tensor([[-0.0851,  0.0754,  0.1345,  ...,  0.3159,  0.1464,  0.0422],\n",
       "                      [ 0.2727,  0.1659, -0.0242,  ..., -0.1598, -0.0930,  0.0657],\n",
       "                      [ 0.1584, -0.0405, -0.1112,  ..., -0.0997, -0.1469, -0.2573],\n",
       "                      ...,\n",
       "                      [ 0.2394, -0.2903, -0.0901,  ...,  0.3789,  0.2290, -0.3430],\n",
       "                      [ 0.0126,  0.0057, -0.0182,  ...,  0.0438, -0.3665, -0.0600],\n",
       "                      [-0.3196,  0.0820,  0.0243,  ...,  0.1395, -0.0446, -0.4631]])),\n",
       "             ('model.decoder.layers.6.fc1.bias',\n",
       "              tensor([-0.1229, -0.1085, -0.0858,  ..., -0.3970, -0.1575,  0.0030])),\n",
       "             ('model.decoder.layers.6.fc2.weight',\n",
       "              tensor([[-0.1260, -0.0411, -0.0211,  ..., -0.1797,  0.1743,  0.2720],\n",
       "                      [-0.2678,  0.2886,  0.2220,  ...,  0.0752,  0.0397,  0.1187],\n",
       "                      [-0.2213, -0.2236, -0.0878,  ..., -0.0323, -0.0224, -0.0335],\n",
       "                      ...,\n",
       "                      [-0.1696,  0.0558,  0.2617,  ..., -0.2012, -0.1853, -0.0159],\n",
       "                      [-0.1678, -0.0100, -0.1174,  ..., -0.1779,  0.0478,  0.1150],\n",
       "                      [ 0.1125, -0.0822,  0.0068,  ..., -0.2668, -0.2499,  0.3943]])),\n",
       "             ('model.decoder.layers.6.fc2.bias',\n",
       "              tensor([ 0.0576, -0.2017,  0.3735,  ..., -0.0267,  0.5044,  0.0363])),\n",
       "             ('model.decoder.layers.6.final_layer_norm.weight',\n",
       "              tensor([0.6333, 0.6772, 0.6577,  ..., 0.6431, 0.6191, 0.6387])),\n",
       "             ('model.decoder.layers.6.final_layer_norm.bias',\n",
       "              tensor([-0.0312,  0.2100,  0.0613,  ..., -0.2198, -0.0563,  0.0056])),\n",
       "             ('model.decoder.layers.7.self_attn.k_proj.weight',\n",
       "              tensor([[-0.2045,  0.0935, -0.1895,  ..., -0.0121, -0.1221, -0.0599],\n",
       "                      [-0.0090, -0.3213, -0.0044,  ..., -0.0103, -0.0870,  0.2140],\n",
       "                      [-0.0508, -0.1552, -0.1685,  ..., -0.0257, -0.1137, -0.3550],\n",
       "                      ...,\n",
       "                      [ 0.0738,  0.0740, -0.0399,  ...,  0.0630,  0.0160, -0.0910],\n",
       "                      [ 0.0360, -0.0048, -0.0846,  ..., -0.0117,  0.0858, -0.0630],\n",
       "                      [-0.0284,  0.0037, -0.2744,  ..., -0.1581, -0.0492, -0.3699]])),\n",
       "             ('model.decoder.layers.7.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0037,  0.0206,  0.0255,  ..., -0.0012, -0.0041, -0.0044])),\n",
       "             ('model.decoder.layers.7.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0089,  0.4580,  0.1262,  ...,  0.0078,  0.3901, -0.1656],\n",
       "                      [ 0.1880, -0.0881,  0.2271,  ..., -0.1149, -0.0731, -0.2908],\n",
       "                      [-0.1429, -0.2030, -0.2288,  ..., -0.1512,  0.1527, -0.1395],\n",
       "                      ...,\n",
       "                      [ 0.1218,  0.2458, -0.0825,  ...,  0.2137,  0.2061, -0.0891],\n",
       "                      [ 0.1573,  0.2622, -0.0113,  ...,  0.0221, -0.0740,  0.0135],\n",
       "                      [-0.1108, -0.0303,  0.2396,  ..., -0.0332, -0.0999,  0.1989]])),\n",
       "             ('model.decoder.layers.7.self_attn.v_proj.bias',\n",
       "              tensor([-0.0156,  0.0625,  0.0502,  ..., -0.3411,  0.2294,  0.1083])),\n",
       "             ('model.decoder.layers.7.self_attn.q_proj.weight',\n",
       "              tensor([[-0.1903, -0.0818,  0.0698,  ..., -0.1096, -0.1135, -0.2006],\n",
       "                      [-0.1033,  0.3027,  0.1921,  ...,  0.1516, -0.0990,  0.0273],\n",
       "                      [ 0.1155,  0.1646, -0.0541,  ..., -0.2522,  0.0373, -0.0517],\n",
       "                      ...,\n",
       "                      [-0.1730, -0.1442,  0.3608,  ...,  0.0540, -0.0877,  0.0861],\n",
       "                      [ 0.1680,  0.1840, -0.1689,  ...,  0.0338,  0.0041,  0.0251],\n",
       "                      [ 0.0446,  0.1721, -0.0281,  ...,  0.0358,  0.1898,  0.2507]])),\n",
       "             ('model.decoder.layers.7.self_attn.q_proj.bias',\n",
       "              tensor([ 0.0382, -0.0410, -0.0156,  ..., -0.2959, -0.1064, -0.0396])),\n",
       "             ('model.decoder.layers.7.self_attn.out_proj.weight',\n",
       "              tensor([[-0.1278, -0.2954, -0.0012,  ...,  0.0865, -0.0617,  0.0590],\n",
       "                      [-0.0119,  0.1688,  0.1304,  ...,  0.3594,  0.0177, -0.3987],\n",
       "                      [-0.4302,  0.1376,  0.2698,  ...,  0.2532,  0.1486,  0.0579],\n",
       "                      ...,\n",
       "                      [ 0.1769,  0.1301,  0.2328,  ...,  0.0317, -0.1260, -0.1396],\n",
       "                      [ 0.1765,  0.1703,  0.3884,  ...,  0.1753, -0.0198,  0.1588],\n",
       "                      [-0.3267, -0.1447, -0.5879,  ..., -0.1171,  0.1121,  0.1244]])),\n",
       "             ('model.decoder.layers.7.self_attn.out_proj.bias',\n",
       "              tensor([-0.1833,  0.3335, -0.2705,  ..., -0.1660, -0.0508,  0.5049])),\n",
       "             ('model.decoder.layers.7.self_attn_layer_norm.weight',\n",
       "              tensor([0.3381, 0.3489, 0.3391,  ..., 0.2952, 0.3115, 0.3103])),\n",
       "             ('model.decoder.layers.7.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0134,  0.0132,  0.0375,  ..., -0.0148,  0.0051, -0.0371])),\n",
       "             ('model.decoder.layers.7.encoder_attn.k_proj.weight',\n",
       "              tensor([[-0.3711, -0.0614,  0.0996,  ..., -0.1182,  0.0873, -0.0322],\n",
       "                      [-0.2235,  0.3223,  0.1847,  ..., -0.1969, -0.2405,  0.1166],\n",
       "                      [-0.2136,  0.0717, -0.1069,  ...,  0.2681,  0.1412,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.2046, -0.1971, -0.0922,  ..., -0.1748,  0.0704, -0.0338],\n",
       "                      [-0.2839,  0.0338, -0.0923,  ..., -0.1671, -0.1193, -0.1566],\n",
       "                      [ 0.1528,  0.1070,  0.4490,  ..., -0.1133,  0.0535, -0.0446]])),\n",
       "             ('model.decoder.layers.7.encoder_attn.k_proj.bias',\n",
       "              tensor([-0.0200, -0.0157, -0.0066,  ..., -0.0196,  0.0060, -0.0199])),\n",
       "             ('model.decoder.layers.7.encoder_attn.v_proj.weight',\n",
       "              tensor([[ 0.1058, -0.1161, -0.4517,  ..., -0.2382,  0.5146,  0.0323],\n",
       "                      [-0.3267, -0.0266, -0.2542,  ...,  0.1765,  0.0455,  0.0431],\n",
       "                      [-0.0636, -0.2844, -0.0081,  ...,  0.1559,  0.4092, -0.1288],\n",
       "                      ...,\n",
       "                      [ 0.1447,  0.1050,  0.2108,  ..., -0.6055,  0.3354, -0.0129],\n",
       "                      [ 0.3230,  0.1869,  0.1417,  ...,  0.0695, -0.3398, -0.0172],\n",
       "                      [ 0.1238, -0.2754,  0.4731,  ..., -0.1932, -0.1099, -0.0350]])),\n",
       "             ('model.decoder.layers.7.encoder_attn.v_proj.bias',\n",
       "              tensor([-0.1930, -0.0421,  0.1481,  ..., -0.1722, -0.0246,  0.0468])),\n",
       "             ('model.decoder.layers.7.encoder_attn.q_proj.weight',\n",
       "              tensor([[-0.4705, -0.0850, -0.0084,  ..., -0.3408,  0.0473,  0.0865],\n",
       "                      [ 0.1898,  0.0959,  0.1753,  ...,  0.1469, -0.1388,  0.1073],\n",
       "                      [ 0.1635,  0.0573, -0.0794,  ..., -0.0566, -0.0975, -0.1176],\n",
       "                      ...,\n",
       "                      [ 0.0769, -0.1356, -0.0036,  ..., -0.0539,  0.0759, -0.1608],\n",
       "                      [ 0.1654, -0.2303,  0.0085,  ..., -0.2002, -0.1268, -0.1071],\n",
       "                      [-0.1133, -0.1595, -0.2695,  ..., -0.3528,  0.2029,  0.0522]])),\n",
       "             ('model.decoder.layers.7.encoder_attn.q_proj.bias',\n",
       "              tensor([-0.0659,  0.3047,  0.0176,  ..., -0.0298, -0.1838,  0.0766])),\n",
       "             ('model.decoder.layers.7.encoder_attn.out_proj.weight',\n",
       "              tensor([[ 0.1005, -0.2866,  0.0150,  ...,  0.0687,  0.0201,  0.1119],\n",
       "                      [-0.3127,  0.0046, -0.4910,  ...,  0.1282, -0.5044, -0.1580],\n",
       "                      [-0.0880,  0.1805, -0.3584,  ...,  0.2240, -0.0374, -0.4968],\n",
       "                      ...,\n",
       "                      [-0.4656,  0.1459, -0.1335,  ..., -0.4026, -0.0428, -0.1377],\n",
       "                      [ 0.1560,  0.2075,  0.0927,  ..., -0.0337,  0.0627,  0.0725],\n",
       "                      [-0.0769,  0.4978,  0.0244,  ...,  0.6108, -0.0981,  0.1642]])),\n",
       "             ('model.decoder.layers.7.encoder_attn.out_proj.bias',\n",
       "              tensor([ 0.1737,  0.0965,  0.4023,  ..., -0.2581, -0.4963, -0.3105])),\n",
       "             ('model.decoder.layers.7.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1185, 0.1268, 0.1243,  ..., 0.0994, 0.1035, 0.1086])),\n",
       "             ('model.decoder.layers.7.encoder_attn_layer_norm.bias',\n",
       "              tensor([-0.0175, -0.0222, -0.0108,  ..., -0.0543, -0.0084, -0.0273])),\n",
       "             ('model.decoder.layers.7.fc1.weight',\n",
       "              tensor([[ 0.0210, -0.0683,  0.2157,  ...,  0.0250,  0.3140,  0.4192],\n",
       "                      [ 0.2238,  0.2529,  0.1068,  ...,  0.2678,  0.0637,  0.1896],\n",
       "                      [ 0.4622,  0.0760,  0.2435,  ...,  0.1342,  0.2158, -0.1066],\n",
       "                      ...,\n",
       "                      [-0.0657,  0.0519, -0.1526,  ...,  0.2031,  0.1222, -0.0950],\n",
       "                      [ 0.3167,  0.2002,  0.4006,  ...,  0.2532,  0.0899,  0.1620],\n",
       "                      [ 0.0201, -0.0225, -0.0485,  ...,  0.0050, -0.2136, -0.0174]])),\n",
       "             ('model.decoder.layers.7.fc1.bias',\n",
       "              tensor([-0.1454,  0.0207, -0.2444,  ..., -0.2520, -0.2686, -0.0136])),\n",
       "             ('model.decoder.layers.7.fc2.weight',\n",
       "              tensor([[ 0.0240,  0.0366, -0.3574,  ..., -0.0224, -0.1803, -0.0250],\n",
       "                      [-0.1412, -0.0702, -0.0832,  ...,  0.1190,  0.1544, -0.0342],\n",
       "                      [ 0.1305, -0.0353, -0.0725,  ..., -0.1155,  0.4028, -0.3269],\n",
       "                      ...,\n",
       "                      [-0.0705, -0.2150,  0.0446,  ...,  0.0066,  0.0831,  0.1426],\n",
       "                      [-0.1654,  0.1814, -0.2573,  ..., -0.2487,  0.2815, -0.0829],\n",
       "                      [-0.1708,  0.1843,  0.0834,  ...,  0.1592,  0.1464, -0.4170]])),\n",
       "             ('model.decoder.layers.7.fc2.bias',\n",
       "              tensor([ 0.3928, -0.1920,  0.4954,  ..., -0.3877,  0.5059,  0.1106])),\n",
       "             ('model.decoder.layers.7.final_layer_norm.weight',\n",
       "              tensor([0.7974, 0.8071, 0.7432,  ..., 0.8247, 0.7603, 0.7881])),\n",
       "             ('model.decoder.layers.7.final_layer_norm.bias',\n",
       "              tensor([-0.1459,  0.1334,  0.0034,  ..., -0.1862, -0.0635, -0.1029])),\n",
       "             ('model.decoder.layers.8.self_attn.k_proj.weight',\n",
       "              tensor([[-0.1423,  0.0493, -0.0364,  ..., -0.2412, -0.0038, -0.1906],\n",
       "                      [-0.1034,  0.0400, -0.0563,  ...,  0.0095,  0.1847,  0.1011],\n",
       "                      [ 0.0593, -0.0185, -0.1671,  ...,  0.0114, -0.2832, -0.0166],\n",
       "                      ...,\n",
       "                      [-0.0690, -0.1224,  0.2441,  ..., -0.0893,  0.0546,  0.2991],\n",
       "                      [-0.0170, -0.0854,  0.1444,  ..., -0.0768,  0.1268,  0.1306],\n",
       "                      [ 0.1249,  0.1434,  0.1725,  ..., -0.2191, -0.0253, -0.0129]])),\n",
       "             ('model.decoder.layers.8.self_attn.k_proj.bias',\n",
       "              tensor([-0.0060,  0.0152,  0.0233,  ..., -0.0166,  0.0221, -0.0134])),\n",
       "             ('model.decoder.layers.8.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.1410,  0.1727, -0.0191,  ..., -0.1436,  0.0425,  0.3655],\n",
       "                      [ 0.2812, -0.0403,  0.1802,  ..., -0.1816, -0.2754, -0.1415],\n",
       "                      [ 0.1836,  0.2639, -0.3894,  ..., -0.3167,  0.1504, -0.4910],\n",
       "                      ...,\n",
       "                      [ 0.2883, -0.0306, -0.3403,  ...,  0.0630, -0.2426, -0.0954],\n",
       "                      [-0.0223,  0.1888, -0.0462,  ..., -0.0820,  0.1877, -0.0966],\n",
       "                      [-0.1565, -0.0103,  0.1691,  ..., -0.1262, -0.2935,  0.1974]])),\n",
       "             ('model.decoder.layers.8.self_attn.v_proj.bias',\n",
       "              tensor([ 0.0117,  0.1648,  0.0165,  ..., -0.2168,  0.2529, -0.0457])),\n",
       "             ('model.decoder.layers.8.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0427, -0.1158, -0.1973,  ..., -0.0760,  0.0096,  0.0612],\n",
       "                      [ 0.0319, -0.3293,  0.0804,  ...,  0.0157,  0.0489,  0.0158],\n",
       "                      [ 0.1538,  0.0602,  0.1025,  ...,  0.0074, -0.0712, -0.3633],\n",
       "                      ...,\n",
       "                      [-0.0150, -0.0189,  0.0870,  ...,  0.0556,  0.0825,  0.0517],\n",
       "                      [ 0.0912, -0.2279, -0.0597,  ...,  0.1247, -0.0776, -0.0311],\n",
       "                      [ 0.0519,  0.0293, -0.0393,  ...,  0.1355,  0.0712, -0.2561]])),\n",
       "             ('model.decoder.layers.8.self_attn.q_proj.bias',\n",
       "              tensor([-0.1656, -0.0432, -0.0004,  ...,  0.0256, -0.1768, -0.0460])),\n",
       "             ('model.decoder.layers.8.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.4148,  0.2620,  0.1461,  ..., -0.2103, -0.0971, -0.0137],\n",
       "                      [ 0.0315,  0.1788, -0.1793,  ...,  0.2517, -0.2411, -0.2437],\n",
       "                      [ 0.2203, -0.0828, -0.3955,  ...,  0.0433, -0.3066, -0.2430],\n",
       "                      ...,\n",
       "                      [ 0.2292,  0.0508, -0.0446,  ..., -0.0126,  0.1786, -0.0538],\n",
       "                      [-0.1774, -0.1196, -0.0755,  ...,  0.2372, -0.1290,  0.0710],\n",
       "                      [-0.3435,  0.0744,  0.2435,  ...,  0.1345, -0.1583, -0.0632]])),\n",
       "             ('model.decoder.layers.8.self_attn.out_proj.bias',\n",
       "              tensor([ 0.1243,  0.2539, -0.2976,  ...,  0.2218,  0.1137,  0.5005])),\n",
       "             ('model.decoder.layers.8.self_attn_layer_norm.weight',\n",
       "              tensor([0.4102, 0.3804, 0.3806,  ..., 0.3447, 0.3757, 0.3708])),\n",
       "             ('model.decoder.layers.8.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0182,  0.0095,  0.0290,  ..., -0.0136,  0.0106, -0.0280])),\n",
       "             ('model.decoder.layers.8.encoder_attn.k_proj.weight',\n",
       "              tensor([[-0.1570, -0.1323,  0.1772,  ...,  0.0469, -0.0352,  0.0019],\n",
       "                      [ 0.1349, -0.2822,  0.0867,  ...,  0.0389,  0.1478, -0.0174],\n",
       "                      [ 0.3291,  0.3396, -0.0086,  ...,  0.1729, -0.1600, -0.0617],\n",
       "                      ...,\n",
       "                      [ 0.0868, -0.1453,  0.1772,  ...,  0.0400,  0.0132,  0.0280],\n",
       "                      [ 0.2019, -0.1984, -0.2167,  ..., -0.0485,  0.0724,  0.0322],\n",
       "                      [-0.0590, -0.1819,  0.2715,  ..., -0.0208,  0.1816,  0.0951]])),\n",
       "             ('model.decoder.layers.8.encoder_attn.k_proj.bias',\n",
       "              tensor([-0.0289,  0.0023, -0.0247,  ..., -0.0080, -0.0143, -0.0205])),\n",
       "             ('model.decoder.layers.8.encoder_attn.v_proj.weight',\n",
       "              tensor([[ 0.1135,  0.0676,  0.0016,  ...,  0.1982,  0.0425, -0.0825],\n",
       "                      [ 0.2925, -0.0800, -0.0367,  ...,  0.3589,  0.2947,  0.0084],\n",
       "                      [ 0.2712, -0.0820,  0.3889,  ..., -0.0808, -0.3374, -0.0158],\n",
       "                      ...,\n",
       "                      [ 0.2910,  0.1021, -0.0634,  ..., -0.1815,  0.0561,  0.0302],\n",
       "                      [-0.0091,  0.0112,  0.0781,  ...,  0.1381, -0.3035, -0.0421],\n",
       "                      [ 0.1682, -0.0823, -0.3062,  ..., -0.4304,  0.1691, -0.1249]])),\n",
       "             ('model.decoder.layers.8.encoder_attn.v_proj.bias',\n",
       "              tensor([ 0.0173, -0.0172,  0.1008,  ..., -0.0360,  0.2008,  0.3875])),\n",
       "             ('model.decoder.layers.8.encoder_attn.q_proj.weight',\n",
       "              tensor([[-0.1891, -0.1931, -0.1881,  ..., -0.2788, -0.0663, -0.0147],\n",
       "                      [ 0.1151, -0.2732, -0.0383,  ..., -0.3081, -0.1140,  0.0378],\n",
       "                      [ 0.0862,  0.1174,  0.1058,  ...,  0.0028,  0.0516,  0.1337],\n",
       "                      ...,\n",
       "                      [ 0.1978, -0.2303,  0.2014,  ...,  0.0764, -0.0551, -0.0450],\n",
       "                      [-0.0210,  0.0884,  0.1086,  ...,  0.0174, -0.0796,  0.1521],\n",
       "                      [ 0.0110, -0.0436, -0.0723,  ..., -0.0287, -0.0842, -0.0739]])),\n",
       "             ('model.decoder.layers.8.encoder_attn.q_proj.bias',\n",
       "              tensor([ 0.0989,  0.0967, -0.0564,  ..., -0.0964, -0.0172,  0.1020])),\n",
       "             ('model.decoder.layers.8.encoder_attn.out_proj.weight',\n",
       "              tensor([[-0.1542,  0.2229, -0.0826,  ..., -0.1195, -0.4167,  0.3042],\n",
       "                      [ 0.1630, -0.0679,  0.4854,  ..., -0.0095,  0.1709,  0.2983],\n",
       "                      [ 0.0172,  0.0249, -0.0252,  ..., -0.3215,  0.0625,  0.4380],\n",
       "                      ...,\n",
       "                      [-0.0400, -0.0425,  0.1798,  ...,  0.2079,  0.4265, -0.1339],\n",
       "                      [-0.0088,  0.0418, -0.2290,  ..., -0.1038, -0.1912,  0.1637],\n",
       "                      [-0.0147,  0.0756,  0.0246,  ..., -0.1266, -0.1960,  0.1256]])),\n",
       "             ('model.decoder.layers.8.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.3616,  0.0895, -0.0173,  ...,  0.2443, -0.3513, -0.4993])),\n",
       "             ('model.decoder.layers.8.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1331, 0.1266, 0.1376,  ..., 0.1132, 0.1064, 0.1123])),\n",
       "             ('model.decoder.layers.8.encoder_attn_layer_norm.bias',\n",
       "              tensor([-0.0070, -0.0088, -0.0046,  ..., -0.0548, -0.0182, -0.0306])),\n",
       "             ('model.decoder.layers.8.fc1.weight',\n",
       "              tensor([[-0.4417, -0.2146,  0.1429,  ..., -0.1768, -0.0359,  0.2349],\n",
       "                      [-0.0397, -0.4207,  0.0406,  ...,  0.1848, -0.0008, -0.0076],\n",
       "                      [ 0.1375, -0.1194,  0.0920,  ..., -0.1770,  0.4021, -0.0606],\n",
       "                      ...,\n",
       "                      [-0.0806, -0.1650,  0.2084,  ...,  0.1735,  0.2954, -0.1643],\n",
       "                      [ 0.5210, -0.0171,  0.2417,  ...,  0.1959, -0.0807, -0.0129],\n",
       "                      [-0.1121,  0.0619, -0.0764,  ..., -0.1229, -0.0679, -0.0245]])),\n",
       "             ('model.decoder.layers.8.fc1.bias',\n",
       "              tensor([-0.1028, -0.3286, -0.2461,  ..., -0.3433, -0.4426, -0.0970])),\n",
       "             ('model.decoder.layers.8.fc2.weight',\n",
       "              tensor([[ 0.0093,  0.0911, -0.0565,  ..., -0.0317, -0.2344, -0.0305],\n",
       "                      [-0.1876,  0.0094,  0.1968,  ..., -0.0558, -0.0459,  0.0405],\n",
       "                      [ 0.0945, -0.0292,  0.0138,  ..., -0.2668,  0.0869,  0.0090],\n",
       "                      ...,\n",
       "                      [-0.2488,  0.2688,  0.2932,  ...,  0.2343, -0.2141, -0.0137],\n",
       "                      [ 0.0183,  0.1749, -0.2096,  ...,  0.0723,  0.1492,  0.2289],\n",
       "                      [ 0.0099, -0.0686,  0.1202,  ...,  0.0895,  0.4971,  0.1537]])),\n",
       "             ('model.decoder.layers.8.fc2.bias',\n",
       "              tensor([ 0.5020, -0.3313,  0.4246,  ..., -0.5049,  0.5127, -0.2952])),\n",
       "             ('model.decoder.layers.8.final_layer_norm.weight',\n",
       "              tensor([1.0195, 1.0068, 0.9004,  ..., 1.0664, 0.9517, 1.0547])),\n",
       "             ('model.decoder.layers.8.final_layer_norm.bias',\n",
       "              tensor([-0.0638,  0.1104, -0.0249,  ..., -0.1152, -0.0183, -0.0844])),\n",
       "             ('model.decoder.layers.9.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0412,  0.0670, -0.1750,  ...,  0.0121,  0.0238, -0.1610],\n",
       "                      [-0.0029, -0.0763, -0.0480,  ..., -0.0039, -0.0032, -0.0043],\n",
       "                      [-0.1422, -0.0167, -0.0263,  ..., -0.0453,  0.0587,  0.0488],\n",
       "                      ...,\n",
       "                      [-0.0135, -0.2300, -0.1558,  ...,  0.0684, -0.0237, -0.0024],\n",
       "                      [ 0.0916, -0.0360,  0.0775,  ...,  0.0323,  0.3794,  0.0118],\n",
       "                      [ 0.0227, -0.2444, -0.0505,  ...,  0.1031,  0.0501, -0.0388]])),\n",
       "             ('model.decoder.layers.9.self_attn.k_proj.bias',\n",
       "              tensor([ 0.0125, -0.0178, -0.0150,  ...,  0.0234, -0.0055, -0.0001])),\n",
       "             ('model.decoder.layers.9.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0773, -0.0530,  0.0605,  ..., -0.1372,  0.0030,  0.0672],\n",
       "                      [ 0.2700,  0.0829, -0.3567,  ...,  0.2375, -0.0533,  0.0267],\n",
       "                      [ 0.0301,  0.2338,  0.0894,  ..., -0.2131, -0.3374,  0.2045],\n",
       "                      ...,\n",
       "                      [-0.2666, -0.2839, -0.1636,  ...,  0.3601,  0.0040, -0.3984],\n",
       "                      [-0.0587,  0.0031, -0.1780,  ...,  0.2126,  0.2250,  0.1445],\n",
       "                      [-0.2861,  0.4861,  0.2120,  ..., -0.1197,  0.0020, -0.3445]])),\n",
       "             ('model.decoder.layers.9.self_attn.v_proj.bias',\n",
       "              tensor([ 0.2822,  0.0126,  0.1081,  ..., -0.1305, -0.0354, -0.1240])),\n",
       "             ('model.decoder.layers.9.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0190,  0.2964, -0.2443,  ..., -0.0494,  0.1162, -0.1262],\n",
       "                      [-0.0831,  0.1069, -0.2057,  ...,  0.0015, -0.0763, -0.0235],\n",
       "                      [-0.0543, -0.1862, -0.0384,  ...,  0.0033,  0.2153,  0.0687],\n",
       "                      ...,\n",
       "                      [ 0.1157,  0.0988,  0.0569,  ...,  0.2520,  0.2612, -0.1421],\n",
       "                      [ 0.1771, -0.0110, -0.0386,  ...,  0.0368, -0.1282,  0.1573],\n",
       "                      [ 0.0934, -0.0471,  0.1450,  ...,  0.0489, -0.0100, -0.0621]])),\n",
       "             ('model.decoder.layers.9.self_attn.q_proj.bias',\n",
       "              tensor([ 0.0312, -0.0041,  0.0637,  ...,  0.0500,  0.0525, -0.0266])),\n",
       "             ('model.decoder.layers.9.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.0059,  0.2668,  0.2118,  ..., -0.2654, -0.0499,  0.2559],\n",
       "                      [-0.2656,  0.0468, -0.2856,  ...,  0.0208, -0.1278, -0.0139],\n",
       "                      [ 0.3335, -0.2891,  0.0821,  ...,  0.1823, -0.1914,  0.5107],\n",
       "                      ...,\n",
       "                      [ 0.3220, -0.3181,  0.3914,  ..., -0.2502, -0.1149, -0.1750],\n",
       "                      [ 0.0428,  0.4807, -0.2045,  ..., -0.2837,  0.4756,  0.1137],\n",
       "                      [ 0.2433,  0.1946,  0.1001,  ..., -0.3577,  0.1799,  0.0641]])),\n",
       "             ('model.decoder.layers.9.self_attn.out_proj.bias',\n",
       "              tensor([ 0.1238, -0.4033, -0.2258,  ...,  0.2472,  0.3831,  0.5000])),\n",
       "             ('model.decoder.layers.9.self_attn_layer_norm.weight',\n",
       "              tensor([0.4631, 0.4277, 0.4272,  ..., 0.4153, 0.4421, 0.4478])),\n",
       "             ('model.decoder.layers.9.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0149,  0.0303,  0.0506,  ..., -0.0093,  0.0063, -0.0349])),\n",
       "             ('model.decoder.layers.9.encoder_attn.k_proj.weight',\n",
       "              tensor([[ 0.0770,  0.0722,  0.1160,  ...,  0.3164,  0.0057, -0.0553],\n",
       "                      [ 0.3330, -0.0047,  0.2705,  ..., -0.0791,  0.2482,  0.0441],\n",
       "                      [-0.3704, -0.2062, -0.1801,  ..., -0.1881, -0.1003,  0.1001],\n",
       "                      ...,\n",
       "                      [-0.1610, -0.0007,  0.0804,  ..., -0.1913,  0.0624, -0.0886],\n",
       "                      [ 0.3018,  0.2052, -0.0257,  ..., -0.0631,  0.1400,  0.0320],\n",
       "                      [ 0.1862,  0.1296,  0.1224,  ..., -0.0197, -0.2944, -0.0590]])),\n",
       "             ('model.decoder.layers.9.encoder_attn.k_proj.bias',\n",
       "              tensor([-0.0226, -0.0145,  0.0011,  ...,  0.0308, -0.0099,  0.0002])),\n",
       "             ('model.decoder.layers.9.encoder_attn.v_proj.weight',\n",
       "              tensor([[-0.3394,  0.0433, -0.1200,  ...,  0.0173, -0.2808, -0.0782],\n",
       "                      [-0.0045,  0.4124,  0.1859,  ..., -0.4805,  0.1549,  0.0018],\n",
       "                      [-0.4233,  0.1075,  0.4949,  ...,  0.1077,  0.1019, -0.0264],\n",
       "                      ...,\n",
       "                      [ 0.2230, -0.0882, -0.2074,  ...,  0.0633,  0.1359, -0.0247],\n",
       "                      [-0.5117,  0.3386,  0.1121,  ..., -0.0284, -0.0029, -0.0371],\n",
       "                      [ 0.0726,  0.0780,  0.4983,  ...,  0.0255, -0.1683, -0.0468]])),\n",
       "             ('model.decoder.layers.9.encoder_attn.v_proj.bias',\n",
       "              tensor([ 0.3779, -0.3152,  0.2307,  ...,  0.1636,  0.0676,  0.1646])),\n",
       "             ('model.decoder.layers.9.encoder_attn.q_proj.weight',\n",
       "              tensor([[ 0.2583,  0.1307,  0.1001,  ..., -0.1404,  0.1581, -0.2004],\n",
       "                      [-0.3770,  0.1218, -0.2306,  ..., -0.0723, -0.1956, -0.0639],\n",
       "                      [-0.0841, -0.5142, -0.1659,  ..., -0.0734, -0.1177,  0.3169],\n",
       "                      ...,\n",
       "                      [-0.2815,  0.1479,  0.0992,  ..., -0.1552, -0.0829, -0.0184],\n",
       "                      [-0.2330, -0.3279,  0.1219,  ..., -0.0037,  0.1787, -0.1523],\n",
       "                      [-0.0017,  0.0161,  0.3259,  ..., -0.0775,  0.1696,  0.1904]])),\n",
       "             ('model.decoder.layers.9.encoder_attn.q_proj.bias',\n",
       "              tensor([ 0.0332,  0.0112, -0.0296,  ...,  0.0381, -0.1301,  0.1093])),\n",
       "             ('model.decoder.layers.9.encoder_attn.out_proj.weight',\n",
       "              tensor([[ 0.1061, -0.4470, -0.2705,  ...,  0.2384,  0.1384, -0.0640],\n",
       "                      [-0.0952,  0.2932,  0.1537,  ..., -0.0050,  0.5049,  0.0322],\n",
       "                      [-0.0566,  0.2688,  0.1437,  ...,  0.0014, -0.3093, -0.1046],\n",
       "                      ...,\n",
       "                      [ 0.1476, -0.1459,  0.0959,  ...,  0.0439,  0.0358,  0.0531],\n",
       "                      [-0.2729,  0.0743, -0.1349,  ..., -0.3132, -0.3823,  0.0961],\n",
       "                      [-0.0887,  0.0021,  0.0411,  ...,  0.0129,  0.2395, -0.0094]])),\n",
       "             ('model.decoder.layers.9.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.2170, -0.0152,  0.1914,  ...,  0.2715, -0.2610,  0.1967])),\n",
       "             ('model.decoder.layers.9.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1378, 0.1429, 0.1512,  ..., 0.1249, 0.1181, 0.1212])),\n",
       "             ('model.decoder.layers.9.encoder_attn_layer_norm.bias',\n",
       "              tensor([-0.0044, -0.0116, -0.0141,  ..., -0.0624, -0.0088, -0.0331])),\n",
       "             ('model.decoder.layers.9.fc1.weight',\n",
       "              tensor([[-0.1227, -0.1321,  0.0471,  ..., -0.0699,  0.3000,  0.0210],\n",
       "                      [ 0.0005, -0.0152,  0.2771,  ...,  0.0279,  0.3499,  0.0677],\n",
       "                      [ 0.1163, -0.1272,  0.0693,  ..., -0.0362, -0.1906, -0.1945],\n",
       "                      ...,\n",
       "                      [ 0.0235,  0.0792, -0.1301,  ..., -0.2754,  0.1683,  0.1143],\n",
       "                      [ 0.2629,  0.1120,  0.1351,  ..., -0.4358,  0.3750,  0.1439],\n",
       "                      [-0.0416, -0.0441, -0.0393,  ...,  0.2874, -0.1884, -0.1794]])),\n",
       "             ('model.decoder.layers.9.fc1.bias',\n",
       "              tensor([-0.1962,  0.0445, -0.2059,  ..., -0.0576, -0.1960, -0.2871])),\n",
       "             ('model.decoder.layers.9.fc2.weight',\n",
       "              tensor([[ 8.3008e-02, -5.6122e-02, -7.4219e-02,  ...,  1.0742e-01,\n",
       "                        1.8079e-01,  3.5797e-02],\n",
       "                      [ 6.8481e-02, -3.7988e-01,  6.8176e-02,  ..., -1.2708e-01,\n",
       "                       -2.2437e-01, -1.4900e-02],\n",
       "                      [ 6.8115e-02,  1.5976e-02, -5.3986e-02,  ...,  9.3079e-03,\n",
       "                       -7.8064e-02, -2.0206e-04],\n",
       "                      ...,\n",
       "                      [ 1.0767e-01, -2.2559e-01,  6.6956e-02,  ...,  1.7554e-01,\n",
       "                       -1.1255e-01, -1.7712e-01],\n",
       "                      [ 1.7139e-01, -2.7878e-02,  2.8882e-01,  ..., -3.6530e-02,\n",
       "                        3.8892e-01, -3.6774e-02],\n",
       "                      [ 1.1041e-01,  1.1151e-01,  4.4946e-01,  ...,  2.5220e-01,\n",
       "                       -1.1469e-01,  2.3083e-01]])),\n",
       "             ('model.decoder.layers.9.fc2.bias',\n",
       "              tensor([ 0.4521, -0.3462,  0.2539,  ..., -0.4795,  0.3608, -0.4741])),\n",
       "             ('model.decoder.layers.9.final_layer_norm.weight',\n",
       "              tensor([1.1299, 1.1377, 1.1543,  ..., 1.1816, 1.1719, 1.1152])),\n",
       "             ('model.decoder.layers.9.final_layer_norm.bias',\n",
       "              tensor([-0.0729,  0.0707,  0.0654,  ..., -0.0680, -0.0355, -0.0500])),\n",
       "             ('model.decoder.layers.10.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0400,  0.3137,  0.2144,  ..., -0.0206, -0.0992,  0.0430],\n",
       "                      [ 0.0554, -0.1588, -0.0986,  ...,  0.0203,  0.0802,  0.0438],\n",
       "                      [ 0.0055,  0.3245,  0.2040,  ...,  0.0994,  0.1707, -0.0129],\n",
       "                      ...,\n",
       "                      [ 0.2585,  0.1141, -0.0263,  ...,  0.3691, -0.1616,  0.1261],\n",
       "                      [-0.1812, -0.1230, -0.0220,  ...,  0.2556,  0.0544, -0.1011],\n",
       "                      [ 0.0005,  0.0013, -0.0595,  ...,  0.1917,  0.1915, -0.1076]])),\n",
       "             ('model.decoder.layers.10.self_attn.k_proj.bias',\n",
       "              tensor([-0.0079, -0.0251, -0.0022,  ...,  0.0141, -0.0074, -0.0267])),\n",
       "             ('model.decoder.layers.10.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0881, -0.0816, -0.3635,  ..., -0.1008,  0.2979, -0.1902],\n",
       "                      [-0.4851,  0.2356, -0.1570,  ...,  0.4988,  0.2627, -0.2037],\n",
       "                      [-0.4224,  0.0797, -0.1847,  ...,  0.1333,  0.2605,  0.1112],\n",
       "                      ...,\n",
       "                      [-0.1384,  0.1107,  0.3318,  ...,  0.0625, -0.3311,  0.2041],\n",
       "                      [ 0.2361, -0.0741,  0.0787,  ...,  0.1681,  0.2666,  0.1646],\n",
       "                      [ 0.0750,  0.3904,  0.2969,  ..., -0.0723,  0.3203,  0.2676]])),\n",
       "             ('model.decoder.layers.10.self_attn.v_proj.bias',\n",
       "              tensor([ 0.0285,  0.0306, -0.0096,  ...,  0.1098, -0.0996,  0.0311])),\n",
       "             ('model.decoder.layers.10.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0460, -0.0241, -0.1788,  ...,  0.0292,  0.1790,  0.0556],\n",
       "                      [ 0.1548,  0.5137,  0.0047,  ...,  0.0356, -0.0297, -0.0916],\n",
       "                      [ 0.2031,  0.1603,  0.0542,  ...,  0.1442, -0.1305, -0.3298],\n",
       "                      ...,\n",
       "                      [ 0.1368, -0.1372,  0.1624,  ..., -0.0352,  0.0616,  0.0765],\n",
       "                      [-0.0724,  0.2190,  0.0684,  ...,  0.4541,  0.0286,  0.0825],\n",
       "                      [ 0.0134,  0.2773, -0.0466,  ...,  0.0743, -0.0175,  0.0654]])),\n",
       "             ('model.decoder.layers.10.self_attn.q_proj.bias',\n",
       "              tensor([-0.1167,  0.1196,  0.5557,  ...,  0.0797,  0.0830, -0.0103])),\n",
       "             ('model.decoder.layers.10.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.1730, -0.1925,  0.2759,  ..., -0.1399,  0.3088, -0.1022],\n",
       "                      [ 0.3936,  0.5000,  0.0141,  ..., -0.1937, -0.4775,  0.0814],\n",
       "                      [ 0.2512, -0.1377, -0.1252,  ..., -0.0349, -0.4585, -0.1528],\n",
       "                      ...,\n",
       "                      [ 0.0133,  0.1696,  0.0926,  ..., -0.1819,  0.3171, -0.1846],\n",
       "                      [ 0.3127,  0.3918,  0.3259,  ...,  0.1276,  0.2471, -0.1536],\n",
       "                      [-0.2988, -0.4556,  0.1615,  ...,  0.1554,  0.1527,  0.1873]])),\n",
       "             ('model.decoder.layers.10.self_attn.out_proj.bias',\n",
       "              tensor([-0.0384, -0.5000, -0.2482,  ...,  0.2445,  0.2729,  0.2881])),\n",
       "             ('model.decoder.layers.10.self_attn_layer_norm.weight',\n",
       "              tensor([0.4563, 0.4343, 0.4404,  ..., 0.4568, 0.4783, 0.4636])),\n",
       "             ('model.decoder.layers.10.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0194,  0.0422,  0.0615,  ..., -0.0120,  0.0118, -0.0328])),\n",
       "             ('model.decoder.layers.10.encoder_attn.k_proj.weight',\n",
       "              tensor([[ 3.4149e-02, -5.1575e-02, -6.8474e-03,  ..., -2.6270e-01,\n",
       "                       -1.0559e-01,  3.8544e-02],\n",
       "                      [-1.1383e-02,  1.3562e-01,  5.2887e-02,  ..., -3.1143e-02,\n",
       "                        4.0924e-02, -1.0577e-01],\n",
       "                      [ 1.4014e-01, -5.2338e-02,  1.0590e-01,  ...,  8.5938e-02,\n",
       "                       -1.3757e-01, -5.6519e-02],\n",
       "                      ...,\n",
       "                      [ 8.4595e-02,  1.6101e-01,  5.9387e-02,  ...,  2.5620e-02,\n",
       "                        1.3062e-01,  8.1665e-02],\n",
       "                      [-1.1390e-04,  1.9577e-02, -4.6783e-02,  ...,  6.4331e-02,\n",
       "                        2.7808e-01,  7.2205e-02],\n",
       "                      [ 1.6769e-02,  1.3708e-01,  1.8848e-01,  ...,  1.6357e-01,\n",
       "                       -3.4241e-02,  4.7729e-02]])),\n",
       "             ('model.decoder.layers.10.encoder_attn.k_proj.bias',\n",
       "              tensor([-0.0041, -0.0047, -0.0187,  ..., -0.0097,  0.0097, -0.0026])),\n",
       "             ('model.decoder.layers.10.encoder_attn.v_proj.weight',\n",
       "              tensor([[-0.3379, -0.1038, -0.1940,  ...,  0.1746, -0.0856,  0.0266],\n",
       "                      [-0.2671,  0.0333, -0.0767,  ...,  0.4155, -0.5005, -0.0095],\n",
       "                      [ 0.3232, -0.4922, -0.1841,  ..., -0.3206, -0.0970, -0.0087],\n",
       "                      ...,\n",
       "                      [ 0.1046,  0.0024, -0.0066,  ...,  0.4270, -0.3845, -0.0042],\n",
       "                      [-0.1265, -0.1398,  0.5059,  ..., -0.2610, -0.1061, -0.1057],\n",
       "                      [-0.2502,  0.1376, -0.0500,  ...,  0.3198, -0.2014, -0.0424]])),\n",
       "             ('model.decoder.layers.10.encoder_attn.v_proj.bias',\n",
       "              tensor([-0.1736, -0.1248, -0.0724,  ..., -0.0193,  0.0529, -0.0103])),\n",
       "             ('model.decoder.layers.10.encoder_attn.q_proj.weight',\n",
       "              tensor([[ 0.2465,  0.3572, -0.1578,  ...,  0.1051,  0.1056,  0.2705],\n",
       "                      [ 0.1396,  0.0517,  0.1371,  ..., -0.1715,  0.2111,  0.0310],\n",
       "                      [ 0.2795, -0.0452,  0.0254,  ...,  0.0745, -0.2004, -0.0875],\n",
       "                      ...,\n",
       "                      [ 0.2090,  0.1552,  0.0294,  ...,  0.0163,  0.1880, -0.0748],\n",
       "                      [-0.0975,  0.2720, -0.0205,  ..., -0.2959,  0.0607, -0.1752],\n",
       "                      [-0.0417,  0.0686,  0.0341,  ..., -0.0052, -0.0607,  0.1924]])),\n",
       "             ('model.decoder.layers.10.encoder_attn.q_proj.bias',\n",
       "              tensor([-0.0300, -0.0188,  0.1372,  ...,  0.0365,  0.1879, -0.0723])),\n",
       "             ('model.decoder.layers.10.encoder_attn.out_proj.weight',\n",
       "              tensor([[-0.3167,  0.0122,  0.5063,  ..., -0.1279, -0.4751, -0.0692],\n",
       "                      [ 0.3191, -0.3286, -0.1993,  ...,  0.2517, -0.3118,  0.1114],\n",
       "                      [-0.2600, -0.3374,  0.1562,  ..., -0.0804, -0.0756,  0.4536],\n",
       "                      ...,\n",
       "                      [-0.4307,  0.0817, -0.2385,  ...,  0.0475, -0.0667, -0.0591],\n",
       "                      [-0.3049, -0.0431, -0.3372,  ...,  0.0893, -0.2561, -0.6758],\n",
       "                      [ 0.0142, -0.3330, -0.1852,  ...,  0.0263,  0.1252,  0.2230]])),\n",
       "             ('model.decoder.layers.10.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.4207,  0.3188, -0.1364,  ...,  0.4988,  0.1904,  0.2290])),\n",
       "             ('model.decoder.layers.10.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.1527, 0.1567, 0.1797,  ..., 0.1515, 0.1356, 0.1373])),\n",
       "             ('model.decoder.layers.10.encoder_attn_layer_norm.bias',\n",
       "              tensor([ 0.0061, -0.0064, -0.0070,  ..., -0.0638, -0.0081, -0.0396])),\n",
       "             ('model.decoder.layers.10.fc1.weight',\n",
       "              tensor([[ 0.1241, -0.1385, -0.1334,  ..., -0.0159,  0.2927,  0.0763],\n",
       "                      [ 0.0085,  0.0656,  0.0775,  ..., -0.0494, -0.1176,  0.4116],\n",
       "                      [-0.4534,  0.1737,  0.0739,  ...,  0.1304,  0.3660, -0.1552],\n",
       "                      ...,\n",
       "                      [ 0.4180,  0.0042, -0.1140,  ..., -0.1505, -0.3511, -0.1108],\n",
       "                      [-0.1115,  0.1649,  0.0151,  ...,  0.1686,  0.1980, -0.2250],\n",
       "                      [ 0.4780,  0.0598, -0.0205,  ...,  0.0529,  0.2556, -0.0136]])),\n",
       "             ('model.decoder.layers.10.fc1.bias',\n",
       "              tensor([ 0.0577, -0.0452, -0.0366,  ..., -0.1164, -0.1792, -0.0677])),\n",
       "             ('model.decoder.layers.10.fc2.weight',\n",
       "              tensor([[-0.0407, -0.1617,  0.1550,  ...,  0.1205, -0.5635,  0.1567],\n",
       "                      [-0.4338,  0.2764, -0.1478,  ..., -0.0103, -0.0054,  0.1014],\n",
       "                      [-0.3440, -0.4995, -0.1874,  ...,  0.0639, -0.1431, -0.2632],\n",
       "                      ...,\n",
       "                      [-0.0983,  0.1627,  0.1613,  ..., -0.0656, -0.2231,  0.2571],\n",
       "                      [ 0.2744, -0.3892,  0.2253,  ..., -0.0503,  0.2094,  0.0518],\n",
       "                      [ 0.1449, -0.2666,  0.1943,  ..., -0.1539, -0.0191, -0.0632]])),\n",
       "             ('model.decoder.layers.10.fc2.bias',\n",
       "              tensor([ 0.2487, -0.1239, -0.1879,  ..., -0.2219, -0.0636, -0.2351])),\n",
       "             ('model.decoder.layers.10.final_layer_norm.weight',\n",
       "              tensor([1.2422, 1.2334, 1.1982,  ..., 1.0586, 1.2021, 1.0361])),\n",
       "             ('model.decoder.layers.10.final_layer_norm.bias',\n",
       "              tensor([-0.0576,  0.0120,  0.0236,  ..., -0.0124,  0.0219, -0.0676])),\n",
       "             ('model.decoder.layers.11.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.2886,  0.1121, -0.0573,  ...,  0.0407, -0.0726, -0.2524],\n",
       "                      [-0.0529, -0.0389,  0.0386,  ..., -0.1946, -0.0904,  0.0975],\n",
       "                      [ 0.0696, -0.2507,  0.1560,  ..., -0.0846, -0.1680,  0.0520],\n",
       "                      ...,\n",
       "                      [-0.1136,  0.1085,  0.0013,  ...,  0.1771,  0.0459,  0.0127],\n",
       "                      [-0.0032, -0.1432,  0.0987,  ...,  0.0447, -0.1344, -0.1840],\n",
       "                      [ 0.0046, -0.1639, -0.0983,  ...,  0.0025, -0.1478,  0.0221]])),\n",
       "             ('model.decoder.layers.11.self_attn.k_proj.bias',\n",
       "              tensor([-0.0265, -0.0275, -0.0131,  ..., -0.0024,  0.0249,  0.0030])),\n",
       "             ('model.decoder.layers.11.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0421,  0.3135,  0.3291,  ...,  0.0999,  0.2000, -0.1260],\n",
       "                      [ 0.4084, -0.0457,  0.0492,  ..., -0.3721,  0.1077, -0.1754],\n",
       "                      [ 0.2480, -0.3030, -0.0977,  ..., -0.2354,  0.1797,  0.2527],\n",
       "                      ...,\n",
       "                      [-0.5186, -0.2668,  0.1226,  ..., -0.4153, -0.1384, -0.2952],\n",
       "                      [ 0.1610, -0.0661,  0.2795,  ..., -0.1015,  0.3125,  0.0173],\n",
       "                      [ 0.3323, -0.0244, -0.1360,  ...,  0.2456, -0.2400,  0.1328]])),\n",
       "             ('model.decoder.layers.11.self_attn.v_proj.bias',\n",
       "              tensor([-0.1528,  0.0600,  0.0855,  ..., -0.1096,  0.1088,  0.2185])),\n",
       "             ('model.decoder.layers.11.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0587,  0.0670, -0.1146,  ..., -0.2434, -0.0790, -0.1637],\n",
       "                      [ 0.0241,  0.0916,  0.1638,  ...,  0.0948,  0.0159, -0.2111],\n",
       "                      [ 0.2725,  0.0554, -0.0481,  ...,  0.0247, -0.1354, -0.2389],\n",
       "                      ...,\n",
       "                      [-0.1547,  0.1963,  0.0953,  ...,  0.1492,  0.0555, -0.2155],\n",
       "                      [-0.1432, -0.2502, -0.0406,  ...,  0.1307, -0.1010,  0.1482],\n",
       "                      [ 0.0346,  0.0297,  0.2104,  ...,  0.0958,  0.0427,  0.0003]])),\n",
       "             ('model.decoder.layers.11.self_attn.q_proj.bias',\n",
       "              tensor([ 0.0385, -0.0245,  0.0283,  ..., -0.2791, -0.0375,  0.2209])),\n",
       "             ('model.decoder.layers.11.self_attn.out_proj.weight',\n",
       "              tensor([[-0.0576,  0.1973,  0.3860,  ...,  0.0765, -0.0016, -0.0147],\n",
       "                      [ 0.2068,  0.3779, -0.3418,  ..., -0.0335, -0.2578, -0.1578],\n",
       "                      [ 0.1189,  0.2546, -0.0859,  ..., -0.3020, -0.1877, -0.3271],\n",
       "                      ...,\n",
       "                      [ 0.0778,  0.1593, -0.0295,  ...,  0.2583,  0.0644, -0.0288],\n",
       "                      [ 0.1534,  0.1674,  0.3032,  ..., -0.2220, -0.2607, -0.3271],\n",
       "                      [-0.0537,  0.2104,  0.3301,  ...,  0.2390, -0.0082,  0.1754]])),\n",
       "             ('model.decoder.layers.11.self_attn.out_proj.bias',\n",
       "              tensor([ 0.1097, -0.2463, -0.2507,  ...,  0.0800,  0.3486, -0.2583])),\n",
       "             ('model.decoder.layers.11.self_attn_layer_norm.weight',\n",
       "              tensor([0.4829, 0.4480, 0.4556,  ..., 0.5693, 0.4761, 0.5493])),\n",
       "             ('model.decoder.layers.11.self_attn_layer_norm.bias',\n",
       "              tensor([ 0.0308,  0.0459,  0.0575,  ...,  0.0363,  0.0300, -0.0277])),\n",
       "             ('model.decoder.layers.11.encoder_attn.k_proj.weight',\n",
       "              tensor([[ 0.0150, -0.1483,  0.0195,  ..., -0.0609,  0.2163, -0.0399],\n",
       "                      [ 0.0737,  0.1042,  0.1449,  ..., -0.0804,  0.0560,  0.0888],\n",
       "                      [ 0.0198, -0.0088, -0.1322,  ...,  0.1057, -0.1063,  0.0030],\n",
       "                      ...,\n",
       "                      [ 0.2419, -0.2389, -0.0453,  ...,  0.0304,  0.2489, -0.0426],\n",
       "                      [-0.1217, -0.0621, -0.0611,  ..., -0.0334, -0.2382,  0.0494],\n",
       "                      [ 0.1508,  0.0376, -0.2196,  ...,  0.0021, -0.0352,  0.0072]])),\n",
       "             ('model.decoder.layers.11.encoder_attn.k_proj.bias',\n",
       "              tensor([ 0.0108, -0.0240, -0.0222,  ..., -0.0309,  0.0018,  0.0179])),\n",
       "             ('model.decoder.layers.11.encoder_attn.v_proj.weight',\n",
       "              tensor([[ 0.0915, -0.2383,  0.2522,  ..., -0.1445,  0.1812, -0.0104],\n",
       "                      [-0.0807, -0.5171,  0.2520,  ...,  0.1231, -0.4302, -0.0272],\n",
       "                      [-0.3323, -0.1627, -0.3203,  ...,  0.1217,  0.1506, -0.0217],\n",
       "                      ...,\n",
       "                      [ 0.2469,  0.3311,  0.0141,  ...,  0.2886, -0.0831,  0.0104],\n",
       "                      [-0.2668,  0.0880, -0.2100,  ...,  0.3708,  0.2345,  0.0215],\n",
       "                      [-0.4988,  0.3843, -0.3296,  ..., -0.2578,  0.1772,  0.0237]])),\n",
       "             ('model.decoder.layers.11.encoder_attn.v_proj.bias',\n",
       "              tensor([-0.0620, -0.0496, -0.1109,  ..., -0.0281, -0.0598, -0.0185])),\n",
       "             ('model.decoder.layers.11.encoder_attn.q_proj.weight',\n",
       "              tensor([[ 0.0280,  0.3601, -0.0068,  ..., -0.0675, -0.1942, -0.1337],\n",
       "                      [ 0.0245, -0.2771,  0.0118,  ...,  0.1318,  0.0584, -0.0584],\n",
       "                      [ 0.2629,  0.1715,  0.2456,  ..., -0.2177,  0.0575, -0.1232],\n",
       "                      ...,\n",
       "                      [ 0.1587,  0.0914, -0.1600,  ..., -0.0190,  0.1428,  0.0207],\n",
       "                      [ 0.1343,  0.1017, -0.2053,  ..., -0.0431, -0.0784,  0.0717],\n",
       "                      [ 0.4167,  0.2212,  0.1477,  ...,  0.2322,  0.1138,  0.1298]])),\n",
       "             ('model.decoder.layers.11.encoder_attn.q_proj.bias',\n",
       "              tensor([-0.0859,  0.0497, -0.0948,  ..., -0.1085,  0.3074,  0.0983])),\n",
       "             ('model.decoder.layers.11.encoder_attn.out_proj.weight',\n",
       "              tensor([[ 0.1483, -0.3293,  0.1842,  ...,  0.1281,  0.3018,  0.0837],\n",
       "                      [-0.2546, -0.1025,  0.0375,  ..., -0.1661,  0.2649, -0.0537],\n",
       "                      [ 0.5557, -0.2404,  0.2537,  ...,  0.2489, -0.4336, -0.2517],\n",
       "                      ...,\n",
       "                      [ 0.1147, -0.0643,  0.2102,  ...,  0.0982,  0.2448,  0.1059],\n",
       "                      [ 0.1241, -0.3018, -0.0366,  ...,  0.0471, -0.0987, -0.0247],\n",
       "                      [-0.1238,  0.0363,  0.0179,  ..., -0.1735,  0.0165,  0.0449]])),\n",
       "             ('model.decoder.layers.11.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.4980, -0.1772, -0.2944,  ...,  0.3176,  0.3970, -0.3376])),\n",
       "             ('model.decoder.layers.11.encoder_attn_layer_norm.weight',\n",
       "              tensor([0.2180, 0.2142, 0.2432,  ..., 0.2505, 0.1937, 0.5459])),\n",
       "             ('model.decoder.layers.11.encoder_attn_layer_norm.bias',\n",
       "              tensor([ 0.0131, -0.0211, -0.0297,  ..., -0.0921, -0.0276, -0.1627])),\n",
       "             ('model.decoder.layers.11.fc1.weight',\n",
       "              tensor([[ 0.2133,  0.1367,  0.1583,  ...,  0.1154,  0.1157,  0.2507],\n",
       "                      [ 0.5381,  0.2512,  0.0615,  ..., -0.1866,  0.0807, -0.4976],\n",
       "                      [ 0.1731, -0.0155,  0.1274,  ..., -0.0911, -0.0814,  0.0748],\n",
       "                      ...,\n",
       "                      [-0.4023,  0.0764,  0.1492,  ...,  0.0858, -0.1229, -0.1460],\n",
       "                      [ 0.3167, -0.4993, -0.1406,  ..., -0.0420, -0.0975, -0.1920],\n",
       "                      [ 0.0118, -0.0053,  0.3694,  ..., -0.1689,  0.1758,  0.3420]])),\n",
       "             ('model.decoder.layers.11.fc1.bias',\n",
       "              tensor([-0.0565, -0.1241,  0.1722,  ..., -0.1564, -0.1884, -0.1355])),\n",
       "             ('model.decoder.layers.11.fc2.weight',\n",
       "              tensor([[-0.2155,  0.0026, -0.0821,  ...,  0.2438,  0.4438, -0.1379],\n",
       "                      [-0.0099,  0.0284,  0.1877,  ..., -0.3665, -0.3655, -0.4993],\n",
       "                      [ 0.2686, -0.4487, -0.1062,  ..., -0.1671,  0.3542,  0.2520],\n",
       "                      ...,\n",
       "                      [-0.0320,  0.1042,  0.0439,  ...,  0.0439, -0.1384,  0.1189],\n",
       "                      [-0.0164, -0.0094, -0.0580,  ..., -0.0873,  0.1727,  0.1772],\n",
       "                      [-0.0598, -0.1796, -0.1490,  ...,  0.0716, -0.3953,  0.0061]])),\n",
       "             ('model.decoder.layers.11.fc2.bias',\n",
       "              tensor([ 0.1031,  0.1510, -0.1302,  ...,  0.2443, -0.0541, -0.2505])),\n",
       "             ('model.decoder.layers.11.final_layer_norm.weight',\n",
       "              tensor([1.0381, 1.0693, 1.0654,  ..., 1.0059, 1.0098, 1.0000])),\n",
       "             ('model.decoder.layers.11.final_layer_norm.bias',\n",
       "              tensor([ 0.1394,  0.1179,  0.1831,  ..., -0.1677,  0.1306, -0.0412])),\n",
       "             ('model.decoder.layer_norm.weight',\n",
       "              tensor([0.6274, 1.0703, 1.1914,  ..., 0.8398, 0.4060, 0.4321])),\n",
       "             ('model.decoder.layer_norm.bias',\n",
       "              tensor([ 0.0264,  0.0236,  0.0949,  ...,  0.3755, -0.0079, -0.0789])),\n",
       "             ('lm_head.weight',\n",
       "              tensor([[-inf, inf, inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      ...,\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
       "                      [-inf, -inf, -inf,  ..., inf, -inf, -inf]]))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[2].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # 32 already doesn't fit well to 15GB of GPU memory\n",
    "max_length = 128  # token sequences will be truncated\n",
    "training_steps = 100  # Usually, I set a large number of steps,\n",
    "# and then just interrupt the training manually\n",
    "losses = []  # with this list, I do very simple tracking of average loss\n",
    "MODEL_SAVE_PATH = './NLLB/nllb-eng-kir-v1'  # on my Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfeea81f67fe45e6b3437cf5561b222b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# -100 is a magic value ignored in the loss function\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# because we don't want the model to learn to predict padding ids\u001b[39;00m\n\u001b[1;32m     17\u001b[0m y\u001b[38;5;241m.\u001b[39minput_ids[y\u001b[38;5;241m.\u001b[39minput_ids \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/m2m_100/modeling_m2m_100.py:1599\u001b[0m, in \u001b[0;36mM2M100ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1578\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1579\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1580\u001b[0m         )\n\u001b[1;32m   1582\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1583\u001b[0m     input_ids,\n\u001b[1;32m   1584\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1598\u001b[0m )\n\u001b[0;32m-> 1599\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1601\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;66;03m# move labels to the correct device to enable PP\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "models[0].train()\n",
    "x, y, loss = None, None, None\n",
    "cleanup()\n",
    "\n",
    "tq = trange(len(losses), training_steps)\n",
    "for i in tq:\n",
    "    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)\n",
    "    try:\n",
    "        tokenizer.src_lang = lang1\n",
    "        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
    "        tokenizer.src_lang = lang2\n",
    "        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
    "        # -100 is a magic value ignored in the loss function\n",
    "        # because we don't want the model to learn to predict padding ids\n",
    "        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        loss = models[0](**x, labels=y.input_ids).loss\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    except RuntimeError as e:  # usually, it is out-of-memory\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        x, y, loss = None, None, None\n",
    "        cleanup()\n",
    "        print('error', max(len(s) for s in xx + yy), e)\n",
    "        continue\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        # each 1000 steps, I report average loss at these steps\n",
    "        print(i, np.mean(losses[-1000:]))\n",
    "\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        model.save_pretrained(MODEL_SAVE_PATH)\n",
    "        tokenizer.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the model to a quantized version\n",
    "torch.quantization.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(\n",
    "    text, model, src_lang='run_Latn', tgt_lang='eng_Latn', \n",
    "    a=32, b=3, max_input_length=1024, num_beams=4, **kwargs\n",
    "):\n",
    "    \"\"\"Turn a text or a list of texts into a list of translations\"\"\"\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors='pt', padding=True, truncation=True, \n",
    "        max_length=max_input_length\n",
    "    )\n",
    "    model.eval() # turn off training mode\n",
    "    result = model.generate(\n",
    "        **inputs.to(model.device),\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
    "        num_beams=num_beams, **kwargs\n",
    "    )\n",
    "    return tokenizer.batch_decode(result, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_translate(texts, model, batch_size=16, **kwargs):\n",
    "    \"\"\"Translate texts in batches of similar length\"\"\"\n",
    "    idxs, texts2 = zip(*sorted(enumerate(texts), key=lambda p: len(p[1]), reverse=True))\n",
    "    results = []\n",
    "    for i in trange(0, len(texts2), batch_size):\n",
    "        results.extend(translate(texts2[i: i+batch_size], model, **kwargs))\n",
    "    return [p for i, p in sorted(zip(idxs, results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_test = load_dataset(\"Muennighoff/flores200\", 'eng_Latn-run_Latn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset_test['devtest']\n",
    "flores_test = pd.DataFrame(([dataset_test['sentence_eng_Latn'], dataset_test['sentence_run_Latn']]))\n",
    "flores_test = flores_test.T\n",
    "flores_test.columns = ['eng', 'run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"We now have 4-month-old mice that are non-dia...</td>\n",
       "      <td>Yongeyeko ati: \"Ubu turafise imbeba y'amezi 4 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Ehud Ur, professor of medicine at Dalhousi...</td>\n",
       "      <td>Umuhinga Ehud Ur, umwigisha w'ivy'ubuganga kur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Like some other experts, he is skeptical about...</td>\n",
       "      <td>Cokimwe n'abandi bahinga, arafise amakenga ku ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Monday, Sara Danius, permanent secretary of...</td>\n",
       "      <td>Ku wa mbere, Sara Danius, umunyamabanga ntayeg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Danius said, \"Right now we are doing nothing. ...</td>\n",
       "      <td>Danius yavuze ati: \"Ubu nta co turiko turakora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>As the areas are sparsely populated, and light...</td>\n",
       "      <td>Kuko ivyo bice bibamwo abantu inkehwa, kandi n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>Japanese work culture is more hierarchical and...</td>\n",
       "      <td>Akaranga mu kazi k'Abayapani karasumbasumbana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>Suits are standard business attire, and cowork...</td>\n",
       "      <td>Ikositimu niwo mwambaro w'akazi umenyerewe, ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>Workplace harmony is crucial, emphasizing grou...</td>\n",
       "      <td>Itunganywa ryiza ry'ikibanza c'akazi ni ngombw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>Workers must often get their superiors' approv...</td>\n",
       "      <td>Abakozi bategerezwa kenshi kuronka uruhusha rw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1012 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    eng  \\\n",
       "0     \"We now have 4-month-old mice that are non-dia...   \n",
       "1     Dr. Ehud Ur, professor of medicine at Dalhousi...   \n",
       "2     Like some other experts, he is skeptical about...   \n",
       "3     On Monday, Sara Danius, permanent secretary of...   \n",
       "4     Danius said, \"Right now we are doing nothing. ...   \n",
       "...                                                 ...   \n",
       "1007  As the areas are sparsely populated, and light...   \n",
       "1008  Japanese work culture is more hierarchical and...   \n",
       "1009  Suits are standard business attire, and cowork...   \n",
       "1010  Workplace harmony is crucial, emphasizing grou...   \n",
       "1011  Workers must often get their superiors' approv...   \n",
       "\n",
       "                                                    run  \n",
       "0     Yongeyeko ati: \"Ubu turafise imbeba y'amezi 4 ...  \n",
       "1     Umuhinga Ehud Ur, umwigisha w'ivy'ubuganga kur...  \n",
       "2     Cokimwe n'abandi bahinga, arafise amakenga ku ...  \n",
       "3     Ku wa mbere, Sara Danius, umunyamabanga ntayeg...  \n",
       "4     Danius yavuze ati: \"Ubu nta co turiko turakora...  \n",
       "...                                                 ...  \n",
       "1007  Kuko ivyo bice bibamwo abantu inkehwa, kandi n...  \n",
       "1008  Akaranga mu kazi k'Abayapani karasumbasumbana ...  \n",
       "1009  Ikositimu niwo mwambaro w'akazi umenyerewe, ka...  \n",
       "1010  Itunganywa ryiza ry'ikibanza c'akazi ni ngombw...  \n",
       "1011  Abakozi bategerezwa kenshi kuronka uruhusha rw...  \n",
       "\n",
       "[1012 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6043562b36b54793a2826b7073ee74bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translations = batched_translate(flores_test['run'].tolist()[:100], models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 0.00 0.0/0.0/0.0/0.0 (BP = 1.000 ratio = 8.328 hyp_len = 21420 ref_len = 2572)\n",
      "chrF2++ = 0.97\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "bleu_calc = sacrebleu.BLEU()\n",
    "chrf_calc = sacrebleu.CHRF(word_order=2)  # this metric is called ChrF++\n",
    "\n",
    "print(bleu_calc.corpus_score(translations, [flores_test['eng'].tolist()]))\n",
    "print(chrf_calc.corpus_score(translations, [flores_test['eng'].tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.453369494797222 in 14 minutes inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "flores_train[['eng', 'run']].to_csv('flores-eng-kir.csv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>run</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"We now have 4-month-old mice that are non-dia...</td>\n",
       "      <td>Yongeyeko ati: \"Ubu turafise imbeba y'amezi 4 ...</td>\n",
       "      <td>He added: \"We now have four-month-old mice who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Ehud Ur, professor of medicine at Dalhousi...</td>\n",
       "      <td>Umuhinga Ehud Ur, umwigisha w'ivy'ubuganga kur...</td>\n",
       "      <td>Professor Ehud Ur, a professor of medicine at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Like some other experts, he is skeptical about...</td>\n",
       "      <td>Cokimwe n'abandi bahinga, arafise amakenga ku ...</td>\n",
       "      <td>Like other scientists, he is skeptical of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Monday, Sara Danius, permanent secretary of...</td>\n",
       "      <td>Ku wa mbere, Sara Danius, umunyamabanga ntayeg...</td>\n",
       "      <td>On Monday, Sara Danius, permanent secretary of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Danius said, \"Right now we are doing nothing. ...</td>\n",
       "      <td>Danius yavuze ati: \"Ubu nta co turiko turakora...</td>\n",
       "      <td>Danius says: \"Now that we're doing nothing, I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>As the areas are sparsely populated, and light...</td>\n",
       "      <td>Kuko ivyo bice bibamwo abantu inkehwa, kandi n...</td>\n",
       "      <td>For this is a small part of the human family, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>Japanese work culture is more hierarchical and...</td>\n",
       "      <td>Akaranga mu kazi k'Abayapani karasumbasumbana ...</td>\n",
       "      <td>Japanese craftsmanship is more sophisticated a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>Suits are standard business attire, and cowork...</td>\n",
       "      <td>Ikositimu niwo mwambaro w'akazi umenyerewe, ka...</td>\n",
       "      <td>Costumes are the most common work clothes, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>Workplace harmony is crucial, emphasizing grou...</td>\n",
       "      <td>Itunganywa ryiza ry'ikibanza c'akazi ni ngombw...</td>\n",
       "      <td>Good workplace planning is essential, celebrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>Workers must often get their superiors' approv...</td>\n",
       "      <td>Abakozi bategerezwa kenshi kuronka uruhusha rw...</td>\n",
       "      <td>Employees often have to obtain adult consent f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1012 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    lat  \\\n",
       "0     \"We now have 4-month-old mice that are non-dia...   \n",
       "1     Dr. Ehud Ur, professor of medicine at Dalhousi...   \n",
       "2     Like some other experts, he is skeptical about...   \n",
       "3     On Monday, Sara Danius, permanent secretary of...   \n",
       "4     Danius said, \"Right now we are doing nothing. ...   \n",
       "...                                                 ...   \n",
       "1007  As the areas are sparsely populated, and light...   \n",
       "1008  Japanese work culture is more hierarchical and...   \n",
       "1009  Suits are standard business attire, and cowork...   \n",
       "1010  Workplace harmony is crucial, emphasizing grou...   \n",
       "1011  Workers must often get their superiors' approv...   \n",
       "\n",
       "                                                    run  \\\n",
       "0     Yongeyeko ati: \"Ubu turafise imbeba y'amezi 4 ...   \n",
       "1     Umuhinga Ehud Ur, umwigisha w'ivy'ubuganga kur...   \n",
       "2     Cokimwe n'abandi bahinga, arafise amakenga ku ...   \n",
       "3     Ku wa mbere, Sara Danius, umunyamabanga ntayeg...   \n",
       "4     Danius yavuze ati: \"Ubu nta co turiko turakora...   \n",
       "...                                                 ...   \n",
       "1007  Kuko ivyo bice bibamwo abantu inkehwa, kandi n...   \n",
       "1008  Akaranga mu kazi k'Abayapani karasumbasumbana ...   \n",
       "1009  Ikositimu niwo mwambaro w'akazi umenyerewe, ka...   \n",
       "1010  Itunganywa ryiza ry'ikibanza c'akazi ni ngombw...   \n",
       "1011  Abakozi bategerezwa kenshi kuronka uruhusha rw...   \n",
       "\n",
       "                                             translated  \n",
       "0     He added: \"We now have four-month-old mice who...  \n",
       "1     Professor Ehud Ur, a professor of medicine at ...  \n",
       "2     Like other scientists, he is skeptical of the ...  \n",
       "3     On Monday, Sara Danius, permanent secretary of...  \n",
       "4     Danius says: \"Now that we're doing nothing, I'...  \n",
       "...                                                 ...  \n",
       "1007  For this is a small part of the human family, ...  \n",
       "1008  Japanese craftsmanship is more sophisticated a...  \n",
       "1009  Costumes are the most common work clothes, and...  \n",
       "1010  Good workplace planning is essential, celebrat...  \n",
       "1011  Employees often have to obtain adult consent f...  \n",
       "\n",
       "[1012 rows x 3 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
